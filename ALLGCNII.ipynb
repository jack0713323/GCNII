{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d417fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:42:32.426776Z",
     "iopub.status.busy": "2024-04-24T05:42:32.426433Z",
     "iopub.status.idle": "2024-04-24T05:42:34.627129Z",
     "shell.execute_reply": "2024-04-24T05:42:34.625999Z"
    },
    "papermill": {
     "duration": 2.210276,
     "end_time": "2024-04-24T05:42:34.629603",
     "exception": false,
     "start_time": "2024-04-24T05:42:32.419327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GCNII'...\r\n",
      "remote: Enumerating objects: 242, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (109/109), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (58/58), done.\u001b[K\r\n",
      "remote: Total 242 (delta 67), reused 80 (delta 50), pack-reused 133\u001b[K\r\n",
      "Receiving objects: 100% (242/242), 5.53 MiB | 18.43 MiB/s, done.\r\n",
      "Resolving deltas: 100% (91/91), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jack0713323/GCNII.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c60fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:42:34.644216Z",
     "iopub.status.busy": "2024-04-24T05:42:34.643934Z",
     "iopub.status.idle": "2024-04-24T05:42:34.650019Z",
     "shell.execute_reply": "2024-04-24T05:42:34.649178Z"
    },
    "papermill": {
     "duration": 0.015856,
     "end_time": "2024-04-24T05:42:34.652093",
     "exception": false,
     "start_time": "2024-04-24T05:42:34.636237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/GCNII\n"
     ]
    }
   ],
   "source": [
    "%cd GCNII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67706afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:42:34.666671Z",
     "iopub.status.busy": "2024-04-24T05:42:34.666409Z",
     "iopub.status.idle": "2024-04-24T05:42:35.618659Z",
     "shell.execute_reply": "2024-04-24T05:42:35.617736Z"
    },
    "papermill": {
     "duration": 0.962275,
     "end_time": "2024-04-24T05:42:35.620945",
     "exception": false,
     "start_time": "2024-04-24T05:42:34.658670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111034507linzijie.txt\tfull-supervisedtest10.py  pretrained  train3.py\r\n",
      "PyG\t\t\tfull.sh\t\t\t  process.py  trainTEST.py\r\n",
      "README.md\t\tmodel.py\t\t  semi.sh     trainshow.py\r\n",
      "cora25.ipynb\t\tmodelnew.py\t\t  splits      utils.py\r\n",
      "data\t\t\tnew_data\t\t  train.py\r\n",
      "full-supervised.py\tppi.py\t\t\t  train1.py\r\n",
      "full-supervisedtest.py\tppi.sh\t\t\t  train2.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83833ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:42:35.635628Z",
     "iopub.status.busy": "2024-04-24T05:42:35.635309Z",
     "iopub.status.idle": "2024-04-24T05:42:35.639818Z",
     "shell.execute_reply": "2024-04-24T05:42:35.638977Z"
    },
    "papermill": {
     "duration": 0.014192,
     "end_time": "2024-04-24T05:42:35.641796",
     "exception": false,
     "start_time": "2024-04-24T05:42:35.627604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "repeat=1\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295a0d1",
   "metadata": {
    "papermill": {
     "duration": 0.006546,
     "end_time": "2024-04-24T05:42:35.654884",
     "exception": false,
     "start_time": "2024-04-24T05:42:35.648338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdbe5875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:42:35.670090Z",
     "iopub.status.busy": "2024-04-24T05:42:35.669821Z",
     "iopub.status.idle": "2024-04-24T05:42:35.674204Z",
     "shell.execute_reply": "2024-04-24T05:42:35.673376Z"
    },
    "papermill": {
     "duration": 0.014727,
     "end_time": "2024-04-24T05:42:35.676159",
     "exception": false,
     "start_time": "2024-04-24T05:42:35.661432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a=(64,0,0,0,100,1500,64,0,0.1,0.5,0.6,0.01,0.01,0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0798a46e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:42:35.690168Z",
     "iopub.status.busy": "2024-04-24T05:42:35.689915Z",
     "iopub.status.idle": "2024-04-24T05:42:35.695770Z",
     "shell.execute_reply": "2024-04-24T05:42:35.694901Z"
    },
    "papermill": {
     "duration": 0.014867,
     "end_time": "2024-04-24T05:42:35.697556",
     "exception": false,
     "start_time": "2024-04-24T05:42:35.682689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_num_layers = int(a[0])\n",
    "best_optim = int(a[1])\n",
    "best_al = int(a[2])\n",
    "best_act = int(a[3])\n",
    "best_earlystop = int(a[4])\n",
    "best_epoch1 = int(a[5])\n",
    "best_hidden_dim = int(a[6])\n",
    "best_variant = a[7]\n",
    "\n",
    "best_alpha = a[8]\n",
    "best_lamda = a[9]\n",
    "best_dropout = a[10]\n",
    "best_lr = a[11]\n",
    "best_weight_decay_1 = a[12]\n",
    "best_weight_decay_2 = a[13]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502b9b63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:42:35.711223Z",
     "iopub.status.busy": "2024-04-24T05:42:35.710957Z",
     "iopub.status.idle": "2024-04-24T05:42:35.717285Z",
     "shell.execute_reply": "2024-04-24T05:42:35.716440Z"
    },
    "papermill": {
     "duration": 0.015748,
     "end_time": "2024-04-24T05:42:35.719698",
     "exception": false,
     "start_time": "2024-04-24T05:42:35.703950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_num_layers: 64\n",
      "best_optim: 0\n",
      "best_al: 0\n",
      "best_act: 0\n",
      "best_earlystop: 100\n",
      "best_epoch1: 1500\n",
      "best_hidden_dim: 64\n",
      "best_variant: 0\n",
      "---------\n",
      "best_alpha: 0.1\n",
      "best_lamda: 0.5\n",
      "best_dropout: 0.6\n",
      "best_lr: 0.01\n",
      "best_weight_decay_1: 0.01\n",
      "best_weight_decay_2: 0.0005\n"
     ]
    }
   ],
   "source": [
    "print(f\"best_num_layers: {best_num_layers}\")\n",
    "print(f\"best_optim: {best_optim}\")\n",
    "print(f\"best_al: {best_al}\")\n",
    "print(f\"best_act: {best_act}\")\n",
    "print(f\"best_earlystop: {best_earlystop}\")\n",
    "print(f\"best_epoch1: {best_epoch1}\")\n",
    "print(f\"best_hidden_dim: {best_hidden_dim}\")\n",
    "print(f\"best_variant: {best_variant}\")\n",
    "print(\"---------\")\n",
    "print(f\"best_alpha: {best_alpha}\")\n",
    "print(f\"best_lamda: {best_lamda}\")\n",
    "print(f\"best_dropout: {best_dropout}\")\n",
    "print(f\"best_lr: {best_lr}\")\n",
    "print(f\"best_weight_decay_1: {best_weight_decay_1}\")\n",
    "print(f\"best_weight_decay_2: {best_weight_decay_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7640589e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:42:35.733779Z",
     "iopub.status.busy": "2024-04-24T05:42:35.733504Z",
     "iopub.status.idle": "2024-04-24T05:44:46.640310Z",
     "shell.execute_reply": "2024-04-24T05:44:46.639179Z"
    },
    "papermill": {
     "duration": 130.916484,
     "end_time": "2024-04-24T05:44:46.642912",
     "exception": false,
     "start_time": "2024-04-24T05:42:35.726428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0000 train loss:1.948 acc:13.57 | val loss:1.933 acc:16.20\r\n",
      "Epoch:0001 train loss:1.948 acc:13.57 | val loss:1.935 acc:16.20\r\n",
      "Epoch:0002 train loss:1.943 acc:14.29 | val loss:1.936 acc:16.20\r\n",
      "Epoch:0003 train loss:1.944 acc:14.29 | val loss:1.938 acc:16.20\r\n",
      "Epoch:0004 train loss:1.939 acc:13.57 | val loss:1.939 acc:16.20\r\n",
      "Epoch:0005 train loss:1.937 acc:17.14 | val loss:1.939 acc:16.20\r\n",
      "Epoch:0006 train loss:1.936 acc:13.57 | val loss:1.939 acc:16.20\r\n",
      "Epoch:0007 train loss:1.934 acc:18.57 | val loss:1.938 acc:16.40\r\n",
      "Epoch:0008 train loss:1.926 acc:20.71 | val loss:1.937 acc:16.60\r\n",
      "Epoch:0009 train loss:1.917 acc:26.43 | val loss:1.935 acc:18.00\r\n",
      "Epoch:0010 train loss:1.929 acc:25.71 | val loss:1.932 acc:26.80\r\n",
      "Epoch:0011 train loss:1.926 acc:21.43 | val loss:1.929 acc:38.20\r\n",
      "Epoch:0012 train loss:1.915 acc:31.43 | val loss:1.927 acc:45.40\r\n",
      "Epoch:0013 train loss:1.910 acc:32.86 | val loss:1.924 acc:50.00\r\n",
      "Epoch:0014 train loss:1.908 acc:30.71 | val loss:1.921 acc:51.20\r\n",
      "Epoch:0015 train loss:1.912 acc:30.00 | val loss:1.918 acc:52.40\r\n",
      "Epoch:0016 train loss:1.889 acc:35.00 | val loss:1.915 acc:50.40\r\n",
      "Epoch:0017 train loss:1.907 acc:28.57 | val loss:1.912 acc:53.00\r\n",
      "Epoch:0018 train loss:1.866 acc:40.71 | val loss:1.907 acc:57.80\r\n",
      "Epoch:0019 train loss:2.011 acc:32.14 | val loss:1.902 acc:68.20\r\n",
      "Epoch:0020 train loss:1.894 acc:34.29 | val loss:1.898 acc:72.80\r\n",
      "Epoch:0021 train loss:1.869 acc:43.57 | val loss:1.892 acc:74.80\r\n",
      "Epoch:0022 train loss:1.860 acc:35.71 | val loss:1.887 acc:75.80\r\n",
      "Epoch:0023 train loss:1.857 acc:45.00 | val loss:1.882 acc:75.60\r\n",
      "Epoch:0024 train loss:1.840 acc:42.14 | val loss:1.877 acc:75.20\r\n",
      "Epoch:0025 train loss:1.869 acc:32.86 | val loss:1.872 acc:75.60\r\n",
      "Epoch:0026 train loss:1.877 acc:41.43 | val loss:1.867 acc:74.00\r\n",
      "Epoch:0027 train loss:1.842 acc:35.71 | val loss:1.862 acc:72.80\r\n",
      "Epoch:0028 train loss:1.822 acc:37.86 | val loss:1.856 acc:70.20\r\n",
      "Epoch:0029 train loss:1.843 acc:35.00 | val loss:1.851 acc:68.40\r\n",
      "Epoch:0030 train loss:1.862 acc:38.57 | val loss:1.848 acc:66.40\r\n",
      "Epoch:0031 train loss:1.784 acc:41.43 | val loss:1.842 acc:65.40\r\n",
      "Epoch:0032 train loss:1.819 acc:44.29 | val loss:1.836 acc:59.80\r\n",
      "Epoch:0033 train loss:1.778 acc:45.00 | val loss:1.830 acc:58.40\r\n",
      "Epoch:0034 train loss:1.781 acc:37.14 | val loss:1.825 acc:57.80\r\n",
      "Epoch:0035 train loss:1.795 acc:35.00 | val loss:1.819 acc:58.00\r\n",
      "Epoch:0036 train loss:1.787 acc:45.00 | val loss:1.814 acc:58.40\r\n",
      "Epoch:0037 train loss:1.722 acc:40.71 | val loss:1.807 acc:60.20\r\n",
      "Epoch:0038 train loss:1.806 acc:39.29 | val loss:1.799 acc:60.60\r\n",
      "Epoch:0039 train loss:1.793 acc:35.71 | val loss:1.791 acc:63.00\r\n",
      "Epoch:0040 train loss:1.725 acc:47.14 | val loss:1.781 acc:67.20\r\n",
      "Epoch:0041 train loss:1.741 acc:43.57 | val loss:1.771 acc:70.60\r\n",
      "Epoch:0042 train loss:1.686 acc:51.43 | val loss:1.760 acc:73.60\r\n",
      "Epoch:0043 train loss:1.763 acc:32.14 | val loss:1.752 acc:75.40\r\n",
      "Epoch:0044 train loss:1.702 acc:44.29 | val loss:1.745 acc:75.40\r\n",
      "Epoch:0045 train loss:1.701 acc:38.57 | val loss:1.736 acc:75.80\r\n",
      "Epoch:0046 train loss:1.720 acc:42.86 | val loss:1.728 acc:75.80\r\n",
      "Epoch:0047 train loss:1.697 acc:45.71 | val loss:1.721 acc:75.40\r\n",
      "Epoch:0048 train loss:1.664 acc:50.71 | val loss:1.713 acc:75.60\r\n",
      "Epoch:0049 train loss:1.690 acc:50.71 | val loss:1.704 acc:75.00\r\n",
      "Epoch:0050 train loss:1.654 acc:55.71 | val loss:1.695 acc:74.80\r\n",
      "Epoch:0051 train loss:1.658 acc:50.71 | val loss:1.688 acc:75.40\r\n",
      "Epoch:0052 train loss:1.632 acc:48.57 | val loss:1.681 acc:74.00\r\n",
      "Epoch:0053 train loss:1.593 acc:49.29 | val loss:1.674 acc:73.60\r\n",
      "Epoch:0054 train loss:1.613 acc:44.29 | val loss:1.668 acc:73.40\r\n",
      "Epoch:0055 train loss:1.650 acc:43.57 | val loss:1.661 acc:73.60\r\n",
      "Epoch:0056 train loss:1.582 acc:60.00 | val loss:1.654 acc:74.00\r\n",
      "Epoch:0057 train loss:1.650 acc:50.00 | val loss:1.646 acc:73.40\r\n",
      "Epoch:0058 train loss:1.624 acc:47.14 | val loss:1.639 acc:74.00\r\n",
      "Epoch:0059 train loss:1.603 acc:50.71 | val loss:1.633 acc:74.60\r\n",
      "Epoch:0060 train loss:1.540 acc:50.00 | val loss:1.628 acc:74.20\r\n",
      "Epoch:0061 train loss:1.555 acc:60.71 | val loss:1.622 acc:73.80\r\n",
      "Epoch:0062 train loss:1.575 acc:54.29 | val loss:1.616 acc:74.60\r\n",
      "Epoch:0063 train loss:1.561 acc:51.43 | val loss:1.607 acc:75.40\r\n",
      "Epoch:0064 train loss:1.582 acc:46.43 | val loss:1.598 acc:75.60\r\n",
      "Epoch:0065 train loss:1.500 acc:47.86 | val loss:1.587 acc:76.40\r\n",
      "Epoch:0066 train loss:1.541 acc:55.00 | val loss:1.578 acc:77.80\r\n",
      "Epoch:0067 train loss:1.525 acc:50.71 | val loss:1.569 acc:78.40\r\n",
      "Epoch:0068 train loss:1.465 acc:64.29 | val loss:1.560 acc:78.80\r\n",
      "Epoch:0069 train loss:1.520 acc:60.71 | val loss:1.550 acc:79.20\r\n",
      "Epoch:0070 train loss:1.464 acc:54.29 | val loss:1.542 acc:78.60\r\n",
      "Epoch:0071 train loss:1.533 acc:52.14 | val loss:1.535 acc:78.80\r\n",
      "Epoch:0072 train loss:1.481 acc:59.29 | val loss:1.528 acc:78.60\r\n",
      "Epoch:0073 train loss:1.519 acc:56.43 | val loss:1.520 acc:78.20\r\n",
      "Epoch:0074 train loss:1.510 acc:54.29 | val loss:1.514 acc:77.60\r\n",
      "Epoch:0075 train loss:1.464 acc:57.14 | val loss:1.509 acc:77.40\r\n",
      "Epoch:0076 train loss:1.464 acc:55.00 | val loss:1.504 acc:77.60\r\n",
      "Epoch:0077 train loss:1.437 acc:57.86 | val loss:1.499 acc:75.00\r\n",
      "Epoch:0078 train loss:1.449 acc:49.29 | val loss:1.493 acc:74.60\r\n",
      "Epoch:0079 train loss:1.450 acc:56.43 | val loss:1.486 acc:74.20\r\n",
      "Epoch:0080 train loss:1.462 acc:56.43 | val loss:1.481 acc:75.00\r\n",
      "Epoch:0081 train loss:1.397 acc:60.71 | val loss:1.474 acc:75.60\r\n",
      "Epoch:0082 train loss:1.304 acc:65.71 | val loss:1.466 acc:76.00\r\n",
      "Epoch:0083 train loss:1.367 acc:57.86 | val loss:1.456 acc:77.00\r\n",
      "Epoch:0084 train loss:1.396 acc:61.43 | val loss:1.445 acc:77.40\r\n",
      "Epoch:0085 train loss:1.386 acc:63.57 | val loss:1.434 acc:77.60\r\n",
      "Epoch:0086 train loss:1.403 acc:55.00 | val loss:1.423 acc:78.00\r\n",
      "Epoch:0087 train loss:1.462 acc:50.00 | val loss:1.415 acc:77.80\r\n",
      "Epoch:0088 train loss:1.400 acc:52.86 | val loss:1.411 acc:78.40\r\n",
      "Epoch:0089 train loss:1.355 acc:60.00 | val loss:1.405 acc:78.60\r\n",
      "Epoch:0090 train loss:1.352 acc:63.57 | val loss:1.400 acc:78.60\r\n",
      "Epoch:0091 train loss:1.341 acc:58.57 | val loss:1.395 acc:78.80\r\n",
      "Epoch:0092 train loss:1.337 acc:59.29 | val loss:1.390 acc:79.00\r\n",
      "Epoch:0093 train loss:1.354 acc:60.00 | val loss:1.387 acc:79.00\r\n",
      "Epoch:0094 train loss:1.307 acc:61.43 | val loss:1.384 acc:78.40\r\n",
      "Epoch:0095 train loss:1.384 acc:57.14 | val loss:1.382 acc:78.40\r\n",
      "Epoch:0096 train loss:1.375 acc:58.57 | val loss:1.378 acc:78.60\r\n",
      "Epoch:0097 train loss:1.357 acc:55.00 | val loss:1.374 acc:78.60\r\n",
      "Epoch:0098 train loss:1.335 acc:60.00 | val loss:1.372 acc:79.00\r\n",
      "Epoch:0099 train loss:1.300 acc:64.29 | val loss:1.368 acc:79.40\r\n",
      "Epoch:0100 train loss:1.321 acc:53.57 | val loss:1.364 acc:79.80\r\n",
      "Epoch:0101 train loss:1.311 acc:59.29 | val loss:1.359 acc:79.80\r\n",
      "Epoch:0102 train loss:1.350 acc:60.00 | val loss:1.354 acc:79.00\r\n",
      "Epoch:0103 train loss:1.342 acc:64.29 | val loss:1.349 acc:77.80\r\n",
      "Epoch:0104 train loss:1.334 acc:60.00 | val loss:1.343 acc:78.00\r\n",
      "Epoch:0105 train loss:1.291 acc:66.43 | val loss:1.337 acc:78.20\r\n",
      "Epoch:0106 train loss:1.256 acc:65.71 | val loss:1.331 acc:77.80\r\n",
      "Epoch:0107 train loss:1.279 acc:58.57 | val loss:1.327 acc:78.20\r\n",
      "Epoch:0108 train loss:1.249 acc:62.14 | val loss:1.324 acc:80.00\r\n",
      "Epoch:0109 train loss:1.275 acc:60.71 | val loss:1.321 acc:80.40\r\n",
      "Epoch:0110 train loss:1.303 acc:65.00 | val loss:1.318 acc:80.00\r\n",
      "Epoch:0111 train loss:1.189 acc:70.00 | val loss:1.313 acc:80.20\r\n",
      "Epoch:0112 train loss:1.295 acc:60.71 | val loss:1.307 acc:79.80\r\n",
      "Epoch:0113 train loss:1.335 acc:59.29 | val loss:1.301 acc:79.80\r\n",
      "Epoch:0114 train loss:1.288 acc:61.43 | val loss:1.297 acc:80.40\r\n",
      "Epoch:0115 train loss:1.298 acc:60.00 | val loss:1.294 acc:79.80\r\n",
      "Epoch:0116 train loss:1.264 acc:64.29 | val loss:1.292 acc:79.80\r\n",
      "Epoch:0117 train loss:1.245 acc:62.86 | val loss:1.290 acc:80.00\r\n",
      "Epoch:0118 train loss:1.283 acc:60.00 | val loss:1.287 acc:80.00\r\n",
      "Epoch:0119 train loss:1.282 acc:61.43 | val loss:1.283 acc:79.80\r\n",
      "Epoch:0120 train loss:1.251 acc:57.86 | val loss:1.277 acc:79.80\r\n",
      "Epoch:0121 train loss:1.255 acc:53.57 | val loss:1.270 acc:80.00\r\n",
      "Epoch:0122 train loss:1.258 acc:62.14 | val loss:1.262 acc:80.00\r\n",
      "Epoch:0123 train loss:1.263 acc:63.57 | val loss:1.256 acc:80.20\r\n",
      "Epoch:0124 train loss:1.423 acc:54.29 | val loss:1.253 acc:79.60\r\n",
      "Epoch:0125 train loss:1.254 acc:64.29 | val loss:1.250 acc:80.20\r\n",
      "Epoch:0126 train loss:1.211 acc:65.00 | val loss:1.248 acc:79.80\r\n",
      "Epoch:0127 train loss:1.239 acc:65.00 | val loss:1.249 acc:79.60\r\n",
      "Epoch:0128 train loss:1.166 acc:67.14 | val loss:1.249 acc:80.00\r\n",
      "Epoch:0129 train loss:1.246 acc:65.71 | val loss:1.251 acc:79.20\r\n",
      "Epoch:0130 train loss:1.244 acc:65.71 | val loss:1.250 acc:80.00\r\n",
      "Epoch:0131 train loss:1.188 acc:68.57 | val loss:1.251 acc:80.00\r\n",
      "Epoch:0132 train loss:1.247 acc:60.00 | val loss:1.250 acc:79.80\r\n",
      "Epoch:0133 train loss:1.222 acc:61.43 | val loss:1.248 acc:80.40\r\n",
      "Epoch:0134 train loss:1.167 acc:65.00 | val loss:1.244 acc:80.60\r\n",
      "Epoch:0135 train loss:1.222 acc:67.86 | val loss:1.239 acc:80.40\r\n",
      "Epoch:0136 train loss:1.214 acc:65.71 | val loss:1.235 acc:80.40\r\n",
      "Epoch:0137 train loss:1.283 acc:57.14 | val loss:1.229 acc:81.20\r\n",
      "Epoch:0138 train loss:1.174 acc:70.00 | val loss:1.223 acc:81.40\r\n",
      "Epoch:0139 train loss:1.216 acc:67.86 | val loss:1.219 acc:81.60\r\n",
      "Epoch:0140 train loss:1.078 acc:73.57 | val loss:1.218 acc:81.20\r\n",
      "Epoch:0141 train loss:1.292 acc:69.29 | val loss:1.215 acc:81.40\r\n",
      "Epoch:0142 train loss:1.180 acc:65.71 | val loss:1.210 acc:81.20\r\n",
      "Epoch:0143 train loss:1.174 acc:66.43 | val loss:1.202 acc:81.00\r\n",
      "Epoch:0144 train loss:1.194 acc:67.86 | val loss:1.195 acc:81.40\r\n",
      "Epoch:0145 train loss:1.152 acc:69.29 | val loss:1.188 acc:81.60\r\n",
      "Epoch:0146 train loss:1.210 acc:64.29 | val loss:1.182 acc:81.60\r\n",
      "Epoch:0147 train loss:1.098 acc:67.86 | val loss:1.177 acc:81.20\r\n",
      "Epoch:0148 train loss:1.149 acc:64.29 | val loss:1.174 acc:81.00\r\n",
      "Epoch:0149 train loss:1.163 acc:65.71 | val loss:1.171 acc:80.80\r\n",
      "Epoch:0150 train loss:1.222 acc:65.00 | val loss:1.172 acc:80.80\r\n",
      "Epoch:0151 train loss:1.055 acc:70.71 | val loss:1.172 acc:80.80\r\n",
      "Epoch:0152 train loss:1.153 acc:69.29 | val loss:1.171 acc:80.20\r\n",
      "Epoch:0153 train loss:1.114 acc:65.71 | val loss:1.173 acc:80.80\r\n",
      "Epoch:0154 train loss:1.122 acc:67.86 | val loss:1.174 acc:80.60\r\n",
      "Epoch:0155 train loss:1.059 acc:69.29 | val loss:1.176 acc:81.00\r\n",
      "Epoch:0156 train loss:1.108 acc:67.14 | val loss:1.176 acc:81.00\r\n",
      "Epoch:0157 train loss:1.138 acc:63.57 | val loss:1.175 acc:80.80\r\n",
      "Epoch:0158 train loss:1.301 acc:60.71 | val loss:1.175 acc:81.20\r\n",
      "Epoch:0159 train loss:1.130 acc:64.29 | val loss:1.174 acc:81.20\r\n",
      "Epoch:0160 train loss:1.169 acc:64.29 | val loss:1.172 acc:81.20\r\n",
      "Epoch:0161 train loss:1.159 acc:65.71 | val loss:1.171 acc:81.40\r\n",
      "Epoch:0162 train loss:1.170 acc:63.57 | val loss:1.169 acc:81.00\r\n",
      "Epoch:0163 train loss:1.173 acc:71.43 | val loss:1.169 acc:80.00\r\n",
      "Epoch:0164 train loss:1.020 acc:74.29 | val loss:1.169 acc:80.20\r\n",
      "Epoch:0165 train loss:1.168 acc:64.29 | val loss:1.166 acc:79.40\r\n",
      "Epoch:0166 train loss:1.132 acc:67.86 | val loss:1.165 acc:79.40\r\n",
      "Epoch:0167 train loss:1.143 acc:59.29 | val loss:1.162 acc:79.40\r\n",
      "Epoch:0168 train loss:1.174 acc:65.71 | val loss:1.161 acc:79.80\r\n",
      "Epoch:0169 train loss:1.212 acc:62.14 | val loss:1.161 acc:80.80\r\n",
      "Epoch:0170 train loss:1.065 acc:72.86 | val loss:1.160 acc:81.00\r\n",
      "Epoch:0171 train loss:1.137 acc:65.00 | val loss:1.159 acc:81.60\r\n",
      "Epoch:0172 train loss:1.173 acc:65.71 | val loss:1.159 acc:81.20\r\n",
      "Epoch:0173 train loss:1.136 acc:62.14 | val loss:1.158 acc:81.20\r\n",
      "Epoch:0174 train loss:1.099 acc:67.86 | val loss:1.157 acc:81.60\r\n",
      "Epoch:0175 train loss:1.157 acc:65.00 | val loss:1.155 acc:81.40\r\n",
      "Epoch:0176 train loss:1.061 acc:70.00 | val loss:1.154 acc:81.20\r\n",
      "Epoch:0177 train loss:1.135 acc:64.29 | val loss:1.152 acc:81.40\r\n",
      "Epoch:0178 train loss:1.041 acc:72.86 | val loss:1.147 acc:81.80\r\n",
      "Epoch:0179 train loss:1.111 acc:67.86 | val loss:1.143 acc:81.60\r\n",
      "Epoch:0180 train loss:1.072 acc:69.29 | val loss:1.136 acc:81.60\r\n",
      "Epoch:0181 train loss:1.133 acc:66.43 | val loss:1.131 acc:81.40\r\n",
      "Epoch:0182 train loss:1.177 acc:67.14 | val loss:1.127 acc:81.40\r\n",
      "Epoch:0183 train loss:1.061 acc:68.57 | val loss:1.125 acc:81.00\r\n",
      "Epoch:0184 train loss:1.131 acc:62.86 | val loss:1.123 acc:80.80\r\n",
      "Epoch:0185 train loss:1.084 acc:74.29 | val loss:1.121 acc:81.00\r\n",
      "Epoch:0186 train loss:1.013 acc:72.86 | val loss:1.118 acc:80.80\r\n",
      "Epoch:0187 train loss:1.007 acc:75.71 | val loss:1.114 acc:81.20\r\n",
      "Epoch:0188 train loss:1.033 acc:70.00 | val loss:1.111 acc:80.80\r\n",
      "Epoch:0189 train loss:1.063 acc:73.57 | val loss:1.108 acc:80.40\r\n",
      "Epoch:0190 train loss:1.132 acc:62.86 | val loss:1.105 acc:80.60\r\n",
      "Epoch:0191 train loss:1.101 acc:67.14 | val loss:1.101 acc:81.00\r\n",
      "Epoch:0192 train loss:1.096 acc:65.71 | val loss:1.100 acc:81.40\r\n",
      "Epoch:0193 train loss:1.015 acc:72.86 | val loss:1.098 acc:81.60\r\n",
      "Epoch:0194 train loss:1.119 acc:68.57 | val loss:1.098 acc:81.80\r\n",
      "Epoch:0195 train loss:1.101 acc:60.00 | val loss:1.098 acc:81.80\r\n",
      "Epoch:0196 train loss:1.153 acc:61.43 | val loss:1.096 acc:82.00\r\n",
      "Epoch:0197 train loss:1.235 acc:59.29 | val loss:1.096 acc:81.80\r\n",
      "Epoch:0198 train loss:0.981 acc:73.57 | val loss:1.097 acc:81.20\r\n",
      "Epoch:0199 train loss:1.159 acc:65.00 | val loss:1.097 acc:81.20\r\n",
      "Epoch:0200 train loss:0.991 acc:72.86 | val loss:1.098 acc:80.80\r\n",
      "Epoch:0201 train loss:1.029 acc:65.71 | val loss:1.098 acc:80.80\r\n",
      "Epoch:0202 train loss:1.064 acc:68.57 | val loss:1.100 acc:81.00\r\n",
      "Epoch:0203 train loss:1.119 acc:69.29 | val loss:1.101 acc:80.80\r\n",
      "Epoch:0204 train loss:1.121 acc:68.57 | val loss:1.103 acc:80.00\r\n",
      "Epoch:0205 train loss:1.043 acc:76.43 | val loss:1.101 acc:80.20\r\n",
      "Epoch:0206 train loss:1.462 acc:73.57 | val loss:1.097 acc:80.80\r\n",
      "Epoch:0207 train loss:1.050 acc:72.14 | val loss:1.094 acc:81.00\r\n",
      "Epoch:0208 train loss:1.124 acc:65.71 | val loss:1.092 acc:80.80\r\n",
      "Epoch:0209 train loss:1.081 acc:68.57 | val loss:1.092 acc:81.20\r\n",
      "Epoch:0210 train loss:1.002 acc:73.57 | val loss:1.092 acc:81.00\r\n",
      "Epoch:0211 train loss:1.001 acc:70.71 | val loss:1.091 acc:81.20\r\n",
      "Epoch:0212 train loss:1.064 acc:70.71 | val loss:1.091 acc:81.20\r\n",
      "Epoch:0213 train loss:1.002 acc:72.14 | val loss:1.089 acc:81.20\r\n",
      "Epoch:0214 train loss:1.148 acc:69.29 | val loss:1.086 acc:81.00\r\n",
      "Epoch:0215 train loss:1.081 acc:70.71 | val loss:1.083 acc:81.00\r\n",
      "Epoch:0216 train loss:1.062 acc:63.57 | val loss:1.079 acc:81.40\r\n",
      "Epoch:0217 train loss:1.013 acc:67.14 | val loss:1.076 acc:81.20\r\n",
      "Epoch:0218 train loss:1.079 acc:75.00 | val loss:1.074 acc:80.80\r\n",
      "Epoch:0219 train loss:1.089 acc:67.86 | val loss:1.073 acc:81.20\r\n",
      "Epoch:0220 train loss:1.094 acc:66.43 | val loss:1.075 acc:81.20\r\n",
      "Epoch:0221 train loss:1.004 acc:70.71 | val loss:1.078 acc:81.40\r\n",
      "Epoch:0222 train loss:1.029 acc:72.14 | val loss:1.080 acc:81.60\r\n",
      "Epoch:0223 train loss:1.064 acc:65.71 | val loss:1.081 acc:81.60\r\n",
      "Epoch:0224 train loss:0.988 acc:72.86 | val loss:1.079 acc:81.60\r\n",
      "Epoch:0225 train loss:0.985 acc:72.86 | val loss:1.077 acc:81.40\r\n",
      "Epoch:0226 train loss:1.196 acc:68.57 | val loss:1.076 acc:81.00\r\n",
      "Epoch:0227 train loss:0.995 acc:71.43 | val loss:1.074 acc:81.00\r\n",
      "Epoch:0228 train loss:1.223 acc:70.71 | val loss:1.071 acc:81.00\r\n",
      "Epoch:0229 train loss:1.038 acc:70.71 | val loss:1.068 acc:81.60\r\n",
      "Epoch:0230 train loss:1.035 acc:72.86 | val loss:1.063 acc:81.80\r\n",
      "Epoch:0231 train loss:1.020 acc:72.14 | val loss:1.059 acc:80.60\r\n",
      "Epoch:0232 train loss:1.021 acc:72.14 | val loss:1.056 acc:80.60\r\n",
      "Epoch:0233 train loss:0.988 acc:72.86 | val loss:1.052 acc:80.80\r\n",
      "Epoch:0234 train loss:0.980 acc:71.43 | val loss:1.049 acc:80.40\r\n",
      "Epoch:0235 train loss:1.005 acc:72.14 | val loss:1.047 acc:80.60\r\n",
      "Epoch:0236 train loss:1.019 acc:67.86 | val loss:1.046 acc:80.40\r\n",
      "Epoch:0237 train loss:1.091 acc:67.14 | val loss:1.048 acc:80.60\r\n",
      "Epoch:0238 train loss:1.099 acc:70.00 | val loss:1.047 acc:81.00\r\n",
      "Epoch:0239 train loss:1.010 acc:72.14 | val loss:1.047 acc:80.60\r\n",
      "Epoch:0240 train loss:1.038 acc:67.86 | val loss:1.048 acc:81.00\r\n",
      "Epoch:0241 train loss:1.025 acc:70.71 | val loss:1.049 acc:81.20\r\n",
      "Epoch:0242 train loss:0.922 acc:75.00 | val loss:1.049 acc:81.20\r\n",
      "Epoch:0243 train loss:0.968 acc:77.14 | val loss:1.048 acc:81.00\r\n",
      "Epoch:0244 train loss:0.940 acc:79.29 | val loss:1.048 acc:81.20\r\n",
      "Epoch:0245 train loss:1.044 acc:67.14 | val loss:1.047 acc:81.40\r\n",
      "Epoch:0246 train loss:0.976 acc:72.14 | val loss:1.044 acc:81.00\r\n",
      "Epoch:0247 train loss:1.022 acc:74.29 | val loss:1.042 acc:80.60\r\n",
      "Epoch:0248 train loss:0.992 acc:72.14 | val loss:1.043 acc:80.60\r\n",
      "Epoch:0249 train loss:1.006 acc:67.14 | val loss:1.042 acc:80.40\r\n",
      "Epoch:0250 train loss:1.045 acc:73.57 | val loss:1.040 acc:80.60\r\n",
      "Epoch:0251 train loss:1.041 acc:70.00 | val loss:1.036 acc:80.80\r\n",
      "Epoch:0252 train loss:0.952 acc:78.57 | val loss:1.031 acc:81.00\r\n",
      "Epoch:0253 train loss:0.999 acc:72.86 | val loss:1.027 acc:81.40\r\n",
      "Epoch:0254 train loss:0.995 acc:69.29 | val loss:1.023 acc:81.20\r\n",
      "Epoch:0255 train loss:1.189 acc:70.00 | val loss:1.019 acc:81.60\r\n",
      "Epoch:0256 train loss:0.967 acc:72.14 | val loss:1.015 acc:81.00\r\n",
      "Epoch:0257 train loss:0.910 acc:72.86 | val loss:1.012 acc:81.00\r\n",
      "Epoch:0258 train loss:0.972 acc:70.71 | val loss:1.010 acc:80.80\r\n",
      "Epoch:0259 train loss:0.891 acc:72.86 | val loss:1.007 acc:80.80\r\n",
      "Epoch:0260 train loss:1.042 acc:69.29 | val loss:1.005 acc:81.20\r\n",
      "Epoch:0261 train loss:0.973 acc:72.14 | val loss:1.004 acc:81.20\r\n",
      "Epoch:0262 train loss:0.977 acc:70.71 | val loss:1.005 acc:81.20\r\n",
      "Epoch:0263 train loss:0.996 acc:67.86 | val loss:1.007 acc:81.40\r\n",
      "Epoch:0264 train loss:1.057 acc:67.86 | val loss:1.010 acc:81.40\r\n",
      "Epoch:0265 train loss:0.979 acc:72.86 | val loss:1.012 acc:80.60\r\n",
      "Epoch:0266 train loss:0.963 acc:74.29 | val loss:1.011 acc:80.40\r\n",
      "Epoch:0267 train loss:0.895 acc:75.71 | val loss:1.012 acc:80.60\r\n",
      "Epoch:0268 train loss:0.977 acc:70.71 | val loss:1.012 acc:80.80\r\n",
      "Epoch:0269 train loss:0.909 acc:78.57 | val loss:1.012 acc:80.60\r\n",
      "Epoch:0270 train loss:1.050 acc:67.14 | val loss:1.012 acc:81.00\r\n",
      "Epoch:0271 train loss:1.159 acc:75.71 | val loss:1.014 acc:81.00\r\n",
      "Epoch:0272 train loss:0.890 acc:78.57 | val loss:1.017 acc:81.80\r\n",
      "Epoch:0273 train loss:0.968 acc:74.29 | val loss:1.020 acc:82.00\r\n",
      "Epoch:0274 train loss:0.955 acc:73.57 | val loss:1.021 acc:82.20\r\n",
      "Epoch:0275 train loss:0.928 acc:71.43 | val loss:1.019 acc:82.20\r\n",
      "Epoch:0276 train loss:1.026 acc:68.57 | val loss:1.021 acc:81.80\r\n",
      "Epoch:0277 train loss:1.134 acc:70.71 | val loss:1.019 acc:81.40\r\n",
      "Epoch:0278 train loss:0.906 acc:79.29 | val loss:1.016 acc:81.60\r\n",
      "Epoch:0279 train loss:0.878 acc:75.71 | val loss:1.012 acc:81.60\r\n",
      "Epoch:0280 train loss:0.916 acc:79.29 | val loss:1.007 acc:81.60\r\n",
      "Epoch:0281 train loss:0.890 acc:78.57 | val loss:1.003 acc:81.60\r\n",
      "Epoch:0282 train loss:0.944 acc:74.29 | val loss:0.998 acc:81.80\r\n",
      "Epoch:0283 train loss:0.954 acc:75.71 | val loss:0.992 acc:82.20\r\n",
      "Epoch:0284 train loss:1.035 acc:67.86 | val loss:0.989 acc:82.20\r\n",
      "Epoch:0285 train loss:0.940 acc:70.71 | val loss:0.985 acc:82.60\r\n",
      "Epoch:0286 train loss:0.953 acc:73.57 | val loss:0.983 acc:82.60\r\n",
      "Epoch:0287 train loss:0.965 acc:73.57 | val loss:0.981 acc:82.20\r\n",
      "Epoch:0288 train loss:0.918 acc:72.86 | val loss:0.983 acc:81.60\r\n",
      "Epoch:0289 train loss:0.967 acc:76.43 | val loss:0.984 acc:81.40\r\n",
      "Epoch:0290 train loss:1.043 acc:70.71 | val loss:0.983 acc:81.80\r\n",
      "Epoch:0291 train loss:0.992 acc:70.00 | val loss:0.983 acc:81.60\r\n",
      "Epoch:0292 train loss:1.062 acc:62.14 | val loss:0.985 acc:81.40\r\n",
      "Epoch:0293 train loss:0.965 acc:70.00 | val loss:0.987 acc:81.60\r\n",
      "Epoch:0294 train loss:0.936 acc:75.00 | val loss:0.988 acc:81.80\r\n",
      "Epoch:0295 train loss:0.993 acc:77.14 | val loss:0.989 acc:81.40\r\n",
      "Epoch:0296 train loss:0.909 acc:75.00 | val loss:0.987 acc:80.60\r\n",
      "Epoch:0297 train loss:0.850 acc:78.57 | val loss:0.984 acc:81.60\r\n",
      "Epoch:0298 train loss:0.962 acc:75.71 | val loss:0.981 acc:82.20\r\n",
      "Epoch:0299 train loss:0.889 acc:78.57 | val loss:0.977 acc:82.20\r\n",
      "Epoch:0300 train loss:0.922 acc:77.14 | val loss:0.973 acc:81.80\r\n",
      "Epoch:0301 train loss:1.045 acc:69.29 | val loss:0.969 acc:81.40\r\n",
      "Epoch:0302 train loss:0.930 acc:73.57 | val loss:0.969 acc:81.40\r\n",
      "Epoch:0303 train loss:1.051 acc:67.14 | val loss:0.970 acc:82.20\r\n",
      "Epoch:0304 train loss:0.867 acc:79.29 | val loss:0.972 acc:82.40\r\n",
      "Epoch:0305 train loss:0.934 acc:72.86 | val loss:0.972 acc:82.60\r\n",
      "Epoch:0306 train loss:1.071 acc:71.43 | val loss:0.974 acc:82.00\r\n",
      "Epoch:0307 train loss:1.151 acc:69.29 | val loss:0.978 acc:82.20\r\n",
      "Epoch:0308 train loss:0.936 acc:74.29 | val loss:0.981 acc:82.80\r\n",
      "Epoch:0309 train loss:0.966 acc:73.57 | val loss:0.982 acc:82.60\r\n",
      "Epoch:0310 train loss:0.889 acc:77.14 | val loss:0.984 acc:82.80\r\n",
      "Epoch:0311 train loss:1.025 acc:74.29 | val loss:0.983 acc:82.60\r\n",
      "Epoch:0312 train loss:0.996 acc:76.43 | val loss:0.982 acc:82.40\r\n",
      "Epoch:0313 train loss:0.921 acc:77.14 | val loss:0.980 acc:81.40\r\n",
      "Epoch:0314 train loss:0.968 acc:75.71 | val loss:0.981 acc:81.60\r\n",
      "Epoch:0315 train loss:0.991 acc:70.71 | val loss:0.982 acc:81.40\r\n",
      "Epoch:0316 train loss:0.960 acc:75.00 | val loss:0.983 acc:81.20\r\n",
      "Epoch:0317 train loss:1.413 acc:80.00 | val loss:0.983 acc:80.60\r\n",
      "Epoch:0318 train loss:0.870 acc:78.57 | val loss:0.982 acc:80.80\r\n",
      "Epoch:0319 train loss:0.969 acc:75.00 | val loss:0.981 acc:81.00\r\n",
      "Epoch:0320 train loss:0.961 acc:74.29 | val loss:0.982 acc:81.20\r\n",
      "Epoch:0321 train loss:1.019 acc:72.14 | val loss:0.984 acc:81.00\r\n",
      "Epoch:0322 train loss:0.948 acc:72.14 | val loss:0.986 acc:81.40\r\n",
      "Epoch:0323 train loss:0.922 acc:72.86 | val loss:0.986 acc:81.40\r\n",
      "Epoch:0324 train loss:0.875 acc:78.57 | val loss:0.984 acc:81.40\r\n",
      "Epoch:0325 train loss:0.992 acc:70.00 | val loss:0.982 acc:81.40\r\n",
      "Epoch:0326 train loss:0.884 acc:78.57 | val loss:0.979 acc:81.60\r\n",
      "Epoch:0327 train loss:0.905 acc:75.71 | val loss:0.975 acc:81.60\r\n",
      "Epoch:0328 train loss:0.892 acc:75.71 | val loss:0.969 acc:81.40\r\n",
      "Epoch:0329 train loss:1.545 acc:70.00 | val loss:0.965 acc:81.20\r\n",
      "Epoch:0330 train loss:0.894 acc:76.43 | val loss:0.961 acc:81.20\r\n",
      "Epoch:0331 train loss:0.982 acc:71.43 | val loss:0.960 acc:81.40\r\n",
      "Epoch:0332 train loss:0.993 acc:65.71 | val loss:0.959 acc:81.40\r\n",
      "Epoch:0333 train loss:0.996 acc:70.00 | val loss:0.958 acc:81.60\r\n",
      "Epoch:0334 train loss:0.910 acc:78.57 | val loss:0.959 acc:81.60\r\n",
      "Epoch:0335 train loss:0.963 acc:75.71 | val loss:0.963 acc:81.60\r\n",
      "Epoch:0336 train loss:0.865 acc:77.86 | val loss:0.965 acc:81.80\r\n",
      "Epoch:0337 train loss:0.973 acc:73.57 | val loss:0.969 acc:81.40\r\n",
      "Epoch:0338 train loss:0.898 acc:75.00 | val loss:0.972 acc:82.20\r\n",
      "Epoch:0339 train loss:0.963 acc:72.86 | val loss:0.974 acc:82.20\r\n",
      "Epoch:0340 train loss:0.907 acc:78.57 | val loss:0.974 acc:81.60\r\n",
      "Epoch:0341 train loss:0.928 acc:77.14 | val loss:0.975 acc:81.60\r\n",
      "Epoch:0342 train loss:0.954 acc:70.00 | val loss:0.976 acc:81.20\r\n",
      "Epoch:0343 train loss:0.968 acc:73.57 | val loss:0.977 acc:81.20\r\n",
      "Epoch:0344 train loss:0.923 acc:74.29 | val loss:0.977 acc:81.00\r\n",
      "Epoch:0345 train loss:0.912 acc:75.71 | val loss:0.974 acc:81.20\r\n",
      "Epoch:0346 train loss:1.005 acc:77.86 | val loss:0.970 acc:81.80\r\n",
      "Epoch:0347 train loss:0.975 acc:67.14 | val loss:0.965 acc:82.60\r\n",
      "Epoch:0348 train loss:0.926 acc:74.29 | val loss:0.961 acc:83.00\r\n",
      "Epoch:0349 train loss:0.875 acc:77.14 | val loss:0.959 acc:82.40\r\n",
      "Epoch:0350 train loss:0.948 acc:77.14 | val loss:0.956 acc:82.60\r\n",
      "Epoch:0351 train loss:0.873 acc:73.57 | val loss:0.953 acc:82.60\r\n",
      "Epoch:0352 train loss:0.887 acc:77.14 | val loss:0.951 acc:83.00\r\n",
      "Epoch:0353 train loss:0.930 acc:73.57 | val loss:0.950 acc:82.80\r\n",
      "Epoch:0354 train loss:0.917 acc:74.29 | val loss:0.954 acc:82.80\r\n",
      "Epoch:0355 train loss:0.889 acc:80.00 | val loss:0.959 acc:82.20\r\n",
      "Epoch:0356 train loss:0.901 acc:72.14 | val loss:0.961 acc:82.40\r\n",
      "Epoch:0357 train loss:1.096 acc:69.29 | val loss:0.969 acc:82.40\r\n",
      "Epoch:0358 train loss:0.900 acc:80.00 | val loss:0.974 acc:82.00\r\n",
      "Epoch:0359 train loss:0.870 acc:81.43 | val loss:0.976 acc:81.80\r\n",
      "Epoch:0360 train loss:0.854 acc:78.57 | val loss:0.975 acc:81.60\r\n",
      "Epoch:0361 train loss:0.839 acc:76.43 | val loss:0.972 acc:81.40\r\n",
      "Epoch:0362 train loss:0.987 acc:75.00 | val loss:0.973 acc:81.00\r\n",
      "Epoch:0363 train loss:0.870 acc:73.57 | val loss:0.971 acc:81.00\r\n",
      "Epoch:0364 train loss:0.958 acc:75.00 | val loss:0.969 acc:81.00\r\n",
      "Epoch:0365 train loss:0.911 acc:71.43 | val loss:0.969 acc:81.00\r\n",
      "Epoch:0366 train loss:0.923 acc:72.14 | val loss:0.967 acc:81.60\r\n",
      "Epoch:0367 train loss:0.955 acc:72.86 | val loss:0.965 acc:81.60\r\n",
      "Epoch:0368 train loss:0.910 acc:72.86 | val loss:0.959 acc:82.00\r\n",
      "Epoch:0369 train loss:0.919 acc:77.14 | val loss:0.950 acc:82.60\r\n",
      "Epoch:0370 train loss:0.813 acc:79.29 | val loss:0.942 acc:81.60\r\n",
      "Epoch:0371 train loss:0.940 acc:72.14 | val loss:0.935 acc:81.00\r\n",
      "Epoch:0372 train loss:0.928 acc:72.14 | val loss:0.930 acc:81.80\r\n",
      "Epoch:0373 train loss:0.840 acc:78.57 | val loss:0.925 acc:82.20\r\n",
      "Epoch:0374 train loss:0.913 acc:78.57 | val loss:0.921 acc:81.60\r\n",
      "Epoch:0375 train loss:0.812 acc:77.86 | val loss:0.918 acc:82.00\r\n",
      "Epoch:0376 train loss:0.911 acc:70.71 | val loss:0.919 acc:81.80\r\n",
      "Epoch:0377 train loss:0.816 acc:77.14 | val loss:0.919 acc:81.60\r\n",
      "Epoch:0378 train loss:1.041 acc:67.86 | val loss:0.921 acc:82.00\r\n",
      "Epoch:0379 train loss:0.946 acc:71.43 | val loss:0.925 acc:82.40\r\n",
      "Epoch:0380 train loss:0.908 acc:74.29 | val loss:0.929 acc:82.80\r\n",
      "Epoch:0381 train loss:0.879 acc:72.86 | val loss:0.933 acc:82.60\r\n",
      "Epoch:0382 train loss:0.859 acc:73.57 | val loss:0.937 acc:82.80\r\n",
      "Epoch:0383 train loss:0.911 acc:75.71 | val loss:0.940 acc:82.60\r\n",
      "Epoch:0384 train loss:1.021 acc:70.71 | val loss:0.941 acc:82.40\r\n",
      "Epoch:0385 train loss:7.401 acc:75.71 | val loss:0.943 acc:83.00\r\n",
      "Epoch:0386 train loss:0.853 acc:81.43 | val loss:0.944 acc:82.80\r\n",
      "Epoch:0387 train loss:0.913 acc:77.86 | val loss:0.946 acc:82.20\r\n",
      "Epoch:0388 train loss:0.960 acc:76.43 | val loss:0.944 acc:82.20\r\n",
      "Epoch:0389 train loss:0.893 acc:75.71 | val loss:0.941 acc:82.20\r\n",
      "Epoch:0390 train loss:0.903 acc:72.14 | val loss:0.937 acc:81.80\r\n",
      "Epoch:0391 train loss:1.213 acc:68.57 | val loss:0.933 acc:81.80\r\n",
      "Epoch:0392 train loss:0.935 acc:76.43 | val loss:0.929 acc:81.80\r\n",
      "Epoch:0393 train loss:0.887 acc:75.00 | val loss:0.927 acc:82.00\r\n",
      "Epoch:0394 train loss:0.841 acc:78.57 | val loss:0.925 acc:81.60\r\n",
      "Epoch:0395 train loss:0.960 acc:75.00 | val loss:0.924 acc:81.80\r\n",
      "Epoch:0396 train loss:0.882 acc:72.14 | val loss:0.924 acc:82.00\r\n",
      "Epoch:0397 train loss:0.869 acc:76.43 | val loss:0.923 acc:82.20\r\n",
      "Epoch:0398 train loss:0.897 acc:73.57 | val loss:0.921 acc:82.20\r\n",
      "Epoch:0399 train loss:0.889 acc:78.57 | val loss:0.919 acc:82.80\r\n",
      "Epoch:0400 train loss:0.881 acc:80.71 | val loss:0.918 acc:82.40\r\n",
      "Epoch:0401 train loss:0.836 acc:78.57 | val loss:0.918 acc:82.60\r\n",
      "Epoch:0402 train loss:0.835 acc:79.29 | val loss:0.919 acc:82.40\r\n",
      "Epoch:0403 train loss:0.876 acc:77.86 | val loss:0.920 acc:81.80\r\n",
      "Epoch:0404 train loss:0.979 acc:73.57 | val loss:0.920 acc:82.00\r\n",
      "Epoch:0405 train loss:0.914 acc:71.43 | val loss:0.920 acc:82.60\r\n",
      "Epoch:0406 train loss:0.843 acc:76.43 | val loss:0.922 acc:82.20\r\n",
      "Epoch:0407 train loss:1.030 acc:67.86 | val loss:0.924 acc:82.80\r\n",
      "Epoch:0408 train loss:0.861 acc:79.29 | val loss:0.927 acc:82.40\r\n",
      "Epoch:0409 train loss:0.749 acc:81.43 | val loss:0.928 acc:82.20\r\n",
      "Epoch:0410 train loss:0.863 acc:75.00 | val loss:0.930 acc:82.40\r\n",
      "Epoch:0411 train loss:0.953 acc:71.43 | val loss:0.932 acc:82.20\r\n",
      "Epoch:0412 train loss:0.901 acc:77.86 | val loss:0.933 acc:82.00\r\n",
      "Epoch:0413 train loss:0.897 acc:76.43 | val loss:0.937 acc:82.00\r\n",
      "Epoch:0414 train loss:0.914 acc:75.71 | val loss:0.941 acc:82.00\r\n",
      "Epoch:0415 train loss:1.008 acc:77.86 | val loss:0.949 acc:81.80\r\n",
      "Epoch:0416 train loss:0.877 acc:77.86 | val loss:0.958 acc:81.80\r\n",
      "Epoch:0417 train loss:0.845 acc:76.43 | val loss:0.965 acc:81.80\r\n",
      "Epoch:0418 train loss:0.879 acc:74.29 | val loss:0.970 acc:81.80\r\n",
      "Epoch:0419 train loss:0.932 acc:69.29 | val loss:0.971 acc:81.60\r\n",
      "Epoch:0420 train loss:0.898 acc:75.71 | val loss:0.970 acc:81.60\r\n",
      "Epoch:0421 train loss:0.890 acc:76.43 | val loss:0.967 acc:81.60\r\n",
      "Epoch:0422 train loss:0.905 acc:77.14 | val loss:0.963 acc:81.80\r\n",
      "Epoch:0423 train loss:0.946 acc:70.00 | val loss:0.958 acc:82.00\r\n",
      "Epoch:0424 train loss:0.952 acc:80.00 | val loss:0.954 acc:82.20\r\n",
      "Epoch:0425 train loss:0.932 acc:77.14 | val loss:0.953 acc:82.20\r\n",
      "Epoch:0426 train loss:0.995 acc:72.14 | val loss:0.952 acc:82.20\r\n",
      "Epoch:0427 train loss:0.941 acc:74.29 | val loss:0.951 acc:81.80\r\n",
      "Epoch:0428 train loss:0.950 acc:70.00 | val loss:0.954 acc:81.20\r\n",
      "Epoch:0429 train loss:0.856 acc:74.29 | val loss:0.955 acc:81.00\r\n",
      "Epoch:0430 train loss:0.857 acc:79.29 | val loss:0.957 acc:81.00\r\n",
      "Epoch:0431 train loss:0.917 acc:76.43 | val loss:0.956 acc:81.40\r\n",
      "Epoch:0432 train loss:0.879 acc:73.57 | val loss:0.954 acc:82.00\r\n",
      "Epoch:0433 train loss:0.905 acc:73.57 | val loss:0.950 acc:82.20\r\n",
      "Epoch:0434 train loss:0.945 acc:77.14 | val loss:0.946 acc:82.60\r\n",
      "Epoch:0435 train loss:0.933 acc:75.71 | val loss:0.944 acc:82.80\r\n",
      "Epoch:0436 train loss:0.879 acc:77.86 | val loss:0.942 acc:82.80\r\n",
      "Epoch:0437 train loss:0.898 acc:75.71 | val loss:0.939 acc:83.20\r\n",
      "Epoch:0438 train loss:0.888 acc:75.00 | val loss:0.937 acc:83.00\r\n",
      "Epoch:0439 train loss:0.844 acc:79.29 | val loss:0.936 acc:82.80\r\n",
      "Epoch:0440 train loss:0.899 acc:79.29 | val loss:0.934 acc:82.80\r\n",
      "Epoch:0441 train loss:0.994 acc:72.86 | val loss:0.934 acc:82.20\r\n",
      "Epoch:0442 train loss:0.938 acc:76.43 | val loss:0.934 acc:82.00\r\n",
      "Epoch:0443 train loss:0.913 acc:80.00 | val loss:0.932 acc:81.80\r\n",
      "Epoch:0444 train loss:0.816 acc:77.86 | val loss:0.930 acc:81.80\r\n",
      "Epoch:0445 train loss:0.812 acc:80.00 | val loss:0.930 acc:82.00\r\n",
      "Epoch:0446 train loss:0.931 acc:74.29 | val loss:0.929 acc:82.00\r\n",
      "Epoch:0447 train loss:0.935 acc:80.00 | val loss:0.928 acc:81.80\r\n",
      "Epoch:0448 train loss:0.943 acc:75.71 | val loss:0.927 acc:81.80\r\n",
      "Epoch:0449 train loss:0.843 acc:78.57 | val loss:0.926 acc:82.00\r\n",
      "Epoch:0450 train loss:0.802 acc:77.14 | val loss:0.927 acc:82.60\r\n",
      "Epoch:0451 train loss:0.866 acc:78.57 | val loss:0.930 acc:82.20\r\n",
      "Epoch:0452 train loss:0.891 acc:74.29 | val loss:0.933 acc:82.20\r\n",
      "Epoch:0453 train loss:0.836 acc:77.86 | val loss:0.934 acc:82.40\r\n",
      "Epoch:0454 train loss:0.871 acc:78.57 | val loss:0.932 acc:82.40\r\n",
      "Epoch:0455 train loss:0.789 acc:81.43 | val loss:0.928 acc:82.20\r\n",
      "Epoch:0456 train loss:0.859 acc:71.43 | val loss:0.922 acc:82.40\r\n",
      "Epoch:0457 train loss:0.878 acc:72.14 | val loss:0.917 acc:82.80\r\n",
      "Epoch:0458 train loss:0.899 acc:70.71 | val loss:0.913 acc:82.80\r\n",
      "Epoch:0459 train loss:0.774 acc:82.14 | val loss:0.907 acc:82.80\r\n",
      "Epoch:0460 train loss:1.063 acc:65.71 | val loss:0.906 acc:82.40\r\n",
      "Epoch:0461 train loss:1.006 acc:70.00 | val loss:0.904 acc:81.60\r\n",
      "Epoch:0462 train loss:0.948 acc:71.43 | val loss:0.903 acc:82.00\r\n",
      "Epoch:0463 train loss:0.824 acc:74.29 | val loss:0.905 acc:81.80\r\n",
      "Epoch:0464 train loss:0.888 acc:77.14 | val loss:0.905 acc:81.80\r\n",
      "Epoch:0465 train loss:0.869 acc:78.57 | val loss:0.905 acc:81.60\r\n",
      "Epoch:0466 train loss:0.829 acc:82.14 | val loss:0.906 acc:81.60\r\n",
      "Epoch:0467 train loss:0.853 acc:75.00 | val loss:0.909 acc:82.20\r\n",
      "Epoch:0468 train loss:0.854 acc:77.86 | val loss:0.912 acc:82.40\r\n",
      "Epoch:0469 train loss:0.891 acc:81.43 | val loss:0.914 acc:82.80\r\n",
      "Epoch:0470 train loss:0.830 acc:78.57 | val loss:0.917 acc:83.20\r\n",
      "Epoch:0471 train loss:0.786 acc:81.43 | val loss:0.921 acc:83.40\r\n",
      "Epoch:0472 train loss:0.837 acc:73.57 | val loss:0.924 acc:82.80\r\n",
      "Epoch:0473 train loss:0.840 acc:77.86 | val loss:0.925 acc:82.60\r\n",
      "Epoch:0474 train loss:0.847 acc:73.57 | val loss:0.923 acc:82.60\r\n",
      "Epoch:0475 train loss:0.848 acc:78.57 | val loss:0.919 acc:82.20\r\n",
      "Epoch:0476 train loss:0.895 acc:75.71 | val loss:0.917 acc:82.00\r\n",
      "Epoch:0477 train loss:0.795 acc:77.14 | val loss:0.915 acc:82.00\r\n",
      "Epoch:0478 train loss:0.807 acc:82.14 | val loss:0.911 acc:82.40\r\n",
      "Epoch:0479 train loss:0.912 acc:74.29 | val loss:0.908 acc:82.60\r\n",
      "Epoch:0480 train loss:0.842 acc:77.86 | val loss:0.907 acc:82.20\r\n",
      "Epoch:0481 train loss:0.826 acc:76.43 | val loss:0.906 acc:82.00\r\n",
      "Epoch:0482 train loss:0.790 acc:80.71 | val loss:0.904 acc:81.80\r\n",
      "Epoch:0483 train loss:0.795 acc:82.14 | val loss:0.901 acc:82.40\r\n",
      "Epoch:0484 train loss:0.873 acc:78.57 | val loss:0.898 acc:82.20\r\n",
      "Epoch:0485 train loss:0.940 acc:72.14 | val loss:0.898 acc:82.20\r\n",
      "Epoch:0486 train loss:0.877 acc:77.14 | val loss:0.899 acc:82.00\r\n",
      "Epoch:0487 train loss:0.903 acc:78.57 | val loss:0.900 acc:81.40\r\n",
      "Epoch:0488 train loss:0.894 acc:70.71 | val loss:0.902 acc:81.40\r\n",
      "Epoch:0489 train loss:0.833 acc:76.43 | val loss:0.905 acc:81.40\r\n",
      "Epoch:0490 train loss:0.837 acc:76.43 | val loss:0.907 acc:81.60\r\n",
      "Epoch:0491 train loss:0.779 acc:82.86 | val loss:0.910 acc:81.60\r\n",
      "Epoch:0492 train loss:0.845 acc:81.43 | val loss:0.912 acc:81.80\r\n",
      "Epoch:0493 train loss:0.909 acc:78.57 | val loss:0.911 acc:82.00\r\n",
      "Epoch:0494 train loss:0.833 acc:78.57 | val loss:0.907 acc:81.80\r\n",
      "Epoch:0495 train loss:0.744 acc:80.71 | val loss:0.904 acc:81.80\r\n",
      "Epoch:0496 train loss:0.825 acc:82.14 | val loss:0.901 acc:81.60\r\n",
      "Epoch:0497 train loss:0.939 acc:75.00 | val loss:0.901 acc:82.00\r\n",
      "Epoch:0498 train loss:0.803 acc:76.43 | val loss:0.902 acc:82.00\r\n",
      "Epoch:0499 train loss:0.841 acc:77.14 | val loss:0.901 acc:81.80\r\n",
      "Epoch:0500 train loss:0.869 acc:80.00 | val loss:0.896 acc:81.80\r\n",
      "Epoch:0501 train loss:0.890 acc:74.29 | val loss:0.894 acc:81.80\r\n",
      "Epoch:0502 train loss:0.992 acc:80.71 | val loss:0.897 acc:81.80\r\n",
      "Epoch:0503 train loss:0.846 acc:74.29 | val loss:0.900 acc:82.00\r\n",
      "Epoch:0504 train loss:0.772 acc:80.71 | val loss:0.904 acc:81.80\r\n",
      "Epoch:0505 train loss:0.865 acc:80.71 | val loss:0.908 acc:82.20\r\n",
      "Epoch:0506 train loss:0.839 acc:78.57 | val loss:0.911 acc:81.80\r\n",
      "Epoch:0507 train loss:0.760 acc:77.14 | val loss:0.911 acc:81.80\r\n",
      "Epoch:0508 train loss:0.834 acc:79.29 | val loss:0.911 acc:81.60\r\n",
      "Epoch:0509 train loss:0.862 acc:74.29 | val loss:0.910 acc:81.80\r\n",
      "Epoch:0510 train loss:0.952 acc:76.43 | val loss:0.911 acc:82.00\r\n",
      "Epoch:0511 train loss:0.853 acc:82.14 | val loss:0.911 acc:81.40\r\n",
      "Epoch:0512 train loss:0.834 acc:77.14 | val loss:0.913 acc:81.60\r\n",
      "Epoch:0513 train loss:0.978 acc:67.86 | val loss:0.915 acc:81.80\r\n",
      "Epoch:0514 train loss:0.873 acc:77.86 | val loss:0.918 acc:81.80\r\n",
      "Epoch:0515 train loss:0.891 acc:75.71 | val loss:0.921 acc:81.20\r\n",
      "Epoch:0516 train loss:0.844 acc:77.86 | val loss:0.922 acc:81.40\r\n",
      "Epoch:0517 train loss:0.785 acc:82.14 | val loss:0.921 acc:81.80\r\n",
      "Epoch:0518 train loss:0.854 acc:73.57 | val loss:0.919 acc:82.20\r\n",
      "Epoch:0519 train loss:0.836 acc:80.00 | val loss:0.916 acc:82.00\r\n",
      "Epoch:0520 train loss:0.805 acc:81.43 | val loss:0.915 acc:81.80\r\n",
      "Epoch:0521 train loss:0.972 acc:73.57 | val loss:0.914 acc:82.00\r\n",
      "Epoch:0522 train loss:0.846 acc:80.00 | val loss:0.913 acc:82.20\r\n",
      "Epoch:0523 train loss:0.828 acc:76.43 | val loss:0.911 acc:82.40\r\n",
      "Epoch:0524 train loss:0.855 acc:72.86 | val loss:0.912 acc:82.60\r\n",
      "Epoch:0525 train loss:0.813 acc:82.14 | val loss:0.912 acc:82.60\r\n",
      "Epoch:0526 train loss:0.862 acc:81.43 | val loss:0.914 acc:83.00\r\n",
      "Epoch:0527 train loss:0.912 acc:72.14 | val loss:0.914 acc:83.20\r\n",
      "Epoch:0528 train loss:0.881 acc:75.71 | val loss:0.916 acc:83.40\r\n",
      "Epoch:0529 train loss:0.824 acc:79.29 | val loss:0.915 acc:83.20\r\n",
      "Epoch:0530 train loss:0.900 acc:79.29 | val loss:0.911 acc:83.00\r\n",
      "Epoch:0531 train loss:0.807 acc:78.57 | val loss:0.904 acc:83.00\r\n",
      "Epoch:0532 train loss:0.843 acc:75.00 | val loss:0.898 acc:82.80\r\n",
      "Epoch:0533 train loss:0.892 acc:75.00 | val loss:0.889 acc:82.20\r\n",
      "Epoch:0534 train loss:0.822 acc:77.14 | val loss:0.882 acc:82.60\r\n",
      "Epoch:0535 train loss:0.843 acc:77.14 | val loss:0.878 acc:82.20\r\n",
      "Epoch:0536 train loss:0.835 acc:75.71 | val loss:0.875 acc:82.00\r\n",
      "Epoch:0537 train loss:0.853 acc:74.29 | val loss:0.873 acc:82.20\r\n",
      "Epoch:0538 train loss:0.851 acc:74.29 | val loss:0.872 acc:82.00\r\n",
      "Epoch:0539 train loss:0.934 acc:77.86 | val loss:0.873 acc:82.00\r\n",
      "Epoch:0540 train loss:0.790 acc:80.00 | val loss:0.875 acc:82.60\r\n",
      "Epoch:0541 train loss:0.827 acc:79.29 | val loss:0.880 acc:83.00\r\n",
      "Epoch:0542 train loss:0.763 acc:82.86 | val loss:0.885 acc:83.00\r\n",
      "Epoch:0543 train loss:0.785 acc:80.71 | val loss:0.890 acc:83.00\r\n",
      "Epoch:0544 train loss:0.790 acc:75.71 | val loss:0.893 acc:83.20\r\n",
      "Epoch:0545 train loss:0.771 acc:81.43 | val loss:0.895 acc:82.20\r\n",
      "Epoch:0546 train loss:0.853 acc:68.57 | val loss:0.895 acc:82.40\r\n",
      "Epoch:0547 train loss:0.830 acc:80.71 | val loss:0.892 acc:82.80\r\n",
      "Epoch:0548 train loss:0.902 acc:77.86 | val loss:0.888 acc:82.60\r\n",
      "Epoch:0549 train loss:0.816 acc:77.14 | val loss:0.884 acc:82.40\r\n",
      "Epoch:0550 train loss:0.866 acc:75.71 | val loss:0.882 acc:82.40\r\n",
      "Epoch:0551 train loss:0.848 acc:73.57 | val loss:0.880 acc:82.00\r\n",
      "Epoch:0552 train loss:0.885 acc:76.43 | val loss:0.879 acc:81.80\r\n",
      "Epoch:0553 train loss:0.854 acc:76.43 | val loss:0.879 acc:82.00\r\n",
      "Epoch:0554 train loss:0.838 acc:72.14 | val loss:0.879 acc:82.00\r\n",
      "Epoch:0555 train loss:0.919 acc:68.57 | val loss:0.879 acc:82.20\r\n",
      "Epoch:0556 train loss:0.886 acc:78.57 | val loss:0.882 acc:82.40\r\n",
      "Epoch:0557 train loss:0.818 acc:77.14 | val loss:0.885 acc:82.80\r\n",
      "Epoch:0558 train loss:0.756 acc:80.71 | val loss:0.887 acc:82.60\r\n",
      "Epoch:0559 train loss:0.863 acc:76.43 | val loss:0.891 acc:82.80\r\n",
      "Epoch:0560 train loss:0.892 acc:74.29 | val loss:0.894 acc:82.80\r\n",
      "Epoch:0561 train loss:0.830 acc:81.43 | val loss:0.893 acc:82.80\r\n",
      "Epoch:0562 train loss:0.731 acc:85.71 | val loss:0.889 acc:82.60\r\n",
      "Epoch:0563 train loss:0.690 acc:82.14 | val loss:0.884 acc:82.80\r\n",
      "Epoch:0564 train loss:0.945 acc:72.14 | val loss:0.878 acc:83.20\r\n",
      "Epoch:0565 train loss:0.787 acc:84.29 | val loss:0.873 acc:83.20\r\n",
      "Epoch:0566 train loss:0.839 acc:75.71 | val loss:0.871 acc:83.40\r\n",
      "Epoch:0567 train loss:0.783 acc:82.14 | val loss:0.871 acc:83.20\r\n",
      "Epoch:0568 train loss:0.911 acc:73.57 | val loss:0.873 acc:83.40\r\n",
      "Epoch:0569 train loss:0.846 acc:70.71 | val loss:0.876 acc:83.00\r\n",
      "Epoch:0570 train loss:0.870 acc:73.57 | val loss:0.879 acc:83.40\r\n",
      "Epoch:0571 train loss:0.807 acc:77.86 | val loss:0.880 acc:83.00\r\n",
      "Epoch:0572 train loss:0.835 acc:75.00 | val loss:0.883 acc:82.60\r\n",
      "Epoch:0573 train loss:0.837 acc:75.71 | val loss:0.887 acc:82.00\r\n",
      "Epoch:0574 train loss:0.885 acc:76.43 | val loss:0.889 acc:82.40\r\n",
      "Epoch:0575 train loss:0.822 acc:74.29 | val loss:0.890 acc:82.20\r\n",
      "Epoch:0576 train loss:0.830 acc:75.71 | val loss:0.889 acc:82.80\r\n",
      "Epoch:0577 train loss:0.875 acc:72.86 | val loss:0.886 acc:83.40\r\n",
      "Epoch:0578 train loss:0.925 acc:76.43 | val loss:0.885 acc:83.60\r\n",
      "Epoch:0579 train loss:0.957 acc:74.29 | val loss:0.884 acc:83.20\r\n",
      "Epoch:0580 train loss:0.846 acc:78.57 | val loss:0.883 acc:82.60\r\n",
      "Epoch:0581 train loss:0.795 acc:83.57 | val loss:0.881 acc:82.60\r\n",
      "Epoch:0582 train loss:0.762 acc:82.86 | val loss:0.879 acc:82.60\r\n",
      "Epoch:0583 train loss:0.839 acc:78.57 | val loss:0.875 acc:82.40\r\n",
      "Epoch:0584 train loss:0.828 acc:78.57 | val loss:0.871 acc:82.60\r\n",
      "Epoch:0585 train loss:0.787 acc:78.57 | val loss:0.867 acc:83.60\r\n",
      "Epoch:0586 train loss:0.797 acc:77.86 | val loss:0.865 acc:83.80\r\n",
      "Epoch:0587 train loss:0.879 acc:73.57 | val loss:0.864 acc:83.60\r\n",
      "Epoch:0588 train loss:1.055 acc:75.00 | val loss:0.864 acc:83.00\r\n",
      "Epoch:0589 train loss:0.895 acc:79.29 | val loss:0.868 acc:83.00\r\n",
      "Epoch:0590 train loss:0.894 acc:77.86 | val loss:0.874 acc:83.00\r\n",
      "Epoch:0591 train loss:0.840 acc:74.29 | val loss:0.883 acc:82.60\r\n",
      "Epoch:0592 train loss:0.792 acc:78.57 | val loss:0.892 acc:82.40\r\n",
      "Epoch:0593 train loss:0.933 acc:72.14 | val loss:0.899 acc:82.40\r\n",
      "Epoch:0594 train loss:0.975 acc:72.14 | val loss:0.903 acc:82.80\r\n",
      "Epoch:0595 train loss:0.862 acc:75.00 | val loss:0.906 acc:82.60\r\n",
      "Epoch:0596 train loss:0.830 acc:78.57 | val loss:0.907 acc:83.00\r\n",
      "Epoch:0597 train loss:0.780 acc:76.43 | val loss:0.906 acc:83.00\r\n",
      "Epoch:0598 train loss:0.760 acc:78.57 | val loss:0.904 acc:83.00\r\n",
      "Epoch:0599 train loss:0.801 acc:82.14 | val loss:0.902 acc:83.00\r\n",
      "Epoch:0600 train loss:0.903 acc:72.86 | val loss:0.900 acc:82.80\r\n",
      "Epoch:0601 train loss:0.876 acc:77.14 | val loss:0.898 acc:82.40\r\n",
      "Epoch:0602 train loss:0.841 acc:77.14 | val loss:0.895 acc:82.60\r\n",
      "Epoch:0603 train loss:0.801 acc:75.00 | val loss:0.893 acc:82.80\r\n",
      "Epoch:0604 train loss:0.837 acc:79.29 | val loss:0.891 acc:82.40\r\n",
      "Epoch:0605 train loss:0.764 acc:83.57 | val loss:0.887 acc:82.20\r\n",
      "Epoch:0606 train loss:0.824 acc:79.29 | val loss:0.883 acc:82.40\r\n",
      "Epoch:0607 train loss:0.805 acc:77.14 | val loss:0.879 acc:81.40\r\n",
      "Epoch:0608 train loss:0.892 acc:72.86 | val loss:0.876 acc:81.80\r\n",
      "Epoch:0609 train loss:0.833 acc:73.57 | val loss:0.873 acc:81.40\r\n",
      "Epoch:0610 train loss:0.917 acc:75.00 | val loss:0.871 acc:81.60\r\n",
      "Epoch:0611 train loss:0.825 acc:75.71 | val loss:0.870 acc:81.60\r\n",
      "Epoch:0612 train loss:0.854 acc:79.29 | val loss:0.867 acc:81.80\r\n",
      "Epoch:0613 train loss:0.844 acc:79.29 | val loss:0.864 acc:81.80\r\n",
      "Epoch:0614 train loss:0.757 acc:82.14 | val loss:0.862 acc:82.20\r\n",
      "Epoch:0615 train loss:0.804 acc:77.14 | val loss:0.861 acc:83.20\r\n",
      "Epoch:0616 train loss:0.718 acc:82.14 | val loss:0.860 acc:83.20\r\n",
      "Epoch:0617 train loss:0.846 acc:80.00 | val loss:0.858 acc:83.00\r\n",
      "Epoch:0618 train loss:0.838 acc:75.71 | val loss:0.858 acc:82.80\r\n",
      "Epoch:0619 train loss:0.844 acc:77.86 | val loss:0.859 acc:82.40\r\n",
      "Epoch:0620 train loss:0.810 acc:80.71 | val loss:0.862 acc:82.40\r\n",
      "Epoch:0621 train loss:0.848 acc:78.57 | val loss:0.866 acc:82.40\r\n",
      "Epoch:0622 train loss:0.854 acc:75.71 | val loss:0.871 acc:82.80\r\n",
      "Epoch:0623 train loss:0.807 acc:78.57 | val loss:0.875 acc:82.80\r\n",
      "Epoch:0624 train loss:0.869 acc:74.29 | val loss:0.881 acc:83.00\r\n",
      "Epoch:0625 train loss:0.820 acc:77.14 | val loss:0.886 acc:82.80\r\n",
      "Epoch:0626 train loss:0.800 acc:77.14 | val loss:0.891 acc:83.20\r\n",
      "Epoch:0627 train loss:0.844 acc:75.00 | val loss:0.895 acc:83.20\r\n",
      "Epoch:0628 train loss:0.919 acc:76.43 | val loss:0.896 acc:83.20\r\n",
      "Epoch:0629 train loss:0.833 acc:74.29 | val loss:0.896 acc:82.80\r\n",
      "Epoch:0630 train loss:0.842 acc:77.86 | val loss:0.898 acc:82.80\r\n",
      "Epoch:0631 train loss:0.843 acc:77.14 | val loss:0.898 acc:82.60\r\n",
      "Epoch:0632 train loss:0.875 acc:80.00 | val loss:0.895 acc:83.00\r\n",
      "Epoch:0633 train loss:0.915 acc:75.00 | val loss:0.892 acc:83.00\r\n",
      "Epoch:0634 train loss:0.831 acc:77.86 | val loss:0.889 acc:82.80\r\n",
      "Epoch:0635 train loss:0.926 acc:74.29 | val loss:0.887 acc:82.20\r\n",
      "Epoch:0636 train loss:0.929 acc:74.29 | val loss:0.882 acc:81.80\r\n",
      "Epoch:0637 train loss:0.758 acc:80.00 | val loss:0.878 acc:81.60\r\n",
      "Epoch:0638 train loss:0.753 acc:83.57 | val loss:0.872 acc:81.40\r\n",
      "Epoch:0639 train loss:0.684 acc:82.14 | val loss:0.867 acc:81.20\r\n",
      "Epoch:0640 train loss:0.855 acc:77.14 | val loss:0.863 acc:80.80\r\n",
      "Epoch:0641 train loss:0.747 acc:80.00 | val loss:0.861 acc:81.20\r\n",
      "Epoch:0642 train loss:0.784 acc:76.43 | val loss:0.859 acc:82.20\r\n",
      "Epoch:0643 train loss:0.744 acc:80.00 | val loss:0.857 acc:83.20\r\n",
      "Epoch:0644 train loss:0.922 acc:77.86 | val loss:0.860 acc:83.40\r\n",
      "Epoch:0645 train loss:0.839 acc:76.43 | val loss:0.862 acc:83.60\r\n",
      "Epoch:0646 train loss:0.738 acc:80.71 | val loss:0.863 acc:83.80\r\n",
      "Epoch:0647 train loss:0.840 acc:74.29 | val loss:0.864 acc:83.40\r\n",
      "Epoch:0648 train loss:0.808 acc:82.14 | val loss:0.865 acc:83.00\r\n",
      "Epoch:0649 train loss:0.844 acc:77.86 | val loss:0.866 acc:82.40\r\n",
      "Epoch:0650 train loss:0.808 acc:77.86 | val loss:0.867 acc:82.20\r\n",
      "Epoch:0651 train loss:0.818 acc:78.57 | val loss:0.867 acc:82.00\r\n",
      "Epoch:0652 train loss:0.786 acc:78.57 | val loss:0.866 acc:81.80\r\n",
      "Epoch:0653 train loss:0.827 acc:75.00 | val loss:0.867 acc:82.20\r\n",
      "Epoch:0654 train loss:0.870 acc:74.29 | val loss:0.867 acc:82.20\r\n",
      "Epoch:0655 train loss:0.837 acc:78.57 | val loss:0.867 acc:82.40\r\n",
      "Epoch:0656 train loss:0.876 acc:77.14 | val loss:0.867 acc:82.80\r\n",
      "Epoch:0657 train loss:0.810 acc:79.29 | val loss:0.868 acc:83.20\r\n",
      "Epoch:0658 train loss:0.813 acc:71.43 | val loss:0.868 acc:83.00\r\n",
      "Epoch:0659 train loss:0.756 acc:80.71 | val loss:0.867 acc:82.60\r\n",
      "Epoch:0660 train loss:0.813 acc:80.00 | val loss:0.867 acc:82.60\r\n",
      "Epoch:0661 train loss:0.913 acc:72.14 | val loss:0.867 acc:82.20\r\n",
      "Epoch:0662 train loss:0.805 acc:78.57 | val loss:0.868 acc:82.00\r\n",
      "Epoch:0663 train loss:0.808 acc:80.00 | val loss:0.871 acc:82.40\r\n",
      "Epoch:0664 train loss:0.891 acc:77.86 | val loss:0.874 acc:82.40\r\n",
      "Epoch:0665 train loss:0.803 acc:78.57 | val loss:0.876 acc:82.80\r\n",
      "Epoch:0666 train loss:0.782 acc:78.57 | val loss:0.878 acc:82.60\r\n",
      "Epoch:0667 train loss:0.918 acc:77.14 | val loss:0.882 acc:82.60\r\n",
      "Epoch:0668 train loss:0.775 acc:77.86 | val loss:0.885 acc:82.80\r\n",
      "Epoch:0669 train loss:0.763 acc:82.86 | val loss:0.886 acc:82.80\r\n",
      "Epoch:0670 train loss:0.755 acc:79.29 | val loss:0.885 acc:82.60\r\n",
      "Epoch:0671 train loss:0.795 acc:78.57 | val loss:0.882 acc:82.40\r\n",
      "Epoch:0672 train loss:0.797 acc:77.14 | val loss:0.877 acc:83.00\r\n",
      "Epoch:0673 train loss:0.817 acc:78.57 | val loss:0.872 acc:82.60\r\n",
      "Epoch:0674 train loss:0.810 acc:75.71 | val loss:0.869 acc:82.80\r\n",
      "Epoch:0675 train loss:0.857 acc:72.86 | val loss:0.867 acc:82.80\r\n",
      "Epoch:0676 train loss:0.759 acc:79.29 | val loss:0.866 acc:82.40\r\n",
      "Epoch:0677 train loss:0.812 acc:81.43 | val loss:0.866 acc:82.40\r\n",
      "Epoch:0678 train loss:0.793 acc:82.14 | val loss:0.866 acc:82.40\r\n",
      "Epoch:0679 train loss:0.870 acc:72.14 | val loss:0.866 acc:82.60\r\n",
      "Epoch:0680 train loss:0.784 acc:81.43 | val loss:0.866 acc:82.00\r\n",
      "Epoch:0681 train loss:0.857 acc:73.57 | val loss:0.867 acc:81.80\r\n",
      "Epoch:0682 train loss:0.822 acc:75.71 | val loss:0.866 acc:82.00\r\n",
      "Epoch:0683 train loss:0.766 acc:76.43 | val loss:0.865 acc:82.20\r\n",
      "Epoch:0684 train loss:0.779 acc:80.00 | val loss:0.864 acc:82.40\r\n",
      "Epoch:0685 train loss:0.787 acc:77.86 | val loss:0.864 acc:82.40\r\n",
      "Epoch:0686 train loss:0.807 acc:74.29 | val loss:0.862 acc:82.60\r\n",
      "Epoch:0687 train loss:0.817 acc:74.29 | val loss:0.860 acc:82.60\r\n",
      "Epoch:0688 train loss:0.877 acc:73.57 | val loss:0.859 acc:82.80\r\n",
      "Epoch:0689 train loss:0.922 acc:73.57 | val loss:0.859 acc:83.00\r\n",
      "Epoch:0690 train loss:0.784 acc:80.00 | val loss:0.859 acc:82.60\r\n",
      "Epoch:0691 train loss:0.827 acc:76.43 | val loss:0.860 acc:82.40\r\n",
      "Epoch:0692 train loss:0.804 acc:79.29 | val loss:0.863 acc:82.00\r\n",
      "Epoch:0693 train loss:0.784 acc:80.00 | val loss:0.865 acc:82.20\r\n",
      "Epoch:0694 train loss:0.844 acc:74.29 | val loss:0.865 acc:82.60\r\n",
      "Epoch:0695 train loss:0.783 acc:80.71 | val loss:0.863 acc:82.40\r\n",
      "Epoch:0696 train loss:0.792 acc:79.29 | val loss:0.863 acc:82.20\r\n",
      "Epoch:0697 train loss:0.841 acc:78.57 | val loss:0.865 acc:83.00\r\n",
      "Epoch:0698 train loss:0.756 acc:81.43 | val loss:0.870 acc:82.80\r\n",
      "Epoch:0699 train loss:0.788 acc:81.43 | val loss:0.875 acc:82.60\r\n",
      "Epoch:0700 train loss:0.806 acc:82.86 | val loss:0.877 acc:82.40\r\n",
      "Epoch:0701 train loss:0.794 acc:76.43 | val loss:0.874 acc:82.60\r\n",
      "Epoch:0702 train loss:0.780 acc:81.43 | val loss:0.869 acc:82.40\r\n",
      "Epoch:0703 train loss:0.825 acc:78.57 | val loss:0.864 acc:83.00\r\n",
      "Epoch:0704 train loss:0.757 acc:79.29 | val loss:0.860 acc:82.60\r\n",
      "Epoch:0705 train loss:0.782 acc:77.86 | val loss:0.855 acc:82.60\r\n",
      "Epoch:0706 train loss:0.996 acc:77.86 | val loss:0.850 acc:82.20\r\n",
      "Epoch:0707 train loss:0.739 acc:80.71 | val loss:0.846 acc:83.00\r\n",
      "Epoch:0708 train loss:1.215 acc:71.43 | val loss:0.843 acc:83.20\r\n",
      "Epoch:0709 train loss:0.810 acc:79.29 | val loss:0.842 acc:83.00\r\n",
      "Epoch:0710 train loss:0.825 acc:75.00 | val loss:0.843 acc:83.00\r\n",
      "Epoch:0711 train loss:0.950 acc:75.00 | val loss:0.848 acc:82.80\r\n",
      "Epoch:0712 train loss:0.814 acc:77.86 | val loss:0.852 acc:82.80\r\n",
      "Epoch:0713 train loss:0.870 acc:75.00 | val loss:0.858 acc:82.60\r\n",
      "Epoch:0714 train loss:0.715 acc:82.14 | val loss:0.863 acc:82.40\r\n",
      "Epoch:0715 train loss:0.895 acc:70.71 | val loss:0.866 acc:82.20\r\n",
      "Epoch:0716 train loss:0.722 acc:82.86 | val loss:0.868 acc:82.20\r\n",
      "Epoch:0717 train loss:0.771 acc:82.14 | val loss:0.871 acc:82.00\r\n",
      "Epoch:0718 train loss:0.739 acc:77.14 | val loss:0.872 acc:82.60\r\n",
      "Epoch:0719 train loss:0.819 acc:78.57 | val loss:0.874 acc:83.20\r\n",
      "Epoch:0720 train loss:0.731 acc:82.14 | val loss:0.873 acc:82.80\r\n",
      "Epoch:0721 train loss:0.757 acc:80.00 | val loss:0.872 acc:82.80\r\n",
      "Epoch:0722 train loss:0.792 acc:78.57 | val loss:0.871 acc:82.60\r\n",
      "Epoch:0723 train loss:0.787 acc:82.14 | val loss:0.870 acc:82.60\r\n",
      "Epoch:0724 train loss:0.810 acc:75.71 | val loss:0.870 acc:82.80\r\n",
      "Epoch:0725 train loss:0.955 acc:70.00 | val loss:0.870 acc:82.60\r\n",
      "Epoch:0726 train loss:0.796 acc:78.57 | val loss:0.871 acc:82.80\r\n",
      "Epoch:0727 train loss:0.857 acc:72.86 | val loss:0.873 acc:82.80\r\n",
      "Epoch:0728 train loss:0.912 acc:74.29 | val loss:0.875 acc:83.00\r\n",
      "Epoch:0729 train loss:0.763 acc:81.43 | val loss:0.875 acc:82.80\r\n",
      "Epoch:0730 train loss:0.844 acc:75.00 | val loss:0.874 acc:83.20\r\n",
      "Epoch:0731 train loss:0.734 acc:83.57 | val loss:0.872 acc:83.40\r\n",
      "Epoch:0732 train loss:0.787 acc:82.14 | val loss:0.868 acc:83.40\r\n",
      "Epoch:0733 train loss:0.890 acc:82.14 | val loss:0.865 acc:83.20\r\n",
      "Epoch:0734 train loss:0.744 acc:82.86 | val loss:0.863 acc:82.80\r\n",
      "Epoch:0735 train loss:0.783 acc:80.71 | val loss:0.860 acc:82.60\r\n",
      "Epoch:0736 train loss:0.789 acc:75.00 | val loss:0.858 acc:82.40\r\n",
      "Epoch:0737 train loss:0.835 acc:86.43 | val loss:0.857 acc:82.20\r\n",
      "Epoch:0738 train loss:0.828 acc:77.14 | val loss:0.858 acc:82.40\r\n",
      "Epoch:0739 train loss:0.771 acc:76.43 | val loss:0.858 acc:82.80\r\n",
      "Epoch:0740 train loss:0.862 acc:72.86 | val loss:0.859 acc:82.80\r\n",
      "Epoch:0741 train loss:0.862 acc:75.71 | val loss:0.864 acc:83.60\r\n",
      "Epoch:0742 train loss:0.802 acc:76.43 | val loss:0.869 acc:84.00\r\n",
      "Epoch:0743 train loss:0.797 acc:75.00 | val loss:0.875 acc:84.00\r\n",
      "Epoch:0744 train loss:0.835 acc:76.43 | val loss:0.879 acc:83.40\r\n",
      "Epoch:0745 train loss:0.778 acc:76.43 | val loss:0.880 acc:83.60\r\n",
      "Epoch:0746 train loss:0.802 acc:77.86 | val loss:0.878 acc:83.80\r\n",
      "Epoch:0747 train loss:0.699 acc:80.71 | val loss:0.874 acc:83.40\r\n",
      "Epoch:0748 train loss:0.746 acc:82.86 | val loss:0.872 acc:83.40\r\n",
      "Epoch:0749 train loss:0.790 acc:80.00 | val loss:0.869 acc:83.40\r\n",
      "Epoch:0750 train loss:0.786 acc:82.14 | val loss:0.865 acc:83.40\r\n",
      "Epoch:0751 train loss:0.780 acc:76.43 | val loss:0.864 acc:82.60\r\n",
      "Epoch:0752 train loss:0.840 acc:70.71 | val loss:0.863 acc:82.40\r\n",
      "Epoch:0753 train loss:0.813 acc:80.71 | val loss:0.864 acc:82.60\r\n",
      "Epoch:0754 train loss:0.734 acc:82.14 | val loss:0.864 acc:82.60\r\n",
      "Epoch:0755 train loss:0.829 acc:76.43 | val loss:0.862 acc:82.80\r\n",
      "Epoch:0756 train loss:0.815 acc:82.14 | val loss:0.862 acc:82.80\r\n",
      "Epoch:0757 train loss:0.751 acc:80.00 | val loss:0.860 acc:82.80\r\n",
      "Epoch:0758 train loss:0.845 acc:76.43 | val loss:0.860 acc:82.60\r\n",
      "Epoch:0759 train loss:0.799 acc:72.86 | val loss:0.861 acc:82.60\r\n",
      "Epoch:0760 train loss:0.778 acc:81.43 | val loss:0.862 acc:82.80\r\n",
      "Epoch:0761 train loss:0.895 acc:76.43 | val loss:0.864 acc:83.20\r\n",
      "Epoch:0762 train loss:0.810 acc:80.00 | val loss:0.866 acc:83.00\r\n",
      "Epoch:0763 train loss:0.819 acc:77.14 | val loss:0.865 acc:83.00\r\n",
      "Epoch:0764 train loss:0.753 acc:79.29 | val loss:0.862 acc:82.80\r\n",
      "Epoch:0765 train loss:0.755 acc:83.57 | val loss:0.857 acc:83.60\r\n",
      "Epoch:0766 train loss:0.857 acc:73.57 | val loss:0.856 acc:83.40\r\n",
      "Epoch:0767 train loss:0.845 acc:73.57 | val loss:0.856 acc:83.20\r\n",
      "Epoch:0768 train loss:0.812 acc:74.29 | val loss:0.859 acc:82.60\r\n",
      "Epoch:0769 train loss:0.864 acc:75.71 | val loss:0.860 acc:83.00\r\n",
      "Epoch:0770 train loss:0.781 acc:79.29 | val loss:0.862 acc:82.60\r\n",
      "Epoch:0771 train loss:0.861 acc:75.00 | val loss:0.863 acc:83.00\r\n",
      "Epoch:0772 train loss:0.761 acc:75.71 | val loss:0.864 acc:82.60\r\n",
      "Epoch:0773 train loss:0.764 acc:75.71 | val loss:0.865 acc:82.40\r\n",
      "Epoch:0774 train loss:0.841 acc:75.00 | val loss:0.867 acc:82.40\r\n",
      "Epoch:0775 train loss:0.732 acc:85.00 | val loss:0.868 acc:82.20\r\n",
      "Epoch:0776 train loss:0.801 acc:74.29 | val loss:0.869 acc:82.20\r\n",
      "Epoch:0777 train loss:0.860 acc:85.00 | val loss:0.868 acc:82.40\r\n",
      "Epoch:0778 train loss:0.783 acc:77.86 | val loss:0.867 acc:82.20\r\n",
      "Epoch:0779 train loss:0.807 acc:81.43 | val loss:0.868 acc:82.20\r\n",
      "Epoch:0780 train loss:0.794 acc:77.86 | val loss:0.869 acc:82.40\r\n",
      "Epoch:0781 train loss:0.829 acc:79.29 | val loss:0.871 acc:82.40\r\n",
      "Epoch:0782 train loss:0.759 acc:77.86 | val loss:0.872 acc:82.60\r\n",
      "Epoch:0783 train loss:0.797 acc:81.43 | val loss:0.872 acc:82.80\r\n",
      "Epoch:0784 train loss:0.776 acc:76.43 | val loss:0.871 acc:82.80\r\n",
      "Epoch:0785 train loss:0.779 acc:80.71 | val loss:0.868 acc:82.80\r\n",
      "Epoch:0786 train loss:0.816 acc:73.57 | val loss:0.863 acc:82.80\r\n",
      "Epoch:0787 train loss:0.783 acc:79.29 | val loss:0.861 acc:82.80\r\n",
      "Epoch:0788 train loss:0.786 acc:77.14 | val loss:0.858 acc:82.60\r\n",
      "Epoch:0789 train loss:0.767 acc:76.43 | val loss:0.855 acc:82.60\r\n",
      "Epoch:0790 train loss:0.726 acc:85.00 | val loss:0.851 acc:83.00\r\n",
      "Epoch:0791 train loss:0.765 acc:79.29 | val loss:0.847 acc:82.60\r\n",
      "Epoch:0792 train loss:0.779 acc:80.71 | val loss:0.843 acc:81.60\r\n",
      "Epoch:0793 train loss:0.731 acc:80.00 | val loss:0.841 acc:81.20\r\n",
      "Epoch:0794 train loss:0.767 acc:81.43 | val loss:0.841 acc:81.40\r\n",
      "Epoch:0795 train loss:0.811 acc:79.29 | val loss:0.842 acc:82.40\r\n",
      "Epoch:0796 train loss:0.762 acc:84.29 | val loss:0.842 acc:82.60\r\n",
      "Epoch:0797 train loss:0.776 acc:78.57 | val loss:0.842 acc:82.60\r\n",
      "Epoch:0798 train loss:0.753 acc:79.29 | val loss:0.841 acc:83.20\r\n",
      "Epoch:0799 train loss:0.769 acc:77.14 | val loss:0.842 acc:82.20\r\n",
      "Epoch:0800 train loss:0.833 acc:77.14 | val loss:0.843 acc:81.80\r\n",
      "Epoch:0801 train loss:0.812 acc:80.71 | val loss:0.844 acc:81.60\r\n",
      "Epoch:0802 train loss:0.818 acc:80.00 | val loss:0.844 acc:81.80\r\n",
      "Epoch:0803 train loss:0.811 acc:75.71 | val loss:0.843 acc:82.20\r\n",
      "Epoch:0804 train loss:0.713 acc:81.43 | val loss:0.841 acc:83.20\r\n",
      "Epoch:0805 train loss:0.802 acc:82.14 | val loss:0.840 acc:83.60\r\n",
      "Epoch:0806 train loss:0.707 acc:83.57 | val loss:0.840 acc:82.80\r\n",
      "Epoch:0807 train loss:0.770 acc:82.14 | val loss:0.840 acc:82.80\r\n",
      "Epoch:0808 train loss:0.740 acc:80.00 | val loss:0.841 acc:83.00\r\n",
      "Epoch:0809 train loss:0.794 acc:73.57 | val loss:0.843 acc:82.80\r\n",
      "Epoch:0810 train loss:0.871 acc:77.86 | val loss:0.848 acc:82.80\r\n",
      "Epoch:0811 train loss:0.755 acc:80.00 | val loss:0.851 acc:82.80\r\n",
      "Epoch:0812 train loss:0.758 acc:80.00 | val loss:0.853 acc:82.20\r\n",
      "Epoch:0813 train loss:0.796 acc:83.57 | val loss:0.853 acc:82.00\r\n",
      "Epoch:0814 train loss:0.735 acc:77.86 | val loss:0.852 acc:81.80\r\n",
      "Epoch:0815 train loss:0.707 acc:82.86 | val loss:0.850 acc:81.60\r\n",
      "Epoch:0816 train loss:0.773 acc:81.43 | val loss:0.846 acc:82.00\r\n",
      "Epoch:0817 train loss:0.756 acc:74.29 | val loss:0.843 acc:82.00\r\n",
      "Epoch:0818 train loss:0.743 acc:81.43 | val loss:0.839 acc:82.40\r\n",
      "Epoch:0819 train loss:0.754 acc:78.57 | val loss:0.837 acc:82.40\r\n",
      "Epoch:0820 train loss:0.715 acc:84.29 | val loss:0.834 acc:82.80\r\n",
      "Epoch:0821 train loss:0.774 acc:75.71 | val loss:0.833 acc:83.60\r\n",
      "Epoch:0822 train loss:0.811 acc:75.00 | val loss:0.831 acc:83.60\r\n",
      "Epoch:0823 train loss:0.791 acc:79.29 | val loss:0.829 acc:83.60\r\n",
      "Epoch:0824 train loss:0.815 acc:81.43 | val loss:0.830 acc:84.00\r\n",
      "Epoch:0825 train loss:0.662 acc:85.00 | val loss:0.831 acc:84.00\r\n",
      "Epoch:0826 train loss:0.762 acc:85.71 | val loss:0.832 acc:84.20\r\n",
      "Epoch:0827 train loss:0.674 acc:78.57 | val loss:0.833 acc:83.40\r\n",
      "Epoch:0828 train loss:0.774 acc:81.43 | val loss:0.833 acc:83.00\r\n",
      "Epoch:0829 train loss:0.755 acc:81.43 | val loss:0.835 acc:82.80\r\n",
      "Epoch:0830 train loss:0.687 acc:79.29 | val loss:0.835 acc:82.80\r\n",
      "Epoch:0831 train loss:0.792 acc:77.86 | val loss:0.834 acc:83.20\r\n",
      "Epoch:0832 train loss:0.735 acc:79.29 | val loss:0.831 acc:83.00\r\n",
      "Epoch:0833 train loss:0.789 acc:77.14 | val loss:0.829 acc:83.20\r\n",
      "Epoch:0834 train loss:0.757 acc:79.29 | val loss:0.826 acc:83.20\r\n",
      "Epoch:0835 train loss:0.777 acc:78.57 | val loss:0.823 acc:83.40\r\n",
      "Epoch:0836 train loss:0.807 acc:76.43 | val loss:0.820 acc:83.60\r\n",
      "Epoch:0837 train loss:0.753 acc:80.00 | val loss:0.820 acc:83.40\r\n",
      "Epoch:0838 train loss:0.891 acc:75.00 | val loss:0.820 acc:83.00\r\n",
      "Epoch:0839 train loss:0.789 acc:76.43 | val loss:0.821 acc:83.00\r\n",
      "Epoch:0840 train loss:0.690 acc:83.57 | val loss:0.823 acc:82.80\r\n",
      "Epoch:0841 train loss:0.740 acc:79.29 | val loss:0.826 acc:83.00\r\n",
      "Epoch:0842 train loss:0.765 acc:77.14 | val loss:0.830 acc:83.40\r\n",
      "Epoch:0843 train loss:0.731 acc:83.57 | val loss:0.832 acc:83.20\r\n",
      "Epoch:0844 train loss:0.831 acc:72.14 | val loss:0.836 acc:83.20\r\n",
      "Epoch:0845 train loss:0.744 acc:80.71 | val loss:0.838 acc:82.80\r\n",
      "Epoch:0846 train loss:0.783 acc:80.71 | val loss:0.841 acc:82.80\r\n",
      "Epoch:0847 train loss:0.805 acc:75.00 | val loss:0.844 acc:82.80\r\n",
      "Epoch:0848 train loss:0.878 acc:72.14 | val loss:0.843 acc:82.40\r\n",
      "Epoch:0849 train loss:0.727 acc:79.29 | val loss:0.842 acc:82.60\r\n",
      "Epoch:0850 train loss:0.792 acc:82.86 | val loss:0.840 acc:82.60\r\n",
      "Epoch:0851 train loss:0.771 acc:77.86 | val loss:0.840 acc:82.80\r\n",
      "Epoch:0852 train loss:0.810 acc:77.14 | val loss:0.840 acc:83.40\r\n",
      "Epoch:0853 train loss:0.776 acc:77.14 | val loss:0.843 acc:83.00\r\n",
      "Epoch:0854 train loss:0.778 acc:81.43 | val loss:0.846 acc:83.20\r\n",
      "Epoch:0855 train loss:0.761 acc:79.29 | val loss:0.849 acc:82.80\r\n",
      "Epoch:0856 train loss:0.880 acc:73.57 | val loss:0.854 acc:82.80\r\n",
      "Epoch:0857 train loss:0.752 acc:76.43 | val loss:0.858 acc:82.40\r\n",
      "Epoch:0858 train loss:0.761 acc:82.14 | val loss:0.860 acc:82.60\r\n",
      "Epoch:0859 train loss:0.792 acc:77.14 | val loss:0.860 acc:82.60\r\n",
      "Epoch:0860 train loss:0.789 acc:77.86 | val loss:0.860 acc:82.60\r\n",
      "Epoch:0861 train loss:0.784 acc:82.14 | val loss:0.859 acc:82.60\r\n",
      "Epoch:0862 train loss:0.843 acc:80.00 | val loss:0.859 acc:82.40\r\n",
      "Epoch:0863 train loss:0.719 acc:80.71 | val loss:0.857 acc:82.20\r\n",
      "Epoch:0864 train loss:0.773 acc:82.14 | val loss:0.854 acc:82.40\r\n",
      "Epoch:0865 train loss:0.791 acc:76.43 | val loss:0.850 acc:82.60\r\n",
      "Epoch:0866 train loss:0.862 acc:75.71 | val loss:0.847 acc:82.60\r\n",
      "Epoch:0867 train loss:0.820 acc:78.57 | val loss:0.845 acc:82.80\r\n",
      "Epoch:0868 train loss:0.797 acc:82.86 | val loss:0.845 acc:83.20\r\n",
      "Epoch:0869 train loss:0.825 acc:76.43 | val loss:0.845 acc:82.80\r\n",
      "Epoch:0870 train loss:0.761 acc:76.43 | val loss:0.842 acc:83.00\r\n",
      "Epoch:0871 train loss:0.841 acc:80.71 | val loss:0.839 acc:83.20\r\n",
      "Epoch:0872 train loss:0.718 acc:82.14 | val loss:0.837 acc:83.40\r\n",
      "Epoch:0873 train loss:0.780 acc:75.71 | val loss:0.839 acc:83.00\r\n",
      "Epoch:0874 train loss:0.804 acc:80.00 | val loss:0.841 acc:82.80\r\n",
      "Epoch:0875 train loss:0.806 acc:73.57 | val loss:0.843 acc:82.80\r\n",
      "Epoch:0876 train loss:0.780 acc:77.86 | val loss:0.844 acc:82.40\r\n",
      "Epoch:0877 train loss:0.788 acc:75.71 | val loss:0.844 acc:82.60\r\n",
      "Epoch:0878 train loss:0.804 acc:75.71 | val loss:0.845 acc:82.40\r\n",
      "Epoch:0879 train loss:0.742 acc:82.86 | val loss:0.846 acc:82.20\r\n",
      "Epoch:0880 train loss:0.756 acc:79.29 | val loss:0.847 acc:82.40\r\n",
      "Epoch:0881 train loss:0.761 acc:80.71 | val loss:0.847 acc:82.20\r\n",
      "Epoch:0882 train loss:0.796 acc:77.14 | val loss:0.848 acc:82.40\r\n",
      "Epoch:0883 train loss:0.744 acc:82.86 | val loss:0.847 acc:82.40\r\n",
      "Epoch:0884 train loss:0.751 acc:80.00 | val loss:0.847 acc:83.00\r\n",
      "Epoch:0885 train loss:0.666 acc:83.57 | val loss:0.848 acc:83.20\r\n",
      "Epoch:0886 train loss:0.831 acc:80.00 | val loss:0.849 acc:83.20\r\n",
      "Epoch:0887 train loss:0.687 acc:82.14 | val loss:0.848 acc:83.00\r\n",
      "Epoch:0888 train loss:0.729 acc:80.71 | val loss:0.846 acc:83.40\r\n",
      "Epoch:0889 train loss:0.841 acc:75.71 | val loss:0.845 acc:83.60\r\n",
      "Epoch:0890 train loss:0.857 acc:77.14 | val loss:0.843 acc:83.40\r\n",
      "Epoch:0891 train loss:0.797 acc:75.71 | val loss:0.839 acc:83.40\r\n",
      "Epoch:0892 train loss:0.994 acc:75.00 | val loss:0.840 acc:83.00\r\n",
      "Epoch:0893 train loss:0.805 acc:78.57 | val loss:0.843 acc:83.20\r\n",
      "Epoch:0894 train loss:0.772 acc:82.86 | val loss:0.845 acc:83.60\r\n",
      "Epoch:0895 train loss:0.732 acc:78.57 | val loss:0.843 acc:83.40\r\n",
      "Epoch:0896 train loss:0.747 acc:80.71 | val loss:0.843 acc:83.40\r\n",
      "Epoch:0897 train loss:0.824 acc:77.86 | val loss:0.842 acc:83.20\r\n",
      "Epoch:0898 train loss:0.754 acc:80.00 | val loss:0.842 acc:82.60\r\n",
      "Epoch:0899 train loss:0.733 acc:81.43 | val loss:0.841 acc:82.40\r\n",
      "Epoch:0900 train loss:0.845 acc:76.43 | val loss:0.840 acc:82.60\r\n",
      "Epoch:0901 train loss:0.852 acc:75.71 | val loss:0.840 acc:83.00\r\n",
      "Epoch:0902 train loss:0.772 acc:80.00 | val loss:0.840 acc:83.00\r\n",
      "Epoch:0903 train loss:0.797 acc:79.29 | val loss:0.840 acc:82.80\r\n",
      "Epoch:0904 train loss:0.687 acc:85.00 | val loss:0.839 acc:83.00\r\n",
      "Epoch:0905 train loss:0.847 acc:74.29 | val loss:0.841 acc:83.60\r\n",
      "Epoch:0906 train loss:0.700 acc:77.14 | val loss:0.843 acc:83.40\r\n",
      "Epoch:0907 train loss:0.793 acc:80.71 | val loss:0.844 acc:83.20\r\n",
      "Epoch:0908 train loss:0.813 acc:77.86 | val loss:0.848 acc:82.00\r\n",
      "Epoch:0909 train loss:0.835 acc:80.00 | val loss:0.849 acc:82.00\r\n",
      "Epoch:0910 train loss:0.824 acc:78.57 | val loss:0.850 acc:81.80\r\n",
      "Epoch:0911 train loss:0.817 acc:76.43 | val loss:0.851 acc:81.80\r\n",
      "Epoch:0912 train loss:0.762 acc:82.86 | val loss:0.852 acc:82.20\r\n",
      "Epoch:0913 train loss:0.808 acc:80.00 | val loss:0.853 acc:82.00\r\n",
      "Epoch:0914 train loss:0.786 acc:75.00 | val loss:0.854 acc:81.80\r\n",
      "Epoch:0915 train loss:0.799 acc:79.29 | val loss:0.853 acc:82.20\r\n",
      "Epoch:0916 train loss:0.735 acc:79.29 | val loss:0.852 acc:83.00\r\n",
      "Epoch:0917 train loss:0.752 acc:77.14 | val loss:0.852 acc:83.00\r\n",
      "Epoch:0918 train loss:0.685 acc:80.00 | val loss:0.852 acc:83.00\r\n",
      "Epoch:0919 train loss:0.723 acc:80.71 | val loss:0.851 acc:82.60\r\n",
      "Epoch:0920 train loss:0.730 acc:81.43 | val loss:0.848 acc:82.60\r\n",
      "Epoch:0921 train loss:0.812 acc:79.29 | val loss:0.846 acc:82.80\r\n",
      "Epoch:0922 train loss:0.723 acc:85.00 | val loss:0.843 acc:82.80\r\n",
      "Epoch:0923 train loss:0.791 acc:77.14 | val loss:0.841 acc:83.00\r\n",
      "Epoch:0924 train loss:0.776 acc:80.71 | val loss:0.839 acc:83.40\r\n",
      "Epoch:0925 train loss:0.782 acc:75.71 | val loss:0.836 acc:83.60\r\n",
      "Epoch:0926 train loss:0.753 acc:80.71 | val loss:0.832 acc:83.80\r\n",
      "Epoch:0927 train loss:0.729 acc:77.86 | val loss:0.829 acc:83.60\r\n",
      "Epoch:0928 train loss:0.755 acc:84.29 | val loss:0.824 acc:83.60\r\n",
      "Epoch:0929 train loss:0.778 acc:74.29 | val loss:0.822 acc:83.60\r\n",
      "Epoch:0930 train loss:0.758 acc:84.29 | val loss:0.820 acc:83.20\r\n",
      "Epoch:0931 train loss:0.755 acc:80.00 | val loss:0.819 acc:82.60\r\n",
      "Epoch:0932 train loss:0.680 acc:84.29 | val loss:0.819 acc:82.60\r\n",
      "Epoch:0933 train loss:0.673 acc:83.57 | val loss:0.818 acc:82.40\r\n",
      "Epoch:0934 train loss:0.746 acc:77.86 | val loss:0.820 acc:82.20\r\n",
      "Epoch:0935 train loss:0.781 acc:78.57 | val loss:0.822 acc:82.60\r\n",
      "Epoch:0936 train loss:0.776 acc:79.29 | val loss:0.823 acc:82.20\r\n",
      "Epoch:0937 train loss:0.781 acc:82.14 | val loss:0.823 acc:82.20\r\n",
      "Epoch:0938 train loss:0.754 acc:80.00 | val loss:0.824 acc:82.40\r\n",
      "Epoch:0939 train loss:0.869 acc:79.29 | val loss:0.824 acc:82.40\r\n",
      "Epoch:0940 train loss:0.722 acc:77.86 | val loss:0.825 acc:82.60\r\n",
      "Epoch:0941 train loss:0.757 acc:77.86 | val loss:0.828 acc:83.00\r\n",
      "Epoch:0942 train loss:0.790 acc:77.14 | val loss:0.830 acc:83.20\r\n",
      "Epoch:0943 train loss:0.719 acc:80.71 | val loss:0.831 acc:83.00\r\n",
      "Epoch:0944 train loss:0.779 acc:76.43 | val loss:0.831 acc:83.00\r\n",
      "Epoch:0945 train loss:0.765 acc:80.00 | val loss:0.833 acc:83.00\r\n",
      "Epoch:0946 train loss:0.787 acc:77.86 | val loss:0.832 acc:82.80\r\n",
      "Epoch:0947 train loss:0.772 acc:77.14 | val loss:0.832 acc:82.60\r\n",
      "Epoch:0948 train loss:0.823 acc:82.14 | val loss:0.832 acc:82.80\r\n",
      "Epoch:0949 train loss:0.716 acc:83.57 | val loss:0.831 acc:82.80\r\n",
      "Epoch:0950 train loss:0.695 acc:82.86 | val loss:0.832 acc:83.00\r\n",
      "Epoch:0951 train loss:0.757 acc:80.00 | val loss:0.833 acc:83.20\r\n",
      "Epoch:0952 train loss:0.904 acc:78.57 | val loss:0.835 acc:83.20\r\n",
      "Epoch:0953 train loss:0.824 acc:76.43 | val loss:0.837 acc:83.00\r\n",
      "Epoch:0954 train loss:0.753 acc:80.71 | val loss:0.840 acc:83.20\r\n",
      "Epoch:0955 train loss:0.726 acc:82.14 | val loss:0.843 acc:83.40\r\n",
      "Epoch:0956 train loss:0.768 acc:80.00 | val loss:0.845 acc:83.40\r\n",
      "Epoch:0957 train loss:0.760 acc:78.57 | val loss:0.846 acc:83.40\r\n",
      "Epoch:0958 train loss:0.688 acc:81.43 | val loss:0.846 acc:83.40\r\n",
      "Epoch:0959 train loss:0.741 acc:82.86 | val loss:0.845 acc:83.80\r\n",
      "Epoch:0960 train loss:0.841 acc:74.29 | val loss:0.844 acc:83.20\r\n",
      "Epoch:0961 train loss:0.667 acc:85.00 | val loss:0.846 acc:83.20\r\n",
      "Epoch:0962 train loss:0.677 acc:85.00 | val loss:0.847 acc:83.20\r\n",
      "Epoch:0963 train loss:0.789 acc:81.43 | val loss:0.848 acc:82.60\r\n",
      "Epoch:0964 train loss:0.706 acc:80.00 | val loss:0.848 acc:82.40\r\n",
      "Epoch:0965 train loss:0.748 acc:82.14 | val loss:0.844 acc:83.00\r\n",
      "Epoch:0966 train loss:0.727 acc:80.00 | val loss:0.840 acc:83.00\r\n",
      "Epoch:0967 train loss:0.811 acc:81.43 | val loss:0.836 acc:83.20\r\n",
      "Epoch:0968 train loss:0.822 acc:75.71 | val loss:0.834 acc:83.20\r\n",
      "Epoch:0969 train loss:0.779 acc:80.71 | val loss:0.834 acc:83.40\r\n",
      "Epoch:0970 train loss:0.766 acc:77.86 | val loss:0.833 acc:83.20\r\n",
      "Epoch:0971 train loss:0.785 acc:77.86 | val loss:0.833 acc:83.20\r\n",
      "Epoch:0972 train loss:0.757 acc:82.14 | val loss:0.833 acc:83.40\r\n",
      "Epoch:0973 train loss:0.784 acc:75.00 | val loss:0.832 acc:83.60\r\n",
      "Epoch:0974 train loss:0.742 acc:77.14 | val loss:0.831 acc:83.60\r\n",
      "Epoch:0975 train loss:0.836 acc:77.14 | val loss:0.830 acc:83.60\r\n",
      "Epoch:0976 train loss:0.801 acc:75.71 | val loss:0.830 acc:83.80\r\n",
      "Epoch:0977 train loss:0.761 acc:83.57 | val loss:0.830 acc:83.60\r\n",
      "Epoch:0978 train loss:0.681 acc:77.14 | val loss:0.830 acc:83.60\r\n",
      "Epoch:0979 train loss:0.719 acc:85.71 | val loss:0.832 acc:83.60\r\n",
      "Epoch:0980 train loss:0.774 acc:80.71 | val loss:0.833 acc:83.80\r\n",
      "Epoch:0981 train loss:0.755 acc:80.00 | val loss:0.835 acc:83.80\r\n",
      "Epoch:0982 train loss:0.756 acc:78.57 | val loss:0.837 acc:83.40\r\n",
      "Epoch:0983 train loss:0.825 acc:79.29 | val loss:0.838 acc:83.40\r\n",
      "Epoch:0984 train loss:0.760 acc:81.43 | val loss:0.839 acc:83.20\r\n",
      "Epoch:0985 train loss:0.742 acc:84.29 | val loss:0.840 acc:82.60\r\n",
      "Epoch:0986 train loss:0.779 acc:82.14 | val loss:0.843 acc:83.00\r\n",
      "Epoch:0987 train loss:0.776 acc:77.14 | val loss:0.844 acc:82.80\r\n",
      "Epoch:0988 train loss:0.703 acc:84.29 | val loss:0.843 acc:83.00\r\n",
      "Epoch:0989 train loss:0.712 acc:82.14 | val loss:0.839 acc:82.60\r\n",
      "Epoch:0990 train loss:0.709 acc:80.71 | val loss:0.837 acc:82.60\r\n",
      "Epoch:0991 train loss:0.709 acc:82.86 | val loss:0.836 acc:82.60\r\n",
      "Epoch:0992 train loss:0.792 acc:80.00 | val loss:0.836 acc:83.00\r\n",
      "Epoch:0993 train loss:0.926 acc:75.71 | val loss:0.838 acc:83.40\r\n",
      "Epoch:0994 train loss:0.692 acc:83.57 | val loss:0.837 acc:83.40\r\n",
      "Epoch:0995 train loss:0.746 acc:82.14 | val loss:0.836 acc:83.00\r\n",
      "Epoch:0996 train loss:0.744 acc:81.43 | val loss:0.833 acc:83.40\r\n",
      "Epoch:0997 train loss:0.911 acc:78.57 | val loss:0.834 acc:83.40\r\n",
      "Epoch:0998 train loss:0.752 acc:80.71 | val loss:0.834 acc:83.40\r\n",
      "Epoch:0999 train loss:0.747 acc:82.14 | val loss:0.837 acc:83.20\r\n",
      "Epoch:1000 train loss:0.775 acc:80.00 | val loss:0.840 acc:83.20\r\n",
      "Epoch:1001 train loss:0.709 acc:85.71 | val loss:0.839 acc:83.60\r\n",
      "Epoch:1002 train loss:0.708 acc:84.29 | val loss:0.838 acc:83.00\r\n",
      "Epoch:1003 train loss:0.875 acc:80.71 | val loss:0.838 acc:83.60\r\n",
      "Epoch:1004 train loss:0.765 acc:79.29 | val loss:0.840 acc:83.80\r\n",
      "Epoch:1005 train loss:0.772 acc:85.00 | val loss:0.843 acc:83.60\r\n",
      "Epoch:1006 train loss:0.790 acc:79.29 | val loss:0.847 acc:83.40\r\n",
      "Epoch:1007 train loss:0.869 acc:79.29 | val loss:0.846 acc:83.40\r\n",
      "Epoch:1008 train loss:0.717 acc:82.14 | val loss:0.845 acc:83.80\r\n",
      "Epoch:1009 train loss:0.856 acc:80.00 | val loss:0.847 acc:83.80\r\n",
      "Epoch:1010 train loss:0.815 acc:78.57 | val loss:0.848 acc:83.60\r\n",
      "Epoch:1011 train loss:0.687 acc:82.86 | val loss:0.848 acc:83.40\r\n",
      "Epoch:1012 train loss:1.109 acc:75.71 | val loss:0.850 acc:83.40\r\n",
      "Epoch:1013 train loss:0.774 acc:78.57 | val loss:0.852 acc:83.40\r\n",
      "Epoch:1014 train loss:0.804 acc:76.43 | val loss:0.853 acc:83.00\r\n",
      "Epoch:1015 train loss:1.078 acc:78.57 | val loss:0.856 acc:82.20\r\n",
      "Epoch:1016 train loss:0.825 acc:76.43 | val loss:0.860 acc:81.40\r\n",
      "Epoch:1017 train loss:0.853 acc:71.43 | val loss:0.865 acc:81.20\r\n",
      "Epoch:1018 train loss:0.756 acc:82.14 | val loss:0.870 acc:82.00\r\n",
      "Epoch:1019 train loss:0.829 acc:80.00 | val loss:0.874 acc:82.60\r\n",
      "Epoch:1020 train loss:0.724 acc:82.14 | val loss:0.876 acc:82.20\r\n",
      "Epoch:1021 train loss:0.823 acc:75.71 | val loss:0.877 acc:82.20\r\n",
      "Epoch:1022 train loss:0.815 acc:74.29 | val loss:0.878 acc:82.00\r\n",
      "Epoch:1023 train loss:0.864 acc:71.43 | val loss:0.875 acc:82.20\r\n",
      "Epoch:1024 train loss:0.705 acc:85.00 | val loss:0.870 acc:83.00\r\n",
      "Epoch:1025 train loss:0.858 acc:77.14 | val loss:0.865 acc:83.00\r\n",
      "Epoch:1026 train loss:0.843 acc:77.14 | val loss:0.860 acc:83.20\r\n",
      "Epoch:1027 train loss:0.791 acc:80.00 | val loss:0.855 acc:83.60\r\n",
      "Epoch:1028 train loss:0.769 acc:80.71 | val loss:0.852 acc:83.40\r\n",
      "Epoch:1029 train loss:0.790 acc:77.86 | val loss:0.851 acc:82.80\r\n",
      "Epoch:1030 train loss:0.769 acc:82.14 | val loss:0.850 acc:82.60\r\n",
      "Epoch:1031 train loss:0.831 acc:79.29 | val loss:0.852 acc:82.40\r\n",
      "Epoch:1032 train loss:0.752 acc:80.71 | val loss:0.853 acc:82.60\r\n",
      "Epoch:1033 train loss:2.605 acc:85.00 | val loss:0.857 acc:82.40\r\n",
      "Load 933th epoch\r\n",
      "Test acc.:85.2\r\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_test_acc_list=[]\n",
    "for k in range(repeat):\n",
    "    if best_al==0 and best_variant==0:\n",
    "        !python -u trainshow.py --data cora --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --test  \n",
    "    elif best_al==1 and best_variant==0:\n",
    "        !python -u trainshow.py --data cora --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --lamda {best_lamda} --alpha {best_alpha} --test  \n",
    "    elif best_al==0 and best_variant==1:\n",
    "        !python -u trainshow.py --data cora --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --test --variant  \n",
    "    else:\n",
    "        !python -u trainshow.py --data cora --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --lamda {best_lamda} --alpha {best_alpha} --test --variant  \n",
    "end_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e0a8b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:44:46.814694Z",
     "iopub.status.busy": "2024-04-24T05:44:46.814364Z",
     "iopub.status.idle": "2024-04-24T05:44:46.822311Z",
     "shell.execute_reply": "2024-04-24T05:44:46.821389Z"
    },
    "papermill": {
     "duration": 0.09644,
     "end_time": "2024-04-24T05:44:46.824537",
     "exception": false,
     "start_time": "2024-04-24T05:44:46.728097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora\n",
      "best_num_layers: 64\n",
      "best_optim: 0\n",
      "best_al: 0\n",
      "best_act: 0\n",
      "best_earlystop: 100\n",
      "best_epoch1: 1500\n",
      "best_hidden_dim: 64\n",
      "best_variant: 0\n",
      "---------\n",
      "best_alpha: 0.1\n",
      "best_lamda: 0.5\n",
      "best_dropout: 0.6\n",
      "best_lr: 0.01\n",
      "best_weight_decay_1: 0.01\n",
      "best_weight_decay_2: 0.0005\n",
      "---------\n",
      ": 130.88554692268372\n"
     ]
    }
   ],
   "source": [
    "print(\"cora\")\n",
    "print(f\"best_num_layers: {best_num_layers}\")\n",
    "print(f\"best_optim: {best_optim}\")\n",
    "print(f\"best_al: {best_al}\")\n",
    "print(f\"best_act: {best_act}\")\n",
    "print(f\"best_earlystop: {best_earlystop}\")\n",
    "print(f\"best_epoch1: {best_epoch1}\")\n",
    "print(f\"best_hidden_dim: {best_hidden_dim}\")\n",
    "print(f\"best_variant: {best_variant}\")\n",
    "print(\"---------\")\n",
    "print(f\"best_alpha: {best_alpha}\")\n",
    "print(f\"best_lamda: {best_lamda}\")\n",
    "print(f\"best_dropout: {best_dropout}\")\n",
    "print(f\"best_lr: {best_lr}\")\n",
    "print(f\"best_weight_decay_1: {best_weight_decay_1}\")\n",
    "print(f\"best_weight_decay_2: {best_weight_decay_2}\")\n",
    "print(\"---------\")\n",
    "import numpy as np\n",
    "print(\":\", (end_time-start_time)/repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa27d7a",
   "metadata": {
    "papermill": {
     "duration": 0.084659,
     "end_time": "2024-04-24T05:44:46.994043",
     "exception": false,
     "start_time": "2024-04-24T05:44:46.909384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for citeseer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc95fbe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:44:47.210843Z",
     "iopub.status.busy": "2024-04-24T05:44:47.210459Z",
     "iopub.status.idle": "2024-04-24T05:44:47.215138Z",
     "shell.execute_reply": "2024-04-24T05:44:47.214285Z"
    },
    "papermill": {
     "duration": 0.094005,
     "end_time": "2024-04-24T05:44:47.216915",
     "exception": false,
     "start_time": "2024-04-24T05:44:47.122910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a=(32,0,1,0,100,1500,256,0,0.1,0.6,0.7,0.01,0.01,0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d443446a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:44:47.387279Z",
     "iopub.status.busy": "2024-04-24T05:44:47.386973Z",
     "iopub.status.idle": "2024-04-24T05:45:52.744176Z",
     "shell.execute_reply": "2024-04-24T05:45:52.743034Z"
    },
    "papermill": {
     "duration": 65.444941,
     "end_time": "2024-04-24T05:45:52.746595",
     "exception": false,
     "start_time": "2024-04-24T05:44:47.301654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0000 train loss:1.793 acc:14.17 | val loss:1.799 acc:13.80\r\n",
      "Epoch:0001 train loss:1.794 acc:10.00 | val loss:1.795 acc:13.80\r\n",
      "Epoch:0002 train loss:1.787 acc:15.83 | val loss:1.791 acc:17.60\r\n",
      "Epoch:0003 train loss:1.796 acc:17.50 | val loss:1.788 acc:28.40\r\n",
      "Epoch:0004 train loss:1.784 acc:23.33 | val loss:1.786 acc:19.40\r\n",
      "Epoch:0005 train loss:1.792 acc:24.17 | val loss:1.784 acc:19.00\r\n",
      "Epoch:0006 train loss:1.794 acc:22.50 | val loss:1.782 acc:19.00\r\n",
      "Epoch:0007 train loss:1.778 acc:27.50 | val loss:1.780 acc:19.20\r\n",
      "Epoch:0008 train loss:1.786 acc:20.83 | val loss:1.779 acc:21.20\r\n",
      "Epoch:0009 train loss:1.783 acc:20.00 | val loss:1.778 acc:24.80\r\n",
      "Epoch:0010 train loss:1.765 acc:23.33 | val loss:1.777 acc:30.40\r\n",
      "Epoch:0011 train loss:1.777 acc:26.67 | val loss:1.776 acc:42.40\r\n",
      "Epoch:0012 train loss:1.908 acc:30.00 | val loss:1.775 acc:47.60\r\n",
      "Epoch:0013 train loss:1.772 acc:30.83 | val loss:1.775 acc:47.80\r\n",
      "Epoch:0014 train loss:1.765 acc:26.67 | val loss:1.774 acc:45.40\r\n",
      "Epoch:0015 train loss:1.781 acc:25.00 | val loss:1.774 acc:47.60\r\n",
      "Epoch:0016 train loss:1.781 acc:32.50 | val loss:1.773 acc:53.00\r\n",
      "Epoch:0017 train loss:1.967 acc:29.17 | val loss:1.772 acc:60.00\r\n",
      "Epoch:0018 train loss:1.745 acc:32.50 | val loss:1.771 acc:62.40\r\n",
      "Epoch:0019 train loss:1.783 acc:24.17 | val loss:1.770 acc:56.40\r\n",
      "Epoch:0020 train loss:1.755 acc:22.50 | val loss:1.769 acc:47.40\r\n",
      "Epoch:0021 train loss:1.756 acc:26.67 | val loss:1.768 acc:44.60\r\n",
      "Epoch:0022 train loss:1.763 acc:30.00 | val loss:1.766 acc:45.00\r\n",
      "Epoch:0023 train loss:1.740 acc:30.00 | val loss:1.764 acc:45.80\r\n",
      "Epoch:0024 train loss:1.738 acc:36.67 | val loss:1.762 acc:48.00\r\n",
      "Epoch:0025 train loss:2.093 acc:26.67 | val loss:1.760 acc:47.20\r\n",
      "Epoch:0026 train loss:1.755 acc:31.67 | val loss:1.757 acc:48.00\r\n",
      "Epoch:0027 train loss:1.759 acc:33.33 | val loss:1.755 acc:49.80\r\n",
      "Epoch:0028 train loss:1.743 acc:35.00 | val loss:1.753 acc:56.40\r\n",
      "Epoch:0029 train loss:1.748 acc:32.50 | val loss:1.752 acc:60.00\r\n",
      "Epoch:0030 train loss:1.766 acc:36.67 | val loss:1.750 acc:62.40\r\n",
      "Epoch:0031 train loss:1.722 acc:36.67 | val loss:1.748 acc:64.20\r\n",
      "Epoch:0032 train loss:1.735 acc:39.17 | val loss:1.746 acc:64.00\r\n",
      "Epoch:0033 train loss:1.720 acc:35.83 | val loss:1.744 acc:63.00\r\n",
      "Epoch:0034 train loss:1.725 acc:42.50 | val loss:1.742 acc:60.40\r\n",
      "Epoch:0035 train loss:1.734 acc:39.17 | val loss:1.741 acc:58.20\r\n",
      "Epoch:0036 train loss:1.713 acc:32.50 | val loss:1.738 acc:57.40\r\n",
      "Epoch:0037 train loss:1.739 acc:27.50 | val loss:1.736 acc:57.80\r\n",
      "Epoch:0038 train loss:1.737 acc:39.17 | val loss:1.733 acc:59.00\r\n",
      "Epoch:0039 train loss:1.707 acc:35.83 | val loss:1.730 acc:60.80\r\n",
      "Epoch:0040 train loss:1.683 acc:45.00 | val loss:1.727 acc:61.40\r\n",
      "Epoch:0041 train loss:1.896 acc:40.00 | val loss:1.725 acc:61.40\r\n",
      "Epoch:0042 train loss:1.718 acc:37.50 | val loss:1.722 acc:60.40\r\n",
      "Epoch:0043 train loss:1.713 acc:34.17 | val loss:1.720 acc:60.00\r\n",
      "Epoch:0044 train loss:1.707 acc:36.67 | val loss:1.716 acc:60.60\r\n",
      "Epoch:0045 train loss:1.720 acc:35.00 | val loss:1.712 acc:62.00\r\n",
      "Epoch:0046 train loss:1.758 acc:38.33 | val loss:1.708 acc:63.20\r\n",
      "Epoch:0047 train loss:1.674 acc:40.00 | val loss:1.704 acc:65.20\r\n",
      "Epoch:0048 train loss:1.859 acc:36.67 | val loss:1.700 acc:67.60\r\n",
      "Epoch:0049 train loss:1.656 acc:41.67 | val loss:1.697 acc:70.00\r\n",
      "Epoch:0050 train loss:1.693 acc:34.17 | val loss:1.694 acc:69.60\r\n",
      "Epoch:0051 train loss:1.702 acc:35.83 | val loss:1.691 acc:70.20\r\n",
      "Epoch:0052 train loss:1.714 acc:32.50 | val loss:1.689 acc:70.60\r\n",
      "Epoch:0053 train loss:1.877 acc:47.50 | val loss:1.686 acc:69.00\r\n",
      "Epoch:0054 train loss:1.673 acc:44.17 | val loss:1.685 acc:69.20\r\n",
      "Epoch:0055 train loss:1.667 acc:45.83 | val loss:1.683 acc:68.40\r\n",
      "Epoch:0056 train loss:1.647 acc:39.17 | val loss:1.682 acc:67.60\r\n",
      "Epoch:0057 train loss:1.663 acc:50.00 | val loss:1.680 acc:67.00\r\n",
      "Epoch:0058 train loss:1.626 acc:39.17 | val loss:1.678 acc:66.60\r\n",
      "Epoch:0059 train loss:1.678 acc:40.83 | val loss:1.675 acc:65.60\r\n",
      "Epoch:0060 train loss:1.648 acc:38.33 | val loss:1.673 acc:64.60\r\n",
      "Epoch:0061 train loss:1.667 acc:50.00 | val loss:1.671 acc:64.60\r\n",
      "Epoch:0062 train loss:1.677 acc:41.67 | val loss:1.670 acc:65.00\r\n",
      "Epoch:0063 train loss:1.724 acc:45.00 | val loss:1.668 acc:65.60\r\n",
      "Epoch:0064 train loss:1.748 acc:43.33 | val loss:1.668 acc:67.20\r\n",
      "Epoch:0065 train loss:1.702 acc:45.83 | val loss:1.667 acc:68.40\r\n",
      "Epoch:0066 train loss:1.630 acc:35.83 | val loss:1.666 acc:67.40\r\n",
      "Epoch:0067 train loss:2.568 acc:49.17 | val loss:1.665 acc:66.00\r\n",
      "Epoch:0068 train loss:1.631 acc:46.67 | val loss:1.663 acc:66.60\r\n",
      "Epoch:0069 train loss:1.628 acc:49.17 | val loss:1.660 acc:67.20\r\n",
      "Epoch:0070 train loss:1.637 acc:48.33 | val loss:1.657 acc:68.00\r\n",
      "Epoch:0071 train loss:1.592 acc:50.00 | val loss:1.654 acc:67.00\r\n",
      "Epoch:0072 train loss:1.615 acc:46.67 | val loss:1.650 acc:67.40\r\n",
      "Epoch:0073 train loss:1.610 acc:44.17 | val loss:1.646 acc:67.80\r\n",
      "Epoch:0074 train loss:1.607 acc:50.83 | val loss:1.642 acc:69.00\r\n",
      "Epoch:0075 train loss:1.636 acc:50.83 | val loss:1.638 acc:69.40\r\n",
      "Epoch:0076 train loss:1.629 acc:48.33 | val loss:1.634 acc:69.40\r\n",
      "Epoch:0077 train loss:1.776 acc:35.83 | val loss:1.631 acc:69.80\r\n",
      "Epoch:0078 train loss:1.666 acc:50.00 | val loss:1.628 acc:70.40\r\n",
      "Epoch:0079 train loss:1.629 acc:43.33 | val loss:1.625 acc:69.20\r\n",
      "Epoch:0080 train loss:2.134 acc:51.67 | val loss:1.622 acc:68.20\r\n",
      "Epoch:0081 train loss:1.655 acc:47.50 | val loss:1.620 acc:68.60\r\n",
      "Epoch:0082 train loss:2.076 acc:47.50 | val loss:1.618 acc:67.80\r\n",
      "Epoch:0083 train loss:1.550 acc:46.67 | val loss:1.614 acc:68.20\r\n",
      "Epoch:0084 train loss:1.550 acc:49.17 | val loss:1.610 acc:67.60\r\n",
      "Epoch:0085 train loss:1.606 acc:44.17 | val loss:1.608 acc:66.80\r\n",
      "Epoch:0086 train loss:1.577 acc:52.50 | val loss:1.606 acc:67.00\r\n",
      "Epoch:0087 train loss:1.530 acc:50.00 | val loss:1.603 acc:67.40\r\n",
      "Epoch:0088 train loss:1.587 acc:46.67 | val loss:1.600 acc:67.80\r\n",
      "Epoch:0089 train loss:1.579 acc:46.67 | val loss:1.595 acc:67.80\r\n",
      "Epoch:0090 train loss:1.578 acc:47.50 | val loss:1.591 acc:67.80\r\n",
      "Epoch:0091 train loss:1.617 acc:50.83 | val loss:1.587 acc:68.60\r\n",
      "Epoch:0092 train loss:1.525 acc:50.00 | val loss:1.583 acc:69.20\r\n",
      "Epoch:0093 train loss:1.581 acc:45.83 | val loss:1.580 acc:69.60\r\n",
      "Epoch:0094 train loss:1.526 acc:50.00 | val loss:1.577 acc:69.20\r\n",
      "Epoch:0095 train loss:1.529 acc:54.17 | val loss:1.574 acc:69.00\r\n",
      "Epoch:0096 train loss:1.564 acc:50.00 | val loss:1.573 acc:69.00\r\n",
      "Epoch:0097 train loss:1.544 acc:55.00 | val loss:1.572 acc:69.20\r\n",
      "Epoch:0098 train loss:1.528 acc:56.67 | val loss:1.571 acc:69.20\r\n",
      "Epoch:0099 train loss:1.536 acc:53.33 | val loss:1.569 acc:69.60\r\n",
      "Epoch:0100 train loss:1.492 acc:50.83 | val loss:1.567 acc:70.60\r\n",
      "Epoch:0101 train loss:1.532 acc:55.83 | val loss:1.564 acc:70.20\r\n",
      "Epoch:0102 train loss:1.527 acc:49.17 | val loss:1.562 acc:70.40\r\n",
      "Epoch:0103 train loss:1.512 acc:56.67 | val loss:1.559 acc:69.80\r\n",
      "Epoch:0104 train loss:1.480 acc:66.67 | val loss:1.555 acc:69.40\r\n",
      "Epoch:0105 train loss:1.523 acc:51.67 | val loss:1.551 acc:69.60\r\n",
      "Epoch:0106 train loss:1.486 acc:57.50 | val loss:1.546 acc:69.60\r\n",
      "Epoch:0107 train loss:1.473 acc:57.50 | val loss:1.543 acc:69.60\r\n",
      "Epoch:0108 train loss:1.512 acc:52.50 | val loss:1.540 acc:70.00\r\n",
      "Epoch:0109 train loss:1.974 acc:51.67 | val loss:1.536 acc:70.60\r\n",
      "Epoch:0110 train loss:1.485 acc:57.50 | val loss:1.534 acc:71.20\r\n",
      "Epoch:0111 train loss:1.464 acc:64.17 | val loss:1.533 acc:71.00\r\n",
      "Epoch:0112 train loss:1.469 acc:57.50 | val loss:1.531 acc:71.00\r\n",
      "Epoch:0113 train loss:1.533 acc:45.00 | val loss:1.530 acc:71.00\r\n",
      "Epoch:0114 train loss:1.533 acc:51.67 | val loss:1.528 acc:71.20\r\n",
      "Epoch:0115 train loss:1.511 acc:49.17 | val loss:1.526 acc:70.40\r\n",
      "Epoch:0116 train loss:1.489 acc:55.00 | val loss:1.525 acc:70.40\r\n",
      "Epoch:0117 train loss:1.481 acc:61.67 | val loss:1.523 acc:70.00\r\n",
      "Epoch:0118 train loss:1.489 acc:55.83 | val loss:1.520 acc:69.80\r\n",
      "Epoch:0119 train loss:1.479 acc:53.33 | val loss:1.517 acc:69.80\r\n",
      "Epoch:0120 train loss:1.479 acc:54.17 | val loss:1.513 acc:69.40\r\n",
      "Epoch:0121 train loss:1.393 acc:63.33 | val loss:1.509 acc:69.40\r\n",
      "Epoch:0122 train loss:1.704 acc:50.00 | val loss:1.504 acc:69.60\r\n",
      "Epoch:0123 train loss:1.491 acc:50.83 | val loss:1.500 acc:70.00\r\n",
      "Epoch:0124 train loss:1.457 acc:53.33 | val loss:1.497 acc:69.80\r\n",
      "Epoch:0125 train loss:1.407 acc:60.83 | val loss:1.492 acc:69.20\r\n",
      "Epoch:0126 train loss:1.461 acc:56.67 | val loss:1.488 acc:70.00\r\n",
      "Epoch:0127 train loss:1.425 acc:50.00 | val loss:1.484 acc:70.20\r\n",
      "Epoch:0128 train loss:1.434 acc:51.67 | val loss:1.481 acc:70.20\r\n",
      "Epoch:0129 train loss:1.422 acc:60.00 | val loss:1.477 acc:69.80\r\n",
      "Epoch:0130 train loss:1.362 acc:57.50 | val loss:1.473 acc:69.60\r\n",
      "Epoch:0131 train loss:1.771 acc:53.33 | val loss:1.468 acc:69.00\r\n",
      "Epoch:0132 train loss:1.429 acc:57.50 | val loss:1.464 acc:68.80\r\n",
      "Epoch:0133 train loss:1.394 acc:57.50 | val loss:1.461 acc:69.00\r\n",
      "Epoch:0134 train loss:1.417 acc:54.17 | val loss:1.458 acc:70.00\r\n",
      "Epoch:0135 train loss:1.432 acc:56.67 | val loss:1.455 acc:70.60\r\n",
      "Epoch:0136 train loss:1.397 acc:57.50 | val loss:1.453 acc:70.80\r\n",
      "Epoch:0137 train loss:1.503 acc:61.67 | val loss:1.452 acc:71.20\r\n",
      "Epoch:0138 train loss:1.434 acc:56.67 | val loss:1.451 acc:71.80\r\n",
      "Epoch:0139 train loss:1.474 acc:58.33 | val loss:1.452 acc:71.60\r\n",
      "Epoch:0140 train loss:1.394 acc:59.17 | val loss:1.454 acc:72.20\r\n",
      "Epoch:0141 train loss:1.431 acc:53.33 | val loss:1.453 acc:71.20\r\n",
      "Epoch:0142 train loss:1.385 acc:64.17 | val loss:1.450 acc:71.20\r\n",
      "Epoch:0143 train loss:1.355 acc:53.33 | val loss:1.446 acc:71.60\r\n",
      "Epoch:0144 train loss:1.435 acc:57.50 | val loss:1.443 acc:71.40\r\n",
      "Epoch:0145 train loss:1.333 acc:57.50 | val loss:1.440 acc:72.00\r\n",
      "Epoch:0146 train loss:1.341 acc:67.50 | val loss:1.438 acc:72.40\r\n",
      "Epoch:0147 train loss:1.336 acc:65.00 | val loss:1.435 acc:71.40\r\n",
      "Epoch:0148 train loss:1.340 acc:58.33 | val loss:1.434 acc:71.20\r\n",
      "Epoch:0149 train loss:1.389 acc:59.17 | val loss:1.434 acc:71.40\r\n",
      "Epoch:0150 train loss:1.426 acc:59.17 | val loss:1.434 acc:71.20\r\n",
      "Epoch:0151 train loss:1.560 acc:60.83 | val loss:1.436 acc:70.60\r\n",
      "Epoch:0152 train loss:1.433 acc:56.67 | val loss:1.437 acc:70.80\r\n",
      "Epoch:0153 train loss:1.410 acc:52.50 | val loss:1.438 acc:70.60\r\n",
      "Epoch:0154 train loss:1.375 acc:60.00 | val loss:1.436 acc:70.80\r\n",
      "Epoch:0155 train loss:1.385 acc:60.00 | val loss:1.433 acc:70.40\r\n",
      "Epoch:0156 train loss:1.446 acc:53.33 | val loss:1.429 acc:71.00\r\n",
      "Epoch:0157 train loss:1.500 acc:56.67 | val loss:1.427 acc:71.40\r\n",
      "Epoch:0158 train loss:1.558 acc:56.67 | val loss:1.426 acc:71.60\r\n",
      "Epoch:0159 train loss:1.322 acc:62.50 | val loss:1.425 acc:72.00\r\n",
      "Epoch:0160 train loss:1.325 acc:58.33 | val loss:1.423 acc:71.40\r\n",
      "Epoch:0161 train loss:1.541 acc:53.33 | val loss:1.422 acc:72.00\r\n",
      "Epoch:0162 train loss:1.379 acc:60.00 | val loss:1.422 acc:72.00\r\n",
      "Epoch:0163 train loss:1.364 acc:55.83 | val loss:1.419 acc:71.40\r\n",
      "Epoch:0164 train loss:1.309 acc:61.67 | val loss:1.418 acc:71.00\r\n",
      "Epoch:0165 train loss:1.358 acc:62.50 | val loss:1.416 acc:71.00\r\n",
      "Epoch:0166 train loss:1.412 acc:57.50 | val loss:1.414 acc:71.20\r\n",
      "Epoch:0167 train loss:1.347 acc:61.67 | val loss:1.411 acc:71.60\r\n",
      "Epoch:0168 train loss:1.385 acc:56.67 | val loss:1.409 acc:71.20\r\n",
      "Epoch:0169 train loss:1.334 acc:55.00 | val loss:1.409 acc:71.00\r\n",
      "Epoch:0170 train loss:1.506 acc:56.67 | val loss:1.408 acc:71.20\r\n",
      "Epoch:0171 train loss:1.354 acc:60.83 | val loss:1.406 acc:71.20\r\n",
      "Epoch:0172 train loss:1.320 acc:58.33 | val loss:1.404 acc:71.80\r\n",
      "Epoch:0173 train loss:1.260 acc:63.33 | val loss:1.401 acc:71.80\r\n",
      "Epoch:0174 train loss:1.359 acc:60.00 | val loss:1.398 acc:71.60\r\n",
      "Epoch:0175 train loss:1.320 acc:63.33 | val loss:1.395 acc:71.20\r\n",
      "Epoch:0176 train loss:1.316 acc:60.83 | val loss:1.393 acc:72.00\r\n",
      "Epoch:0177 train loss:1.491 acc:60.83 | val loss:1.392 acc:72.60\r\n",
      "Epoch:0178 train loss:1.362 acc:56.67 | val loss:1.390 acc:72.00\r\n",
      "Epoch:0179 train loss:1.334 acc:55.00 | val loss:1.390 acc:71.60\r\n",
      "Epoch:0180 train loss:1.336 acc:56.67 | val loss:1.391 acc:72.20\r\n",
      "Epoch:0181 train loss:1.281 acc:62.50 | val loss:1.393 acc:72.20\r\n",
      "Epoch:0182 train loss:1.310 acc:61.67 | val loss:1.393 acc:72.00\r\n",
      "Epoch:0183 train loss:1.363 acc:64.17 | val loss:1.393 acc:71.80\r\n",
      "Epoch:0184 train loss:1.382 acc:63.33 | val loss:1.394 acc:71.40\r\n",
      "Epoch:0185 train loss:1.354 acc:65.83 | val loss:1.394 acc:71.20\r\n",
      "Epoch:0186 train loss:1.369 acc:62.50 | val loss:1.392 acc:71.00\r\n",
      "Epoch:0187 train loss:1.349 acc:58.33 | val loss:1.392 acc:70.60\r\n",
      "Epoch:0188 train loss:1.480 acc:56.67 | val loss:1.393 acc:70.40\r\n",
      "Epoch:0189 train loss:1.373 acc:61.67 | val loss:1.396 acc:70.40\r\n",
      "Epoch:0190 train loss:1.373 acc:57.50 | val loss:1.397 acc:70.40\r\n",
      "Epoch:0191 train loss:1.276 acc:67.50 | val loss:1.397 acc:70.40\r\n",
      "Epoch:0192 train loss:1.285 acc:61.67 | val loss:1.395 acc:70.80\r\n",
      "Epoch:0193 train loss:1.323 acc:60.83 | val loss:1.392 acc:71.40\r\n",
      "Epoch:0194 train loss:1.324 acc:58.33 | val loss:1.389 acc:71.60\r\n",
      "Epoch:0195 train loss:1.416 acc:56.67 | val loss:1.385 acc:71.40\r\n",
      "Epoch:0196 train loss:1.281 acc:65.00 | val loss:1.382 acc:71.20\r\n",
      "Epoch:0197 train loss:1.843 acc:50.00 | val loss:1.378 acc:71.40\r\n",
      "Epoch:0198 train loss:1.269 acc:62.50 | val loss:1.374 acc:71.40\r\n",
      "Epoch:0199 train loss:1.343 acc:63.33 | val loss:1.373 acc:71.40\r\n",
      "Epoch:0200 train loss:1.344 acc:60.00 | val loss:1.372 acc:71.00\r\n",
      "Epoch:0201 train loss:1.342 acc:64.17 | val loss:1.372 acc:71.60\r\n",
      "Epoch:0202 train loss:1.332 acc:60.83 | val loss:1.372 acc:71.80\r\n",
      "Epoch:0203 train loss:1.466 acc:55.83 | val loss:1.372 acc:72.20\r\n",
      "Epoch:0204 train loss:1.331 acc:59.17 | val loss:1.372 acc:72.40\r\n",
      "Epoch:0205 train loss:1.742 acc:59.17 | val loss:1.373 acc:72.20\r\n",
      "Epoch:0206 train loss:1.566 acc:64.17 | val loss:1.375 acc:72.00\r\n",
      "Epoch:0207 train loss:1.325 acc:63.33 | val loss:1.378 acc:71.60\r\n",
      "Epoch:0208 train loss:1.458 acc:60.00 | val loss:1.379 acc:71.20\r\n",
      "Epoch:0209 train loss:1.302 acc:61.67 | val loss:1.380 acc:70.80\r\n",
      "Epoch:0210 train loss:1.397 acc:61.67 | val loss:1.380 acc:70.80\r\n",
      "Epoch:0211 train loss:1.358 acc:62.50 | val loss:1.379 acc:71.00\r\n",
      "Epoch:0212 train loss:1.266 acc:63.33 | val loss:1.379 acc:70.80\r\n",
      "Epoch:0213 train loss:1.250 acc:66.67 | val loss:1.377 acc:71.40\r\n",
      "Epoch:0214 train loss:1.386 acc:58.33 | val loss:1.374 acc:71.20\r\n",
      "Epoch:0215 train loss:1.551 acc:60.83 | val loss:1.373 acc:70.40\r\n",
      "Epoch:0216 train loss:1.244 acc:67.50 | val loss:1.374 acc:71.40\r\n",
      "Epoch:0217 train loss:1.453 acc:60.83 | val loss:1.376 acc:71.00\r\n",
      "Epoch:0218 train loss:1.449 acc:56.67 | val loss:1.380 acc:71.40\r\n",
      "Epoch:0219 train loss:1.341 acc:60.00 | val loss:1.384 acc:71.20\r\n",
      "Epoch:0220 train loss:1.436 acc:69.17 | val loss:1.386 acc:71.20\r\n",
      "Epoch:0221 train loss:1.392 acc:65.83 | val loss:1.387 acc:71.80\r\n",
      "Epoch:0222 train loss:1.227 acc:70.83 | val loss:1.388 acc:71.40\r\n",
      "Epoch:0223 train loss:1.246 acc:60.83 | val loss:1.387 acc:71.00\r\n",
      "Epoch:0224 train loss:1.630 acc:57.50 | val loss:1.390 acc:70.80\r\n",
      "Epoch:0225 train loss:1.266 acc:61.67 | val loss:1.391 acc:70.00\r\n",
      "Epoch:0226 train loss:1.628 acc:57.50 | val loss:1.392 acc:70.40\r\n",
      "Epoch:0227 train loss:1.419 acc:64.17 | val loss:1.392 acc:70.20\r\n",
      "Epoch:0228 train loss:1.250 acc:66.67 | val loss:1.390 acc:70.00\r\n",
      "Epoch:0229 train loss:1.425 acc:62.50 | val loss:1.387 acc:70.00\r\n",
      "Epoch:0230 train loss:1.414 acc:60.00 | val loss:1.387 acc:70.60\r\n",
      "Epoch:0231 train loss:1.363 acc:68.33 | val loss:1.385 acc:70.40\r\n",
      "Epoch:0232 train loss:1.520 acc:57.50 | val loss:1.384 acc:71.40\r\n",
      "Epoch:0233 train loss:1.355 acc:65.83 | val loss:1.383 acc:71.00\r\n",
      "Epoch:0234 train loss:1.366 acc:60.83 | val loss:1.384 acc:71.00\r\n",
      "Epoch:0235 train loss:1.336 acc:68.33 | val loss:1.385 acc:71.40\r\n",
      "Epoch:0236 train loss:1.296 acc:59.17 | val loss:1.386 acc:71.60\r\n",
      "Epoch:0237 train loss:1.375 acc:60.00 | val loss:1.387 acc:71.40\r\n",
      "Epoch:0238 train loss:1.376 acc:59.17 | val loss:1.388 acc:71.40\r\n",
      "Epoch:0239 train loss:1.329 acc:58.33 | val loss:1.388 acc:71.80\r\n",
      "Epoch:0240 train loss:1.361 acc:62.50 | val loss:1.387 acc:71.40\r\n",
      "Epoch:0241 train loss:1.522 acc:55.83 | val loss:1.387 acc:71.40\r\n",
      "Epoch:0242 train loss:1.991 acc:56.67 | val loss:1.386 acc:70.80\r\n",
      "Epoch:0243 train loss:1.232 acc:63.33 | val loss:1.386 acc:69.60\r\n",
      "Epoch:0244 train loss:1.520 acc:60.00 | val loss:1.383 acc:70.00\r\n",
      "Epoch:0245 train loss:1.299 acc:62.50 | val loss:1.379 acc:71.20\r\n",
      "Epoch:0246 train loss:1.320 acc:57.50 | val loss:1.375 acc:71.60\r\n",
      "Epoch:0247 train loss:1.333 acc:56.67 | val loss:1.372 acc:71.40\r\n",
      "Epoch:0248 train loss:1.343 acc:65.83 | val loss:1.371 acc:71.80\r\n",
      "Epoch:0249 train loss:1.277 acc:67.50 | val loss:1.371 acc:71.80\r\n",
      "Epoch:0250 train loss:1.419 acc:62.50 | val loss:1.371 acc:72.20\r\n",
      "Epoch:0251 train loss:1.310 acc:65.00 | val loss:1.372 acc:71.80\r\n",
      "Epoch:0252 train loss:1.301 acc:63.33 | val loss:1.371 acc:72.00\r\n",
      "Epoch:0253 train loss:1.211 acc:69.17 | val loss:1.370 acc:71.60\r\n",
      "Epoch:0254 train loss:1.248 acc:66.67 | val loss:1.367 acc:71.20\r\n",
      "Epoch:0255 train loss:1.429 acc:58.33 | val loss:1.364 acc:71.60\r\n",
      "Epoch:0256 train loss:1.276 acc:62.50 | val loss:1.361 acc:72.00\r\n",
      "Epoch:0257 train loss:1.322 acc:65.83 | val loss:1.359 acc:72.60\r\n",
      "Epoch:0258 train loss:1.333 acc:61.67 | val loss:1.355 acc:72.20\r\n",
      "Epoch:0259 train loss:1.370 acc:57.50 | val loss:1.351 acc:72.20\r\n",
      "Epoch:0260 train loss:1.288 acc:60.00 | val loss:1.349 acc:72.20\r\n",
      "Epoch:0261 train loss:1.193 acc:64.17 | val loss:1.348 acc:72.20\r\n",
      "Epoch:0262 train loss:1.422 acc:56.67 | val loss:1.347 acc:72.20\r\n",
      "Epoch:0263 train loss:1.308 acc:61.67 | val loss:1.345 acc:71.80\r\n",
      "Epoch:0264 train loss:1.246 acc:62.50 | val loss:1.343 acc:71.80\r\n",
      "Epoch:0265 train loss:1.312 acc:63.33 | val loss:1.341 acc:71.80\r\n",
      "Epoch:0266 train loss:1.491 acc:58.33 | val loss:1.343 acc:71.40\r\n",
      "Epoch:0267 train loss:1.251 acc:67.50 | val loss:1.344 acc:71.20\r\n",
      "Epoch:0268 train loss:1.356 acc:59.17 | val loss:1.345 acc:71.60\r\n",
      "Epoch:0269 train loss:1.277 acc:63.33 | val loss:1.345 acc:71.80\r\n",
      "Epoch:0270 train loss:1.212 acc:70.00 | val loss:1.343 acc:72.60\r\n",
      "Epoch:0271 train loss:1.233 acc:65.83 | val loss:1.341 acc:71.60\r\n",
      "Epoch:0272 train loss:1.297 acc:61.67 | val loss:1.340 acc:71.80\r\n",
      "Epoch:0273 train loss:1.409 acc:56.67 | val loss:1.341 acc:71.60\r\n",
      "Epoch:0274 train loss:1.243 acc:61.67 | val loss:1.341 acc:71.60\r\n",
      "Epoch:0275 train loss:1.293 acc:64.17 | val loss:1.341 acc:71.40\r\n",
      "Epoch:0276 train loss:1.366 acc:60.83 | val loss:1.339 acc:71.40\r\n",
      "Epoch:0277 train loss:1.319 acc:63.33 | val loss:1.339 acc:71.20\r\n",
      "Epoch:0278 train loss:1.359 acc:63.33 | val loss:1.339 acc:71.00\r\n",
      "Epoch:0279 train loss:1.477 acc:67.50 | val loss:1.336 acc:71.60\r\n",
      "Epoch:0280 train loss:1.305 acc:70.83 | val loss:1.335 acc:72.20\r\n",
      "Epoch:0281 train loss:1.251 acc:63.33 | val loss:1.333 acc:72.00\r\n",
      "Epoch:0282 train loss:1.242 acc:65.00 | val loss:1.332 acc:72.20\r\n",
      "Epoch:0283 train loss:1.318 acc:63.33 | val loss:1.332 acc:72.00\r\n",
      "Epoch:0284 train loss:1.289 acc:65.00 | val loss:1.332 acc:71.80\r\n",
      "Epoch:0285 train loss:1.318 acc:68.33 | val loss:1.332 acc:72.20\r\n",
      "Epoch:0286 train loss:1.274 acc:60.00 | val loss:1.332 acc:72.20\r\n",
      "Epoch:0287 train loss:1.379 acc:61.67 | val loss:1.333 acc:72.00\r\n",
      "Epoch:0288 train loss:1.396 acc:61.67 | val loss:1.336 acc:70.80\r\n",
      "Epoch:0289 train loss:1.249 acc:63.33 | val loss:1.338 acc:70.80\r\n",
      "Epoch:0290 train loss:1.225 acc:63.33 | val loss:1.339 acc:70.80\r\n",
      "Epoch:0291 train loss:1.251 acc:59.17 | val loss:1.338 acc:70.80\r\n",
      "Epoch:0292 train loss:1.262 acc:61.67 | val loss:1.338 acc:71.00\r\n",
      "Epoch:0293 train loss:1.291 acc:62.50 | val loss:1.336 acc:71.40\r\n",
      "Epoch:0294 train loss:1.224 acc:59.17 | val loss:1.335 acc:71.40\r\n",
      "Epoch:0295 train loss:1.294 acc:57.50 | val loss:1.334 acc:71.20\r\n",
      "Epoch:0296 train loss:1.277 acc:63.33 | val loss:1.334 acc:71.20\r\n",
      "Epoch:0297 train loss:1.236 acc:64.17 | val loss:1.335 acc:71.20\r\n",
      "Epoch:0298 train loss:1.198 acc:62.50 | val loss:1.334 acc:71.60\r\n",
      "Epoch:0299 train loss:1.343 acc:59.17 | val loss:1.333 acc:72.00\r\n",
      "Epoch:0300 train loss:1.275 acc:65.83 | val loss:1.332 acc:72.20\r\n",
      "Epoch:0301 train loss:1.327 acc:60.83 | val loss:1.331 acc:72.00\r\n",
      "Epoch:0302 train loss:1.301 acc:65.83 | val loss:1.329 acc:71.60\r\n",
      "Epoch:0303 train loss:1.233 acc:65.00 | val loss:1.328 acc:71.00\r\n",
      "Epoch:0304 train loss:1.334 acc:63.33 | val loss:1.324 acc:71.40\r\n",
      "Epoch:0305 train loss:1.293 acc:58.33 | val loss:1.323 acc:71.60\r\n",
      "Epoch:0306 train loss:1.287 acc:60.83 | val loss:1.322 acc:71.80\r\n",
      "Epoch:0307 train loss:1.207 acc:65.83 | val loss:1.319 acc:71.80\r\n",
      "Epoch:0308 train loss:1.207 acc:69.17 | val loss:1.315 acc:71.40\r\n",
      "Epoch:0309 train loss:1.237 acc:63.33 | val loss:1.314 acc:71.40\r\n",
      "Epoch:0310 train loss:1.282 acc:65.00 | val loss:1.312 acc:71.00\r\n",
      "Epoch:0311 train loss:1.231 acc:62.50 | val loss:1.311 acc:71.40\r\n",
      "Epoch:0312 train loss:1.470 acc:55.83 | val loss:1.312 acc:71.40\r\n",
      "Epoch:0313 train loss:1.275 acc:63.33 | val loss:1.313 acc:71.20\r\n",
      "Epoch:0314 train loss:1.265 acc:72.50 | val loss:1.313 acc:71.00\r\n",
      "Epoch:0315 train loss:1.745 acc:70.00 | val loss:1.313 acc:71.20\r\n",
      "Epoch:0316 train loss:1.266 acc:62.50 | val loss:1.312 acc:71.60\r\n",
      "Epoch:0317 train loss:1.184 acc:61.67 | val loss:1.307 acc:71.60\r\n",
      "Epoch:0318 train loss:1.364 acc:63.33 | val loss:1.303 acc:72.00\r\n",
      "Epoch:0319 train loss:1.338 acc:62.50 | val loss:1.298 acc:72.40\r\n",
      "Epoch:0320 train loss:1.470 acc:65.00 | val loss:1.296 acc:72.40\r\n",
      "Epoch:0321 train loss:1.448 acc:60.00 | val loss:1.297 acc:72.60\r\n",
      "Epoch:0322 train loss:1.413 acc:64.17 | val loss:1.298 acc:72.40\r\n",
      "Epoch:0323 train loss:1.246 acc:66.67 | val loss:1.301 acc:71.60\r\n",
      "Epoch:0324 train loss:1.326 acc:61.67 | val loss:1.305 acc:72.00\r\n",
      "Epoch:0325 train loss:1.210 acc:64.17 | val loss:1.310 acc:72.20\r\n",
      "Epoch:0326 train loss:1.297 acc:62.50 | val loss:1.314 acc:72.00\r\n",
      "Epoch:0327 train loss:1.954 acc:63.33 | val loss:1.322 acc:71.40\r\n",
      "Epoch:0328 train loss:1.331 acc:64.17 | val loss:1.332 acc:70.80\r\n",
      "Epoch:0329 train loss:1.243 acc:61.67 | val loss:1.345 acc:69.00\r\n",
      "Epoch:0330 train loss:1.274 acc:61.67 | val loss:1.356 acc:68.20\r\n",
      "Epoch:0331 train loss:1.552 acc:60.83 | val loss:1.363 acc:67.60\r\n",
      "Epoch:0332 train loss:1.373 acc:55.00 | val loss:1.366 acc:68.00\r\n",
      "Epoch:0333 train loss:1.298 acc:60.83 | val loss:1.365 acc:68.80\r\n",
      "Epoch:0334 train loss:1.255 acc:68.33 | val loss:1.363 acc:69.60\r\n",
      "Epoch:0335 train loss:1.306 acc:60.83 | val loss:1.363 acc:69.60\r\n",
      "Epoch:0336 train loss:1.323 acc:62.50 | val loss:1.360 acc:70.80\r\n",
      "Epoch:0337 train loss:1.395 acc:55.83 | val loss:1.356 acc:71.20\r\n",
      "Epoch:0338 train loss:1.286 acc:60.83 | val loss:1.353 acc:71.40\r\n",
      "Epoch:0339 train loss:1.253 acc:64.17 | val loss:1.351 acc:71.60\r\n",
      "Epoch:0340 train loss:1.302 acc:63.33 | val loss:1.348 acc:73.00\r\n",
      "Epoch:0341 train loss:1.279 acc:55.83 | val loss:1.345 acc:72.80\r\n",
      "Epoch:0342 train loss:1.252 acc:65.83 | val loss:1.341 acc:72.80\r\n",
      "Epoch:0343 train loss:1.332 acc:67.50 | val loss:1.337 acc:72.40\r\n",
      "Epoch:0344 train loss:1.202 acc:68.33 | val loss:1.333 acc:72.60\r\n",
      "Epoch:0345 train loss:1.345 acc:64.17 | val loss:1.331 acc:71.80\r\n",
      "Epoch:0346 train loss:1.250 acc:60.83 | val loss:1.327 acc:71.40\r\n",
      "Epoch:0347 train loss:1.298 acc:69.17 | val loss:1.323 acc:72.40\r\n",
      "Epoch:0348 train loss:1.206 acc:63.33 | val loss:1.320 acc:72.40\r\n",
      "Epoch:0349 train loss:1.270 acc:65.83 | val loss:1.319 acc:71.80\r\n",
      "Epoch:0350 train loss:1.282 acc:65.00 | val loss:1.317 acc:71.80\r\n",
      "Epoch:0351 train loss:1.163 acc:66.67 | val loss:1.314 acc:72.40\r\n",
      "Epoch:0352 train loss:1.365 acc:65.00 | val loss:1.311 acc:72.60\r\n",
      "Epoch:0353 train loss:1.256 acc:62.50 | val loss:1.308 acc:72.60\r\n",
      "Epoch:0354 train loss:1.362 acc:65.83 | val loss:1.306 acc:72.80\r\n",
      "Epoch:0355 train loss:1.242 acc:69.17 | val loss:1.305 acc:72.00\r\n",
      "Epoch:0356 train loss:1.262 acc:65.00 | val loss:1.305 acc:71.40\r\n",
      "Epoch:0357 train loss:1.216 acc:67.50 | val loss:1.306 acc:71.20\r\n",
      "Epoch:0358 train loss:1.456 acc:70.83 | val loss:1.307 acc:71.40\r\n",
      "Epoch:0359 train loss:1.226 acc:57.50 | val loss:1.309 acc:71.60\r\n",
      "Epoch:0360 train loss:1.276 acc:64.17 | val loss:1.309 acc:71.80\r\n",
      "Epoch:0361 train loss:1.226 acc:69.17 | val loss:1.308 acc:71.80\r\n",
      "Epoch:0362 train loss:1.284 acc:71.67 | val loss:1.304 acc:71.80\r\n",
      "Epoch:0363 train loss:1.127 acc:74.17 | val loss:1.301 acc:71.40\r\n",
      "Epoch:0364 train loss:1.537 acc:68.33 | val loss:1.300 acc:71.40\r\n",
      "Epoch:0365 train loss:1.363 acc:57.50 | val loss:1.300 acc:71.40\r\n",
      "Epoch:0366 train loss:1.153 acc:70.83 | val loss:1.300 acc:71.20\r\n",
      "Epoch:0367 train loss:1.384 acc:62.50 | val loss:1.301 acc:71.40\r\n",
      "Epoch:0368 train loss:1.300 acc:62.50 | val loss:1.301 acc:71.20\r\n",
      "Epoch:0369 train loss:1.287 acc:60.00 | val loss:1.301 acc:71.00\r\n",
      "Epoch:0370 train loss:1.184 acc:63.33 | val loss:1.302 acc:71.00\r\n",
      "Epoch:0371 train loss:1.782 acc:57.50 | val loss:1.302 acc:71.40\r\n",
      "Epoch:0372 train loss:1.307 acc:65.83 | val loss:1.302 acc:71.20\r\n",
      "Epoch:0373 train loss:1.186 acc:65.83 | val loss:1.302 acc:70.80\r\n",
      "Epoch:0374 train loss:1.194 acc:61.67 | val loss:1.302 acc:70.60\r\n",
      "Epoch:0375 train loss:1.385 acc:56.67 | val loss:1.302 acc:70.80\r\n",
      "Epoch:0376 train loss:1.206 acc:59.17 | val loss:1.301 acc:71.00\r\n",
      "Epoch:0377 train loss:1.213 acc:59.17 | val loss:1.299 acc:70.80\r\n",
      "Epoch:0378 train loss:1.191 acc:59.17 | val loss:1.296 acc:70.60\r\n",
      "Epoch:0379 train loss:1.619 acc:59.17 | val loss:1.296 acc:71.00\r\n",
      "Epoch:0380 train loss:1.177 acc:69.17 | val loss:1.297 acc:71.00\r\n",
      "Epoch:0381 train loss:1.281 acc:64.17 | val loss:1.300 acc:71.00\r\n",
      "Epoch:0382 train loss:1.619 acc:65.00 | val loss:1.301 acc:71.20\r\n",
      "Epoch:0383 train loss:1.280 acc:59.17 | val loss:1.303 acc:71.20\r\n",
      "Epoch:0384 train loss:1.259 acc:68.33 | val loss:1.302 acc:72.00\r\n",
      "Epoch:0385 train loss:1.467 acc:60.83 | val loss:1.303 acc:72.00\r\n",
      "Epoch:0386 train loss:1.247 acc:62.50 | val loss:1.305 acc:71.80\r\n",
      "Epoch:0387 train loss:1.646 acc:64.17 | val loss:1.307 acc:72.00\r\n",
      "Epoch:0388 train loss:1.244 acc:66.67 | val loss:1.309 acc:71.40\r\n",
      "Epoch:0389 train loss:1.219 acc:68.33 | val loss:1.312 acc:71.20\r\n",
      "Epoch:0390 train loss:1.174 acc:70.83 | val loss:1.312 acc:71.00\r\n",
      "Epoch:0391 train loss:1.295 acc:68.33 | val loss:1.312 acc:71.20\r\n",
      "Epoch:0392 train loss:1.324 acc:62.50 | val loss:1.311 acc:71.20\r\n",
      "Epoch:0393 train loss:1.122 acc:72.50 | val loss:1.309 acc:71.20\r\n",
      "Epoch:0394 train loss:1.244 acc:61.67 | val loss:1.308 acc:71.20\r\n",
      "Epoch:0395 train loss:214.118 acc:60.00 | val loss:1.308 acc:70.60\r\n",
      "Epoch:0396 train loss:1.354 acc:62.50 | val loss:1.310 acc:71.00\r\n",
      "Epoch:0397 train loss:1.498 acc:63.33 | val loss:1.313 acc:70.80\r\n",
      "Epoch:0398 train loss:1.300 acc:61.67 | val loss:1.314 acc:70.00\r\n",
      "Epoch:0399 train loss:1.413 acc:58.33 | val loss:1.313 acc:71.80\r\n",
      "Epoch:0400 train loss:1.182 acc:71.67 | val loss:1.312 acc:70.40\r\n",
      "Epoch:0401 train loss:1.323 acc:63.33 | val loss:1.315 acc:70.80\r\n",
      "Epoch:0402 train loss:1.272 acc:67.50 | val loss:1.320 acc:70.80\r\n",
      "Epoch:0403 train loss:1.561 acc:55.83 | val loss:1.325 acc:71.00\r\n",
      "Epoch:0404 train loss:1.201 acc:63.33 | val loss:1.329 acc:70.80\r\n",
      "Epoch:0405 train loss:1.311 acc:65.83 | val loss:1.331 acc:70.60\r\n",
      "Epoch:0406 train loss:1.206 acc:66.67 | val loss:1.331 acc:70.00\r\n",
      "Epoch:0407 train loss:1.246 acc:67.50 | val loss:1.329 acc:70.60\r\n",
      "Epoch:0408 train loss:1.642 acc:60.83 | val loss:1.328 acc:70.80\r\n",
      "Epoch:0409 train loss:1.193 acc:68.33 | val loss:1.327 acc:71.20\r\n",
      "Epoch:0410 train loss:1.247 acc:58.33 | val loss:1.324 acc:70.80\r\n",
      "Epoch:0411 train loss:1.308 acc:67.50 | val loss:1.321 acc:71.20\r\n",
      "Epoch:0412 train loss:1.215 acc:65.00 | val loss:1.318 acc:71.60\r\n",
      "Epoch:0413 train loss:1.278 acc:61.67 | val loss:1.317 acc:71.80\r\n",
      "Epoch:0414 train loss:1.266 acc:67.50 | val loss:1.316 acc:71.80\r\n",
      "Epoch:0415 train loss:1.315 acc:60.83 | val loss:1.315 acc:71.80\r\n",
      "Epoch:0416 train loss:1.288 acc:66.67 | val loss:1.313 acc:72.20\r\n",
      "Epoch:0417 train loss:1.250 acc:64.17 | val loss:1.311 acc:72.00\r\n",
      "Epoch:0418 train loss:1.150 acc:77.50 | val loss:1.311 acc:71.60\r\n",
      "Epoch:0419 train loss:1.268 acc:62.50 | val loss:1.312 acc:72.00\r\n",
      "Epoch:0420 train loss:1.245 acc:60.83 | val loss:1.313 acc:72.20\r\n",
      "Epoch:0421 train loss:1.247 acc:60.00 | val loss:1.314 acc:71.20\r\n",
      "Epoch:0422 train loss:1.254 acc:69.17 | val loss:1.315 acc:70.60\r\n",
      "Epoch:0423 train loss:1.267 acc:66.67 | val loss:1.315 acc:70.60\r\n",
      "Epoch:0424 train loss:1.243 acc:60.00 | val loss:1.313 acc:70.80\r\n",
      "Epoch:0425 train loss:1.264 acc:67.50 | val loss:1.311 acc:72.20\r\n",
      "Epoch:0426 train loss:1.220 acc:65.83 | val loss:1.310 acc:73.00\r\n",
      "Epoch:0427 train loss:1.179 acc:70.83 | val loss:1.307 acc:72.80\r\n",
      "Epoch:0428 train loss:1.247 acc:65.00 | val loss:1.305 acc:72.40\r\n",
      "Epoch:0429 train loss:1.188 acc:73.33 | val loss:1.306 acc:72.40\r\n",
      "Epoch:0430 train loss:1.203 acc:65.00 | val loss:1.307 acc:72.40\r\n",
      "Epoch:0431 train loss:1.308 acc:60.00 | val loss:1.307 acc:71.80\r\n",
      "Epoch:0432 train loss:1.242 acc:62.50 | val loss:1.307 acc:71.40\r\n",
      "Epoch:0433 train loss:1.148 acc:67.50 | val loss:1.307 acc:71.40\r\n",
      "Epoch:0434 train loss:1.224 acc:69.17 | val loss:1.305 acc:71.60\r\n",
      "Epoch:0435 train loss:1.186 acc:65.00 | val loss:1.302 acc:72.00\r\n",
      "Epoch:0436 train loss:1.209 acc:66.67 | val loss:1.298 acc:72.00\r\n",
      "Epoch:0437 train loss:1.249 acc:64.17 | val loss:1.295 acc:72.40\r\n",
      "Epoch:0438 train loss:1.190 acc:68.33 | val loss:1.292 acc:72.00\r\n",
      "Epoch:0439 train loss:1.276 acc:67.50 | val loss:1.291 acc:71.00\r\n",
      "Epoch:0440 train loss:1.220 acc:65.00 | val loss:1.290 acc:70.80\r\n",
      "Epoch:0441 train loss:1.210 acc:66.67 | val loss:1.288 acc:71.00\r\n",
      "Epoch:0442 train loss:1.232 acc:55.00 | val loss:1.287 acc:70.80\r\n",
      "Epoch:0443 train loss:1.173 acc:64.17 | val loss:1.286 acc:71.00\r\n",
      "Epoch:0444 train loss:1.236 acc:62.50 | val loss:1.284 acc:71.00\r\n",
      "Epoch:0445 train loss:1.229 acc:60.83 | val loss:1.281 acc:71.00\r\n",
      "Epoch:0446 train loss:1.083 acc:73.33 | val loss:1.278 acc:71.20\r\n",
      "Epoch:0447 train loss:1.181 acc:71.67 | val loss:1.277 acc:71.40\r\n",
      "Epoch:0448 train loss:1.172 acc:65.00 | val loss:1.274 acc:71.40\r\n",
      "Epoch:0449 train loss:1.173 acc:65.00 | val loss:1.272 acc:71.60\r\n",
      "Epoch:0450 train loss:1.181 acc:67.50 | val loss:1.269 acc:72.00\r\n",
      "Epoch:0451 train loss:1.476 acc:65.83 | val loss:1.269 acc:71.80\r\n",
      "Epoch:0452 train loss:1.167 acc:66.67 | val loss:1.269 acc:71.80\r\n",
      "Epoch:0453 train loss:1.196 acc:69.17 | val loss:1.268 acc:71.40\r\n",
      "Epoch:0454 train loss:1.215 acc:67.50 | val loss:1.267 acc:71.20\r\n",
      "Epoch:0455 train loss:1.175 acc:69.17 | val loss:1.267 acc:72.00\r\n",
      "Epoch:0456 train loss:1.122 acc:74.17 | val loss:1.266 acc:71.80\r\n",
      "Epoch:0457 train loss:1.262 acc:57.50 | val loss:1.265 acc:71.80\r\n",
      "Epoch:0458 train loss:1.557 acc:62.50 | val loss:1.266 acc:71.40\r\n",
      "Epoch:0459 train loss:1.210 acc:66.67 | val loss:1.268 acc:71.20\r\n",
      "Epoch:0460 train loss:1.189 acc:60.83 | val loss:1.270 acc:71.20\r\n",
      "Epoch:0461 train loss:1.275 acc:63.33 | val loss:1.271 acc:70.80\r\n",
      "Epoch:0462 train loss:1.199 acc:70.83 | val loss:1.273 acc:71.20\r\n",
      "Epoch:0463 train loss:1.278 acc:62.50 | val loss:1.277 acc:70.40\r\n",
      "Epoch:0464 train loss:1.260 acc:57.50 | val loss:1.280 acc:70.40\r\n",
      "Epoch:0465 train loss:1.205 acc:60.83 | val loss:1.283 acc:70.20\r\n",
      "Epoch:0466 train loss:1.216 acc:64.17 | val loss:1.285 acc:70.00\r\n",
      "Epoch:0467 train loss:1.447 acc:62.50 | val loss:1.284 acc:70.80\r\n",
      "Epoch:0468 train loss:1.206 acc:60.00 | val loss:1.284 acc:71.60\r\n",
      "Epoch:0469 train loss:1.189 acc:64.17 | val loss:1.282 acc:71.80\r\n",
      "Epoch:0470 train loss:1.210 acc:65.00 | val loss:1.278 acc:71.80\r\n",
      "Epoch:0471 train loss:1.224 acc:69.17 | val loss:1.276 acc:71.60\r\n",
      "Epoch:0472 train loss:1.151 acc:67.50 | val loss:1.275 acc:71.60\r\n",
      "Epoch:0473 train loss:1.151 acc:67.50 | val loss:1.273 acc:71.80\r\n",
      "Epoch:0474 train loss:1.210 acc:63.33 | val loss:1.271 acc:72.00\r\n",
      "Epoch:0475 train loss:1.265 acc:65.83 | val loss:1.271 acc:71.80\r\n",
      "Epoch:0476 train loss:1.195 acc:64.17 | val loss:1.271 acc:71.60\r\n",
      "Epoch:0477 train loss:1.188 acc:66.67 | val loss:1.270 acc:71.40\r\n",
      "Epoch:0478 train loss:2.315 acc:69.17 | val loss:1.269 acc:71.80\r\n",
      "Epoch:0479 train loss:1.173 acc:65.00 | val loss:1.270 acc:72.00\r\n",
      "Epoch:0480 train loss:1.157 acc:69.17 | val loss:1.270 acc:72.20\r\n",
      "Epoch:0481 train loss:1.267 acc:60.83 | val loss:1.269 acc:71.80\r\n",
      "Epoch:0482 train loss:1.292 acc:60.00 | val loss:1.269 acc:71.80\r\n",
      "Epoch:0483 train loss:1.225 acc:67.50 | val loss:1.270 acc:71.40\r\n",
      "Epoch:0484 train loss:1.230 acc:68.33 | val loss:1.268 acc:71.60\r\n",
      "Epoch:0485 train loss:1.203 acc:61.67 | val loss:1.264 acc:71.60\r\n",
      "Epoch:0486 train loss:1.132 acc:71.67 | val loss:1.259 acc:71.80\r\n",
      "Epoch:0487 train loss:1.156 acc:60.83 | val loss:1.256 acc:71.60\r\n",
      "Epoch:0488 train loss:1.190 acc:60.83 | val loss:1.253 acc:71.80\r\n",
      "Epoch:0489 train loss:1.155 acc:70.83 | val loss:1.250 acc:71.80\r\n",
      "Epoch:0490 train loss:1.166 acc:70.83 | val loss:1.249 acc:72.00\r\n",
      "Epoch:0491 train loss:1.397 acc:61.67 | val loss:1.252 acc:71.60\r\n",
      "Epoch:0492 train loss:1.260 acc:69.17 | val loss:1.254 acc:71.60\r\n",
      "Epoch:0493 train loss:1.199 acc:70.83 | val loss:1.256 acc:71.80\r\n",
      "Epoch:0494 train loss:1.298 acc:66.67 | val loss:1.258 acc:71.80\r\n",
      "Epoch:0495 train loss:1.169 acc:61.67 | val loss:1.259 acc:72.00\r\n",
      "Epoch:0496 train loss:2.236 acc:60.83 | val loss:1.262 acc:71.80\r\n",
      "Epoch:0497 train loss:1.191 acc:66.67 | val loss:1.266 acc:71.20\r\n",
      "Epoch:0498 train loss:1.251 acc:68.33 | val loss:1.270 acc:71.20\r\n",
      "Epoch:0499 train loss:1.183 acc:69.17 | val loss:1.272 acc:70.60\r\n",
      "Epoch:0500 train loss:1.193 acc:65.83 | val loss:1.273 acc:71.40\r\n",
      "Epoch:0501 train loss:1.555 acc:66.67 | val loss:1.277 acc:71.80\r\n",
      "Epoch:0502 train loss:1.308 acc:65.00 | val loss:1.284 acc:71.00\r\n",
      "Epoch:0503 train loss:1.431 acc:63.33 | val loss:1.289 acc:71.00\r\n",
      "Epoch:0504 train loss:1.289 acc:58.33 | val loss:1.290 acc:70.60\r\n",
      "Epoch:0505 train loss:1.714 acc:70.83 | val loss:1.289 acc:70.40\r\n",
      "Epoch:0506 train loss:1.177 acc:66.67 | val loss:1.288 acc:70.20\r\n",
      "Epoch:0507 train loss:1.214 acc:63.33 | val loss:1.286 acc:70.00\r\n",
      "Epoch:0508 train loss:1.266 acc:62.50 | val loss:1.285 acc:70.40\r\n",
      "Epoch:0509 train loss:1.141 acc:67.50 | val loss:1.284 acc:70.40\r\n",
      "Epoch:0510 train loss:1.311 acc:59.17 | val loss:1.284 acc:70.20\r\n",
      "Epoch:0511 train loss:1.192 acc:65.00 | val loss:1.283 acc:69.60\r\n",
      "Epoch:0512 train loss:1.178 acc:65.00 | val loss:1.283 acc:69.40\r\n",
      "Epoch:0513 train loss:1.223 acc:70.00 | val loss:1.282 acc:69.60\r\n",
      "Epoch:0514 train loss:1.254 acc:60.00 | val loss:1.280 acc:70.00\r\n",
      "Epoch:0515 train loss:1.158 acc:69.17 | val loss:1.277 acc:69.80\r\n",
      "Epoch:0516 train loss:1.146 acc:65.83 | val loss:1.274 acc:70.80\r\n",
      "Epoch:0517 train loss:1.140 acc:62.50 | val loss:1.272 acc:71.00\r\n",
      "Epoch:0518 train loss:1.157 acc:60.00 | val loss:1.269 acc:71.80\r\n",
      "Epoch:0519 train loss:1.218 acc:61.67 | val loss:1.268 acc:71.60\r\n",
      "Epoch:0520 train loss:1.060 acc:70.83 | val loss:1.267 acc:71.40\r\n",
      "Epoch:0521 train loss:1.172 acc:70.00 | val loss:1.265 acc:71.80\r\n",
      "Epoch:0522 train loss:1.162 acc:67.50 | val loss:1.265 acc:71.60\r\n",
      "Epoch:0523 train loss:1.246 acc:58.33 | val loss:1.264 acc:71.00\r\n",
      "Epoch:0524 train loss:1.136 acc:63.33 | val loss:1.263 acc:71.20\r\n",
      "Epoch:0525 train loss:1.142 acc:72.50 | val loss:1.261 acc:71.20\r\n",
      "Epoch:0526 train loss:1.245 acc:62.50 | val loss:1.259 acc:71.00\r\n",
      "Epoch:0527 train loss:1.278 acc:63.33 | val loss:1.256 acc:71.40\r\n",
      "Epoch:0528 train loss:1.170 acc:68.33 | val loss:1.252 acc:72.00\r\n",
      "Epoch:0529 train loss:1.175 acc:62.50 | val loss:1.250 acc:71.60\r\n",
      "Epoch:0530 train loss:1.149 acc:69.17 | val loss:1.249 acc:71.80\r\n",
      "Epoch:0531 train loss:1.191 acc:68.33 | val loss:1.250 acc:72.20\r\n",
      "Epoch:0532 train loss:1.180 acc:71.67 | val loss:1.250 acc:71.60\r\n",
      "Epoch:0533 train loss:1.294 acc:57.50 | val loss:1.251 acc:71.40\r\n",
      "Epoch:0534 train loss:1.133 acc:65.83 | val loss:1.251 acc:71.00\r\n",
      "Epoch:0535 train loss:1.172 acc:64.17 | val loss:1.250 acc:71.20\r\n",
      "Epoch:0536 train loss:1.161 acc:63.33 | val loss:1.250 acc:71.40\r\n",
      "Epoch:0537 train loss:1.196 acc:66.67 | val loss:1.253 acc:71.40\r\n",
      "Epoch:0538 train loss:1.117 acc:63.33 | val loss:1.255 acc:71.40\r\n",
      "Epoch:0539 train loss:1.226 acc:61.67 | val loss:1.254 acc:71.40\r\n",
      "Epoch:0540 train loss:1.125 acc:70.00 | val loss:1.253 acc:71.20\r\n",
      "Epoch:0541 train loss:1.233 acc:67.50 | val loss:1.253 acc:70.80\r\n",
      "Epoch:0542 train loss:1.291 acc:67.50 | val loss:1.254 acc:70.60\r\n",
      "Epoch:0543 train loss:1.047 acc:75.83 | val loss:1.254 acc:70.60\r\n",
      "Epoch:0544 train loss:1.159 acc:59.17 | val loss:1.252 acc:71.00\r\n",
      "Epoch:0545 train loss:1.188 acc:65.83 | val loss:1.248 acc:71.20\r\n",
      "Epoch:0546 train loss:1.233 acc:70.00 | val loss:1.244 acc:71.20\r\n",
      "Epoch:0547 train loss:1.225 acc:71.67 | val loss:1.242 acc:71.20\r\n",
      "Epoch:0548 train loss:1.159 acc:61.67 | val loss:1.240 acc:71.20\r\n",
      "Epoch:0549 train loss:1.164 acc:69.17 | val loss:1.238 acc:71.20\r\n",
      "Epoch:0550 train loss:1.953 acc:70.00 | val loss:1.236 acc:71.60\r\n",
      "Epoch:0551 train loss:1.179 acc:60.83 | val loss:1.234 acc:71.60\r\n",
      "Epoch:0552 train loss:1.174 acc:66.67 | val loss:1.233 acc:71.60\r\n",
      "Epoch:0553 train loss:1.167 acc:62.50 | val loss:1.234 acc:71.20\r\n",
      "Epoch:0554 train loss:1.238 acc:65.00 | val loss:1.235 acc:72.00\r\n",
      "Epoch:0555 train loss:1.163 acc:71.67 | val loss:1.238 acc:70.80\r\n",
      "Epoch:0556 train loss:1.561 acc:59.17 | val loss:1.240 acc:70.40\r\n",
      "Epoch:0557 train loss:1.249 acc:68.33 | val loss:1.242 acc:70.80\r\n",
      "Epoch:0558 train loss:1.163 acc:66.67 | val loss:1.244 acc:72.00\r\n",
      "Epoch:0559 train loss:1.181 acc:66.67 | val loss:1.248 acc:71.80\r\n",
      "Epoch:0560 train loss:1.058 acc:66.67 | val loss:1.252 acc:71.40\r\n",
      "Epoch:0561 train loss:1.214 acc:66.67 | val loss:1.254 acc:71.40\r\n",
      "Epoch:0562 train loss:1.241 acc:61.67 | val loss:1.254 acc:71.40\r\n",
      "Epoch:0563 train loss:1.156 acc:70.83 | val loss:1.253 acc:71.80\r\n",
      "Epoch:0564 train loss:1.083 acc:64.17 | val loss:1.252 acc:71.00\r\n",
      "Epoch:0565 train loss:1.158 acc:66.67 | val loss:1.253 acc:71.00\r\n",
      "Epoch:0566 train loss:1.312 acc:65.83 | val loss:1.253 acc:70.80\r\n",
      "Epoch:0567 train loss:1.182 acc:62.50 | val loss:1.254 acc:70.80\r\n",
      "Epoch:0568 train loss:1.212 acc:65.83 | val loss:1.254 acc:70.40\r\n",
      "Epoch:0569 train loss:1.335 acc:63.33 | val loss:1.256 acc:70.60\r\n",
      "Epoch:0570 train loss:1.147 acc:70.00 | val loss:1.259 acc:71.00\r\n",
      "Epoch:0571 train loss:1.235 acc:66.67 | val loss:1.264 acc:70.80\r\n",
      "Epoch:0572 train loss:1.195 acc:69.17 | val loss:1.267 acc:69.60\r\n",
      "Epoch:0573 train loss:1.120 acc:63.33 | val loss:1.267 acc:69.00\r\n",
      "Epoch:0574 train loss:1.200 acc:59.17 | val loss:1.267 acc:69.40\r\n",
      "Epoch:0575 train loss:1.307 acc:66.67 | val loss:1.264 acc:69.60\r\n",
      "Epoch:0576 train loss:1.066 acc:74.17 | val loss:1.262 acc:69.60\r\n",
      "Epoch:0577 train loss:1.246 acc:64.17 | val loss:1.258 acc:70.60\r\n",
      "Epoch:0578 train loss:1.147 acc:64.17 | val loss:1.255 acc:70.60\r\n",
      "Epoch:0579 train loss:1.211 acc:62.50 | val loss:1.253 acc:70.60\r\n",
      "Epoch:0580 train loss:1.206 acc:64.17 | val loss:1.250 acc:70.60\r\n",
      "Epoch:0581 train loss:1.135 acc:67.50 | val loss:1.246 acc:70.60\r\n",
      "Epoch:0582 train loss:1.296 acc:62.50 | val loss:1.246 acc:70.60\r\n",
      "Epoch:0583 train loss:1.161 acc:65.00 | val loss:1.247 acc:71.20\r\n",
      "Epoch:0584 train loss:1.212 acc:62.50 | val loss:1.248 acc:71.60\r\n",
      "Epoch:0585 train loss:1.098 acc:68.33 | val loss:1.249 acc:70.80\r\n",
      "Epoch:0586 train loss:1.222 acc:64.17 | val loss:1.250 acc:70.60\r\n",
      "Epoch:0587 train loss:1.235 acc:60.83 | val loss:1.254 acc:71.00\r\n",
      "Epoch:0588 train loss:1.305 acc:66.67 | val loss:1.257 acc:71.00\r\n",
      "Epoch:0589 train loss:1.326 acc:62.50 | val loss:1.261 acc:70.60\r\n",
      "Epoch:0590 train loss:1.100 acc:68.33 | val loss:1.264 acc:70.80\r\n",
      "Epoch:0591 train loss:1.195 acc:60.83 | val loss:1.264 acc:70.20\r\n",
      "Epoch:0592 train loss:1.301 acc:67.50 | val loss:1.267 acc:70.60\r\n",
      "Epoch:0593 train loss:1.730 acc:65.00 | val loss:1.272 acc:71.00\r\n",
      "Epoch:0594 train loss:1.119 acc:70.00 | val loss:1.273 acc:71.00\r\n",
      "Epoch:0595 train loss:1.228 acc:58.33 | val loss:1.271 acc:71.20\r\n",
      "Epoch:0596 train loss:1.334 acc:70.83 | val loss:1.268 acc:71.60\r\n",
      "Epoch:0597 train loss:1.308 acc:63.33 | val loss:1.268 acc:71.80\r\n",
      "Epoch:0598 train loss:1.222 acc:66.67 | val loss:1.266 acc:71.40\r\n",
      "Epoch:0599 train loss:1.144 acc:72.50 | val loss:1.264 acc:71.60\r\n",
      "Epoch:0600 train loss:1.131 acc:70.00 | val loss:1.262 acc:71.40\r\n",
      "Epoch:0601 train loss:1.146 acc:69.17 | val loss:1.261 acc:71.80\r\n",
      "Epoch:0602 train loss:1.199 acc:64.17 | val loss:1.261 acc:71.60\r\n",
      "Epoch:0603 train loss:1.174 acc:67.50 | val loss:1.262 acc:71.40\r\n",
      "Epoch:0604 train loss:1.505 acc:58.33 | val loss:1.264 acc:71.00\r\n",
      "Epoch:0605 train loss:1.233 acc:65.00 | val loss:1.265 acc:70.80\r\n",
      "Epoch:0606 train loss:1.282 acc:61.67 | val loss:1.266 acc:71.00\r\n",
      "Epoch:0607 train loss:1.274 acc:66.67 | val loss:1.267 acc:70.60\r\n",
      "Epoch:0608 train loss:1.206 acc:63.33 | val loss:1.265 acc:70.60\r\n",
      "Epoch:0609 train loss:1.234 acc:64.17 | val loss:1.264 acc:70.60\r\n",
      "Epoch:0610 train loss:1.205 acc:69.17 | val loss:1.262 acc:70.60\r\n",
      "Epoch:0611 train loss:1.164 acc:61.67 | val loss:1.263 acc:69.80\r\n",
      "Epoch:0612 train loss:1.254 acc:69.17 | val loss:1.265 acc:70.00\r\n",
      "Epoch:0613 train loss:1.048 acc:70.00 | val loss:1.265 acc:69.60\r\n",
      "Epoch:0614 train loss:1.359 acc:61.67 | val loss:1.269 acc:69.00\r\n",
      "Epoch:0615 train loss:1.082 acc:71.67 | val loss:1.272 acc:68.80\r\n",
      "Epoch:0616 train loss:1.144 acc:61.67 | val loss:1.273 acc:68.80\r\n",
      "Epoch:0617 train loss:1.219 acc:65.00 | val loss:1.273 acc:69.00\r\n",
      "Epoch:0618 train loss:1.210 acc:67.50 | val loss:1.271 acc:68.80\r\n",
      "Epoch:0619 train loss:1.101 acc:64.17 | val loss:1.268 acc:69.00\r\n",
      "Epoch:0620 train loss:1.148 acc:70.00 | val loss:1.264 acc:69.20\r\n",
      "Epoch:0621 train loss:1.212 acc:64.17 | val loss:1.260 acc:69.20\r\n",
      "Epoch:0622 train loss:1.322 acc:64.17 | val loss:1.258 acc:70.40\r\n",
      "Epoch:0623 train loss:1.181 acc:58.33 | val loss:1.256 acc:71.20\r\n",
      "Epoch:0624 train loss:1.168 acc:63.33 | val loss:1.253 acc:72.20\r\n",
      "Epoch:0625 train loss:1.113 acc:74.17 | val loss:1.251 acc:71.40\r\n",
      "Epoch:0626 train loss:1.199 acc:66.67 | val loss:1.251 acc:71.40\r\n",
      "Epoch:0627 train loss:1.118 acc:57.50 | val loss:1.252 acc:72.00\r\n",
      "Epoch:0628 train loss:1.111 acc:70.83 | val loss:1.254 acc:72.00\r\n",
      "Epoch:0629 train loss:1.308 acc:61.67 | val loss:1.258 acc:71.80\r\n",
      "Epoch:0630 train loss:1.163 acc:71.67 | val loss:1.259 acc:71.00\r\n",
      "Epoch:0631 train loss:1.142 acc:64.17 | val loss:1.259 acc:70.80\r\n",
      "Epoch:0632 train loss:1.240 acc:65.83 | val loss:1.259 acc:70.80\r\n",
      "Epoch:0633 train loss:1.157 acc:67.50 | val loss:1.259 acc:70.80\r\n",
      "Epoch:0634 train loss:1.419 acc:70.00 | val loss:1.263 acc:71.00\r\n",
      "Epoch:0635 train loss:1.169 acc:69.17 | val loss:1.267 acc:72.00\r\n",
      "Epoch:0636 train loss:1.526 acc:61.67 | val loss:1.274 acc:71.20\r\n",
      "Epoch:0637 train loss:1.274 acc:63.33 | val loss:1.281 acc:70.80\r\n",
      "Epoch:0638 train loss:1.125 acc:65.83 | val loss:1.285 acc:70.60\r\n",
      "Epoch:0639 train loss:1.253 acc:66.67 | val loss:1.285 acc:70.00\r\n",
      "Epoch:0640 train loss:1.156 acc:67.50 | val loss:1.285 acc:69.60\r\n",
      "Epoch:0641 train loss:1.244 acc:65.00 | val loss:1.280 acc:70.20\r\n",
      "Epoch:0642 train loss:1.202 acc:65.83 | val loss:1.273 acc:70.80\r\n",
      "Epoch:0643 train loss:1.359 acc:70.00 | val loss:1.267 acc:71.60\r\n",
      "Epoch:0644 train loss:1.267 acc:56.67 | val loss:1.265 acc:70.60\r\n",
      "Epoch:0645 train loss:1.248 acc:65.00 | val loss:1.267 acc:70.80\r\n",
      "Epoch:0646 train loss:1.615 acc:63.33 | val loss:1.274 acc:69.80\r\n",
      "Epoch:0647 train loss:1.227 acc:59.17 | val loss:1.278 acc:69.00\r\n",
      "Epoch:0648 train loss:1.131 acc:65.00 | val loss:1.280 acc:69.00\r\n",
      "Epoch:0649 train loss:1.376 acc:62.50 | val loss:1.283 acc:69.00\r\n",
      "Epoch:0650 train loss:1.219 acc:70.00 | val loss:1.283 acc:69.80\r\n",
      "Epoch:0651 train loss:1.279 acc:55.00 | val loss:1.282 acc:70.20\r\n",
      "Epoch:0652 train loss:1.168 acc:65.00 | val loss:1.281 acc:70.80\r\n",
      "Load 552th epoch\r\n",
      "Test acc.:73.7\r\n"
     ]
    }
   ],
   "source": [
    "best_num_layers = int(a[0])\n",
    "best_optim = int(a[1])\n",
    "best_al = int(a[2])\n",
    "best_act = int(a[3])\n",
    "best_earlystop = int(a[4])\n",
    "best_epoch1 = int(a[5])\n",
    "best_hidden_dim = int(a[6])\n",
    "best_variant = a[7]\n",
    "\n",
    "best_alpha = a[8]\n",
    "best_lamda = a[9]\n",
    "best_dropout = a[10]\n",
    "best_lr = a[11]\n",
    "best_weight_decay_1 = a[12]\n",
    "best_weight_decay_2 = a[13]\n",
    "start_time = time.time()\n",
    "best_test_acc_list=[]\n",
    "for k in range(repeat):\n",
    "    if best_al==0 and best_variant==0:\n",
    "        !python -u trainshow.py --data citeseer --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --test  \n",
    "    elif best_al==1 and best_variant==0:\n",
    "        !python -u trainshow.py --data citeseer --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --lamda {best_lamda} --alpha {best_alpha} --test  \n",
    "    elif best_al==0 and best_variant==1:\n",
    "        !python -u trainshow.py --data citeseer --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --test --variant  \n",
    "    else:\n",
    "        !python -u trainshow.py --data citeseer --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --lamda {best_lamda} --alpha {best_alpha} --test --variant  \n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a898284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:45:53.024252Z",
     "iopub.status.busy": "2024-04-24T05:45:53.023891Z",
     "iopub.status.idle": "2024-04-24T05:45:53.031681Z",
     "shell.execute_reply": "2024-04-24T05:45:53.030818Z"
    },
    "papermill": {
     "duration": 0.148832,
     "end_time": "2024-04-24T05:45:53.033972",
     "exception": false,
     "start_time": "2024-04-24T05:45:52.885140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citeseer\n",
      "best_num_layers: 32\n",
      "best_optim: 0\n",
      "best_al: 1\n",
      "best_act: 0\n",
      "best_earlystop: 100\n",
      "best_epoch1: 1500\n",
      "best_hidden_dim: 256\n",
      "best_variant: 0\n",
      "---------\n",
      "best_alpha: 0.1\n",
      "best_lamda: 0.6\n",
      "best_dropout: 0.7\n",
      "best_lr: 0.01\n",
      "best_weight_decay_1: 0.01\n",
      "best_weight_decay_2: 0.0005\n",
      "---------\n",
      ": 65.3266327381134\n"
     ]
    }
   ],
   "source": [
    "print(\"citeseer\")\n",
    "print(f\"best_num_layers: {best_num_layers}\")\n",
    "print(f\"best_optim: {best_optim}\")\n",
    "print(f\"best_al: {best_al}\")\n",
    "print(f\"best_act: {best_act}\")\n",
    "print(f\"best_earlystop: {best_earlystop}\")\n",
    "print(f\"best_epoch1: {best_epoch1}\")\n",
    "print(f\"best_hidden_dim: {best_hidden_dim}\")\n",
    "print(f\"best_variant: {best_variant}\")\n",
    "print(\"---------\")\n",
    "print(f\"best_alpha: {best_alpha}\")\n",
    "print(f\"best_lamda: {best_lamda}\")\n",
    "print(f\"best_dropout: {best_dropout}\")\n",
    "print(f\"best_lr: {best_lr}\")\n",
    "print(f\"best_weight_decay_1: {best_weight_decay_1}\")\n",
    "print(f\"best_weight_decay_2: {best_weight_decay_2}\")\n",
    "print(\"---------\")\n",
    "print(\":\", (end_time-start_time)/repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6993437",
   "metadata": {
    "papermill": {
     "duration": 0.134425,
     "end_time": "2024-04-24T05:45:53.303119",
     "exception": false,
     "start_time": "2024-04-24T05:45:53.168694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af506aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:45:53.575200Z",
     "iopub.status.busy": "2024-04-24T05:45:53.574851Z",
     "iopub.status.idle": "2024-04-24T05:45:53.579670Z",
     "shell.execute_reply": "2024-04-24T05:45:53.578752Z"
    },
    "papermill": {
     "duration": 0.143227,
     "end_time": "2024-04-24T05:45:53.581698",
     "exception": false,
     "start_time": "2024-04-24T05:45:53.438471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a=(16,0,1,0,100,1500,256,1,0.1,0.4,0.5,0.01,5e-4,0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5254f804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:45:53.854760Z",
     "iopub.status.busy": "2024-04-24T05:45:53.854361Z",
     "iopub.status.idle": "2024-04-24T05:47:11.936276Z",
     "shell.execute_reply": "2024-04-24T05:47:11.935002Z"
    },
    "papermill": {
     "duration": 78.221189,
     "end_time": "2024-04-24T05:47:11.938677",
     "exception": false,
     "start_time": "2024-04-24T05:45:53.717488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0000 train loss:1.100 acc:33.33 | val loss:1.096 acc:43.60\r\n",
      "Epoch:0001 train loss:1.096 acc:41.67 | val loss:1.098 acc:20.20\r\n",
      "Epoch:0002 train loss:1.089 acc:50.00 | val loss:1.099 acc:19.60\r\n",
      "Epoch:0003 train loss:1.083 acc:45.00 | val loss:1.100 acc:20.00\r\n",
      "Epoch:0004 train loss:1.078 acc:38.33 | val loss:1.098 acc:23.40\r\n",
      "Epoch:0005 train loss:1.063 acc:45.00 | val loss:1.095 acc:22.00\r\n",
      "Epoch:0006 train loss:1.064 acc:40.00 | val loss:1.090 acc:22.20\r\n",
      "Epoch:0007 train loss:1.044 acc:43.33 | val loss:1.079 acc:28.20\r\n",
      "Epoch:0008 train loss:1.024 acc:50.00 | val loss:1.057 acc:52.00\r\n",
      "Epoch:0009 train loss:0.983 acc:61.67 | val loss:1.027 acc:67.40\r\n",
      "Epoch:0010 train loss:0.955 acc:63.33 | val loss:0.990 acc:70.20\r\n",
      "Epoch:0011 train loss:0.901 acc:80.00 | val loss:0.949 acc:70.40\r\n",
      "Epoch:0012 train loss:0.826 acc:80.00 | val loss:0.892 acc:72.40\r\n",
      "Epoch:0013 train loss:0.748 acc:88.33 | val loss:0.845 acc:71.80\r\n",
      "Epoch:0014 train loss:0.661 acc:86.67 | val loss:0.793 acc:71.20\r\n",
      "Epoch:0015 train loss:0.622 acc:80.00 | val loss:0.897 acc:72.40\r\n",
      "Epoch:0016 train loss:0.567 acc:80.00 | val loss:0.797 acc:56.20\r\n",
      "Epoch:0017 train loss:0.686 acc:73.33 | val loss:0.777 acc:70.00\r\n",
      "Epoch:0018 train loss:0.519 acc:88.33 | val loss:0.919 acc:72.40\r\n",
      "Epoch:0019 train loss:0.583 acc:81.67 | val loss:0.723 acc:71.60\r\n",
      "Epoch:0020 train loss:0.512 acc:90.00 | val loss:0.732 acc:65.60\r\n",
      "Epoch:0021 train loss:0.541 acc:83.33 | val loss:0.698 acc:73.80\r\n",
      "Epoch:0022 train loss:0.519 acc:86.67 | val loss:0.681 acc:78.00\r\n",
      "Epoch:0023 train loss:0.420 acc:86.67 | val loss:0.761 acc:75.40\r\n",
      "Epoch:0024 train loss:0.443 acc:88.33 | val loss:0.765 acc:74.60\r\n",
      "Epoch:0025 train loss:0.424 acc:86.67 | val loss:0.658 acc:77.80\r\n",
      "Epoch:0026 train loss:0.354 acc:91.67 | val loss:0.609 acc:80.00\r\n",
      "Epoch:0027 train loss:0.381 acc:88.33 | val loss:0.603 acc:78.60\r\n",
      "Epoch:0028 train loss:0.362 acc:91.67 | val loss:0.604 acc:77.00\r\n",
      "Epoch:0029 train loss:0.345 acc:93.33 | val loss:0.563 acc:81.40\r\n",
      "Epoch:0030 train loss:0.331 acc:93.33 | val loss:0.593 acc:79.60\r\n",
      "Epoch:0031 train loss:0.260 acc:96.67 | val loss:0.623 acc:79.20\r\n",
      "Epoch:0032 train loss:0.291 acc:90.00 | val loss:0.564 acc:80.40\r\n",
      "Epoch:0033 train loss:0.216 acc:95.00 | val loss:0.547 acc:80.80\r\n",
      "Epoch:0034 train loss:0.247 acc:95.00 | val loss:0.524 acc:80.20\r\n",
      "Epoch:0035 train loss:0.251 acc:93.33 | val loss:0.537 acc:80.60\r\n",
      "Epoch:0036 train loss:0.216 acc:95.00 | val loss:0.553 acc:80.80\r\n",
      "Epoch:0037 train loss:0.162 acc:98.33 | val loss:0.543 acc:82.40\r\n",
      "Epoch:0038 train loss:0.211 acc:95.00 | val loss:0.521 acc:81.40\r\n",
      "Epoch:0039 train loss:0.141 acc:96.67 | val loss:0.667 acc:77.60\r\n",
      "Epoch:0040 train loss:0.189 acc:95.00 | val loss:0.530 acc:80.00\r\n",
      "Epoch:0041 train loss:0.209 acc:95.00 | val loss:0.541 acc:81.60\r\n",
      "Epoch:0042 train loss:0.187 acc:96.67 | val loss:0.615 acc:80.00\r\n",
      "Epoch:0043 train loss:0.165 acc:96.67 | val loss:0.771 acc:69.20\r\n",
      "Epoch:0044 train loss:0.364 acc:83.33 | val loss:0.568 acc:80.20\r\n",
      "Epoch:0045 train loss:0.192 acc:95.00 | val loss:0.750 acc:73.80\r\n",
      "Epoch:0046 train loss:0.319 acc:86.67 | val loss:0.516 acc:80.00\r\n",
      "Epoch:0047 train loss:0.171 acc:98.33 | val loss:0.559 acc:79.60\r\n",
      "Epoch:0048 train loss:0.195 acc:96.67 | val loss:0.604 acc:78.60\r\n",
      "Epoch:0049 train loss:0.200 acc:96.67 | val loss:0.612 acc:77.40\r\n",
      "Epoch:0050 train loss:0.203 acc:96.67 | val loss:0.562 acc:80.80\r\n",
      "Epoch:0051 train loss:0.184 acc:98.33 | val loss:0.550 acc:80.00\r\n",
      "Epoch:0052 train loss:0.189 acc:96.67 | val loss:0.556 acc:80.00\r\n",
      "Epoch:0053 train loss:0.190 acc:95.00 | val loss:0.549 acc:80.20\r\n",
      "Epoch:0054 train loss:0.168 acc:98.33 | val loss:0.539 acc:80.80\r\n",
      "Epoch:0055 train loss:0.159 acc:98.33 | val loss:0.574 acc:80.20\r\n",
      "Epoch:0056 train loss:0.146 acc:98.33 | val loss:0.620 acc:78.20\r\n",
      "Epoch:0057 train loss:0.136 acc:98.33 | val loss:0.581 acc:79.80\r\n",
      "Epoch:0058 train loss:0.131 acc:98.33 | val loss:0.556 acc:80.80\r\n",
      "Epoch:0059 train loss:0.119 acc:98.33 | val loss:0.591 acc:80.20\r\n",
      "Epoch:0060 train loss:0.154 acc:93.33 | val loss:0.578 acc:80.20\r\n",
      "Epoch:0061 train loss:0.118 acc:96.67 | val loss:0.553 acc:79.80\r\n",
      "Epoch:0062 train loss:0.109 acc:98.33 | val loss:0.588 acc:77.60\r\n",
      "Epoch:0063 train loss:0.146 acc:96.67 | val loss:0.533 acc:79.40\r\n",
      "Epoch:0064 train loss:0.105 acc:100.00 | val loss:0.560 acc:80.60\r\n",
      "Epoch:0065 train loss:0.144 acc:96.67 | val loss:0.614 acc:80.00\r\n",
      "Epoch:0066 train loss:0.090 acc:100.00 | val loss:0.564 acc:80.20\r\n",
      "Epoch:0067 train loss:0.080 acc:98.33 | val loss:0.560 acc:78.80\r\n",
      "Epoch:0068 train loss:0.104 acc:98.33 | val loss:0.549 acc:80.80\r\n",
      "Epoch:0069 train loss:0.112 acc:96.67 | val loss:0.521 acc:80.40\r\n",
      "Epoch:0070 train loss:0.134 acc:96.67 | val loss:0.806 acc:75.60\r\n",
      "Epoch:0071 train loss:0.212 acc:90.00 | val loss:0.575 acc:79.20\r\n",
      "Epoch:0072 train loss:0.204 acc:93.33 | val loss:0.692 acc:73.20\r\n",
      "Epoch:0073 train loss:0.221 acc:88.33 | val loss:0.822 acc:74.60\r\n",
      "Epoch:0074 train loss:0.227 acc:93.33 | val loss:0.791 acc:75.00\r\n",
      "Epoch:0075 train loss:0.252 acc:93.33 | val loss:0.532 acc:80.00\r\n",
      "Epoch:0076 train loss:0.130 acc:96.67 | val loss:0.575 acc:77.80\r\n",
      "Epoch:0077 train loss:0.158 acc:98.33 | val loss:0.621 acc:75.00\r\n",
      "Epoch:0078 train loss:0.182 acc:95.00 | val loss:0.597 acc:77.60\r\n",
      "Epoch:0079 train loss:0.164 acc:98.33 | val loss:0.649 acc:75.60\r\n",
      "Epoch:0080 train loss:0.165 acc:98.33 | val loss:0.612 acc:78.20\r\n",
      "Epoch:0081 train loss:0.198 acc:95.00 | val loss:0.579 acc:78.20\r\n",
      "Epoch:0082 train loss:0.173 acc:95.00 | val loss:0.566 acc:78.80\r\n",
      "Epoch:0083 train loss:0.182 acc:95.00 | val loss:0.545 acc:77.80\r\n",
      "Epoch:0084 train loss:0.165 acc:98.33 | val loss:0.542 acc:79.60\r\n",
      "Epoch:0085 train loss:0.137 acc:98.33 | val loss:0.577 acc:79.40\r\n",
      "Epoch:0086 train loss:0.176 acc:96.67 | val loss:0.642 acc:77.20\r\n",
      "Epoch:0087 train loss:0.173 acc:96.67 | val loss:0.635 acc:77.20\r\n",
      "Epoch:0088 train loss:0.106 acc:100.00 | val loss:0.583 acc:79.80\r\n",
      "Epoch:0089 train loss:0.093 acc:100.00 | val loss:0.581 acc:80.00\r\n",
      "Epoch:0090 train loss:0.107 acc:98.33 | val loss:0.611 acc:78.60\r\n",
      "Epoch:0091 train loss:0.098 acc:100.00 | val loss:0.608 acc:78.40\r\n",
      "Epoch:0092 train loss:0.118 acc:96.67 | val loss:0.577 acc:80.40\r\n",
      "Epoch:0093 train loss:0.102 acc:98.33 | val loss:0.550 acc:80.20\r\n",
      "Epoch:0094 train loss:0.088 acc:98.33 | val loss:0.561 acc:79.40\r\n",
      "Epoch:0095 train loss:0.092 acc:100.00 | val loss:0.566 acc:78.80\r\n",
      "Epoch:0096 train loss:0.085 acc:100.00 | val loss:0.550 acc:79.40\r\n",
      "Epoch:0097 train loss:0.115 acc:96.67 | val loss:0.533 acc:80.80\r\n",
      "Epoch:0098 train loss:0.091 acc:100.00 | val loss:0.556 acc:81.40\r\n",
      "Epoch:0099 train loss:0.060 acc:100.00 | val loss:0.577 acc:80.60\r\n",
      "Epoch:0100 train loss:0.091 acc:96.67 | val loss:0.559 acc:81.40\r\n",
      "Epoch:0101 train loss:0.112 acc:96.67 | val loss:0.617 acc:79.00\r\n",
      "Epoch:0102 train loss:0.109 acc:98.33 | val loss:0.547 acc:79.20\r\n",
      "Epoch:0103 train loss:0.116 acc:96.67 | val loss:0.526 acc:80.20\r\n",
      "Epoch:0104 train loss:0.127 acc:96.67 | val loss:0.561 acc:81.80\r\n",
      "Epoch:0105 train loss:0.066 acc:100.00 | val loss:0.671 acc:78.20\r\n",
      "Epoch:0106 train loss:0.147 acc:95.00 | val loss:0.605 acc:80.00\r\n",
      "Epoch:0107 train loss:0.055 acc:100.00 | val loss:0.546 acc:79.60\r\n",
      "Epoch:0108 train loss:0.090 acc:98.33 | val loss:0.524 acc:79.80\r\n",
      "Epoch:0109 train loss:0.106 acc:98.33 | val loss:0.522 acc:79.20\r\n",
      "Epoch:0110 train loss:0.117 acc:96.67 | val loss:0.576 acc:80.60\r\n",
      "Epoch:0111 train loss:0.102 acc:98.33 | val loss:0.583 acc:80.80\r\n",
      "Epoch:0112 train loss:0.078 acc:100.00 | val loss:0.542 acc:80.20\r\n",
      "Epoch:0113 train loss:0.092 acc:100.00 | val loss:0.525 acc:80.00\r\n",
      "Epoch:0114 train loss:0.080 acc:100.00 | val loss:0.515 acc:80.20\r\n",
      "Epoch:0115 train loss:0.108 acc:98.33 | val loss:0.622 acc:79.20\r\n",
      "Epoch:0116 train loss:0.082 acc:100.00 | val loss:0.635 acc:78.00\r\n",
      "Epoch:0117 train loss:0.118 acc:98.33 | val loss:0.522 acc:80.40\r\n",
      "Epoch:0118 train loss:0.070 acc:100.00 | val loss:0.542 acc:78.60\r\n",
      "Epoch:0119 train loss:0.091 acc:98.33 | val loss:0.579 acc:80.20\r\n",
      "Epoch:0120 train loss:0.115 acc:96.67 | val loss:0.578 acc:81.00\r\n",
      "Epoch:0121 train loss:0.084 acc:98.33 | val loss:0.581 acc:81.40\r\n",
      "Epoch:0122 train loss:0.098 acc:96.67 | val loss:0.598 acc:80.00\r\n",
      "Epoch:0123 train loss:0.081 acc:98.33 | val loss:0.625 acc:79.00\r\n",
      "Epoch:0124 train loss:0.140 acc:95.00 | val loss:0.606 acc:77.40\r\n",
      "Epoch:0125 train loss:0.166 acc:95.00 | val loss:0.573 acc:80.40\r\n",
      "Epoch:0126 train loss:0.120 acc:98.33 | val loss:0.731 acc:76.40\r\n",
      "Epoch:0127 train loss:0.166 acc:93.33 | val loss:0.543 acc:79.00\r\n",
      "Epoch:0128 train loss:0.098 acc:100.00 | val loss:0.572 acc:79.00\r\n",
      "Epoch:0129 train loss:0.145 acc:96.67 | val loss:0.524 acc:79.60\r\n",
      "Epoch:0130 train loss:0.129 acc:96.67 | val loss:0.576 acc:78.80\r\n",
      "Epoch:0131 train loss:0.092 acc:100.00 | val loss:0.633 acc:78.20\r\n",
      "Epoch:0132 train loss:0.121 acc:98.33 | val loss:0.599 acc:78.40\r\n",
      "Epoch:0133 train loss:0.093 acc:100.00 | val loss:0.551 acc:79.00\r\n",
      "Epoch:0134 train loss:0.097 acc:100.00 | val loss:0.557 acc:79.60\r\n",
      "Epoch:0135 train loss:0.099 acc:98.33 | val loss:0.532 acc:79.80\r\n",
      "Epoch:0136 train loss:0.109 acc:98.33 | val loss:0.536 acc:80.20\r\n",
      "Epoch:0137 train loss:0.071 acc:100.00 | val loss:0.588 acc:79.00\r\n",
      "Epoch:0138 train loss:0.093 acc:98.33 | val loss:0.614 acc:79.40\r\n",
      "Epoch:0139 train loss:0.063 acc:100.00 | val loss:0.634 acc:78.80\r\n",
      "Epoch:0140 train loss:0.067 acc:98.33 | val loss:0.567 acc:79.60\r\n",
      "Epoch:0141 train loss:0.065 acc:100.00 | val loss:0.542 acc:79.80\r\n",
      "Epoch:0142 train loss:0.110 acc:98.33 | val loss:0.551 acc:79.80\r\n",
      "Epoch:0143 train loss:0.074 acc:98.33 | val loss:0.609 acc:79.00\r\n",
      "Epoch:0144 train loss:0.076 acc:100.00 | val loss:0.611 acc:79.20\r\n",
      "Epoch:0145 train loss:0.124 acc:95.00 | val loss:0.540 acc:80.20\r\n",
      "Epoch:0146 train loss:0.095 acc:98.33 | val loss:0.556 acc:80.60\r\n",
      "Epoch:0147 train loss:0.117 acc:98.33 | val loss:0.649 acc:77.60\r\n",
      "Epoch:0148 train loss:0.078 acc:100.00 | val loss:0.754 acc:75.60\r\n",
      "Epoch:0149 train loss:0.153 acc:95.00 | val loss:0.548 acc:78.20\r\n",
      "Epoch:0150 train loss:0.100 acc:98.33 | val loss:0.634 acc:74.40\r\n",
      "Epoch:0151 train loss:0.183 acc:93.33 | val loss:0.559 acc:80.20\r\n",
      "Epoch:0152 train loss:0.073 acc:100.00 | val loss:0.764 acc:75.40\r\n",
      "Epoch:0153 train loss:0.100 acc:98.33 | val loss:0.716 acc:77.80\r\n",
      "Epoch:0154 train loss:0.099 acc:98.33 | val loss:0.586 acc:79.20\r\n",
      "Epoch:0155 train loss:0.076 acc:100.00 | val loss:0.535 acc:80.20\r\n",
      "Epoch:0156 train loss:0.081 acc:100.00 | val loss:0.535 acc:78.80\r\n",
      "Epoch:0157 train loss:0.103 acc:98.33 | val loss:0.557 acc:78.80\r\n",
      "Epoch:0158 train loss:0.106 acc:98.33 | val loss:0.612 acc:78.80\r\n",
      "Epoch:0159 train loss:0.092 acc:98.33 | val loss:0.642 acc:78.40\r\n",
      "Epoch:0160 train loss:0.085 acc:98.33 | val loss:0.659 acc:77.40\r\n",
      "Epoch:0161 train loss:0.087 acc:98.33 | val loss:0.595 acc:78.80\r\n",
      "Epoch:0162 train loss:0.076 acc:100.00 | val loss:0.563 acc:78.80\r\n",
      "Epoch:0163 train loss:0.081 acc:98.33 | val loss:0.546 acc:78.80\r\n",
      "Epoch:0164 train loss:0.081 acc:100.00 | val loss:0.551 acc:79.20\r\n",
      "Epoch:0165 train loss:0.073 acc:100.00 | val loss:0.562 acc:79.20\r\n",
      "Epoch:0166 train loss:0.090 acc:100.00 | val loss:0.600 acc:79.00\r\n",
      "Epoch:0167 train loss:0.059 acc:98.33 | val loss:0.601 acc:78.80\r\n",
      "Epoch:0168 train loss:0.073 acc:96.67 | val loss:0.558 acc:79.60\r\n",
      "Epoch:0169 train loss:0.092 acc:98.33 | val loss:0.673 acc:77.80\r\n",
      "Epoch:0170 train loss:0.135 acc:95.00 | val loss:0.734 acc:77.80\r\n",
      "Epoch:0171 train loss:0.196 acc:91.67 | val loss:0.569 acc:80.40\r\n",
      "Epoch:0172 train loss:0.156 acc:96.67 | val loss:0.536 acc:80.20\r\n",
      "Epoch:0173 train loss:0.106 acc:98.33 | val loss:0.644 acc:79.00\r\n",
      "Epoch:0174 train loss:0.122 acc:98.33 | val loss:0.748 acc:76.00\r\n",
      "Epoch:0175 train loss:0.141 acc:98.33 | val loss:0.559 acc:79.80\r\n",
      "Epoch:0176 train loss:0.085 acc:100.00 | val loss:0.546 acc:81.00\r\n",
      "Epoch:0177 train loss:0.115 acc:100.00 | val loss:0.531 acc:80.20\r\n",
      "Epoch:0178 train loss:0.099 acc:98.33 | val loss:0.538 acc:80.40\r\n",
      "Epoch:0179 train loss:0.089 acc:100.00 | val loss:0.614 acc:78.20\r\n",
      "Epoch:0180 train loss:0.099 acc:98.33 | val loss:0.609 acc:78.20\r\n",
      "Epoch:0181 train loss:0.108 acc:98.33 | val loss:0.536 acc:79.60\r\n",
      "Epoch:0182 train loss:0.076 acc:98.33 | val loss:0.555 acc:80.20\r\n",
      "Epoch:0183 train loss:0.122 acc:96.67 | val loss:0.546 acc:79.60\r\n",
      "Epoch:0184 train loss:0.059 acc:100.00 | val loss:0.562 acc:80.20\r\n",
      "Epoch:0185 train loss:0.061 acc:98.33 | val loss:0.639 acc:79.40\r\n",
      "Epoch:0186 train loss:0.063 acc:100.00 | val loss:0.650 acc:79.80\r\n",
      "Epoch:0187 train loss:0.063 acc:98.33 | val loss:0.627 acc:79.40\r\n",
      "Epoch:0188 train loss:0.117 acc:96.67 | val loss:0.546 acc:80.80\r\n",
      "Epoch:0189 train loss:0.064 acc:100.00 | val loss:0.535 acc:79.40\r\n",
      "Epoch:0190 train loss:0.087 acc:100.00 | val loss:0.536 acc:80.40\r\n",
      "Epoch:0191 train loss:0.070 acc:98.33 | val loss:0.674 acc:77.00\r\n",
      "Epoch:0192 train loss:0.096 acc:98.33 | val loss:0.626 acc:78.80\r\n",
      "Epoch:0193 train loss:0.096 acc:96.67 | val loss:0.554 acc:78.80\r\n",
      "Epoch:0194 train loss:0.109 acc:96.67 | val loss:0.528 acc:79.80\r\n",
      "Epoch:0195 train loss:0.091 acc:96.67 | val loss:0.572 acc:79.40\r\n",
      "Epoch:0196 train loss:0.089 acc:98.33 | val loss:0.609 acc:78.40\r\n",
      "Epoch:0197 train loss:0.105 acc:98.33 | val loss:0.623 acc:78.80\r\n",
      "Epoch:0198 train loss:0.069 acc:100.00 | val loss:0.587 acc:78.80\r\n",
      "Epoch:0199 train loss:0.101 acc:98.33 | val loss:0.546 acc:81.00\r\n",
      "Epoch:0200 train loss:0.104 acc:98.33 | val loss:0.601 acc:78.60\r\n",
      "Epoch:0201 train loss:0.079 acc:100.00 | val loss:0.621 acc:78.60\r\n",
      "Epoch:0202 train loss:0.111 acc:98.33 | val loss:0.576 acc:80.00\r\n",
      "Epoch:0203 train loss:0.081 acc:100.00 | val loss:0.545 acc:79.40\r\n",
      "Epoch:0204 train loss:0.125 acc:96.67 | val loss:0.521 acc:78.80\r\n",
      "Epoch:0205 train loss:0.092 acc:98.33 | val loss:0.577 acc:79.00\r\n",
      "Epoch:0206 train loss:0.087 acc:98.33 | val loss:0.629 acc:77.20\r\n",
      "Epoch:0207 train loss:0.083 acc:100.00 | val loss:0.612 acc:78.80\r\n",
      "Epoch:0208 train loss:0.107 acc:98.33 | val loss:0.579 acc:78.80\r\n",
      "Epoch:0209 train loss:0.113 acc:98.33 | val loss:0.522 acc:80.00\r\n",
      "Epoch:0210 train loss:0.085 acc:98.33 | val loss:0.510 acc:79.80\r\n",
      "Epoch:0211 train loss:0.094 acc:98.33 | val loss:0.558 acc:80.60\r\n",
      "Epoch:0212 train loss:0.052 acc:100.00 | val loss:0.753 acc:77.40\r\n",
      "Epoch:0213 train loss:0.128 acc:93.33 | val loss:0.532 acc:80.20\r\n",
      "Epoch:0214 train loss:0.057 acc:100.00 | val loss:0.610 acc:76.60\r\n",
      "Epoch:0215 train loss:0.165 acc:91.67 | val loss:0.583 acc:79.20\r\n",
      "Epoch:0216 train loss:0.066 acc:100.00 | val loss:0.730 acc:76.40\r\n",
      "Epoch:0217 train loss:0.145 acc:93.33 | val loss:0.532 acc:80.40\r\n",
      "Epoch:0218 train loss:0.076 acc:98.33 | val loss:0.575 acc:78.40\r\n",
      "Epoch:0219 train loss:0.135 acc:98.33 | val loss:0.543 acc:79.20\r\n",
      "Epoch:0220 train loss:0.097 acc:98.33 | val loss:0.586 acc:79.00\r\n",
      "Epoch:0221 train loss:0.070 acc:100.00 | val loss:0.675 acc:77.00\r\n",
      "Epoch:0222 train loss:0.133 acc:96.67 | val loss:0.569 acc:80.60\r\n",
      "Epoch:0223 train loss:0.068 acc:98.33 | val loss:0.526 acc:79.60\r\n",
      "Epoch:0224 train loss:0.069 acc:100.00 | val loss:0.538 acc:80.40\r\n",
      "Epoch:0225 train loss:0.085 acc:96.67 | val loss:0.535 acc:79.80\r\n",
      "Epoch:0226 train loss:0.073 acc:100.00 | val loss:0.560 acc:79.60\r\n",
      "Epoch:0227 train loss:0.079 acc:100.00 | val loss:0.588 acc:79.80\r\n",
      "Epoch:0228 train loss:0.060 acc:100.00 | val loss:0.603 acc:79.80\r\n",
      "Epoch:0229 train loss:0.071 acc:100.00 | val loss:0.570 acc:80.60\r\n",
      "Epoch:0230 train loss:0.056 acc:100.00 | val loss:0.536 acc:80.40\r\n",
      "Epoch:0231 train loss:0.066 acc:98.33 | val loss:0.547 acc:79.80\r\n",
      "Epoch:0232 train loss:0.093 acc:100.00 | val loss:0.556 acc:80.40\r\n",
      "Epoch:0233 train loss:0.048 acc:98.33 | val loss:0.612 acc:79.80\r\n",
      "Epoch:0234 train loss:0.067 acc:100.00 | val loss:0.628 acc:79.40\r\n",
      "Epoch:0235 train loss:0.068 acc:100.00 | val loss:0.537 acc:81.00\r\n",
      "Epoch:0236 train loss:0.077 acc:98.33 | val loss:0.526 acc:79.40\r\n",
      "Epoch:0237 train loss:0.066 acc:100.00 | val loss:0.541 acc:80.40\r\n",
      "Epoch:0238 train loss:0.063 acc:100.00 | val loss:0.636 acc:78.00\r\n",
      "Epoch:0239 train loss:0.061 acc:100.00 | val loss:0.646 acc:78.60\r\n",
      "Epoch:0240 train loss:0.097 acc:96.67 | val loss:0.529 acc:80.20\r\n",
      "Epoch:0241 train loss:0.076 acc:100.00 | val loss:0.576 acc:78.20\r\n",
      "Epoch:0242 train loss:0.116 acc:100.00 | val loss:0.738 acc:77.20\r\n",
      "Epoch:0243 train loss:0.094 acc:98.33 | val loss:0.785 acc:75.80\r\n",
      "Epoch:0244 train loss:0.180 acc:90.00 | val loss:0.669 acc:76.00\r\n",
      "Epoch:0245 train loss:0.191 acc:93.33 | val loss:0.614 acc:76.60\r\n",
      "Epoch:0246 train loss:0.152 acc:96.67 | val loss:0.553 acc:80.20\r\n",
      "Epoch:0247 train loss:0.161 acc:98.33 | val loss:0.718 acc:75.80\r\n",
      "Epoch:0248 train loss:0.180 acc:93.33 | val loss:0.697 acc:77.20\r\n",
      "Epoch:0249 train loss:0.100 acc:98.33 | val loss:0.690 acc:77.20\r\n",
      "Epoch:0250 train loss:0.172 acc:95.00 | val loss:0.594 acc:79.20\r\n",
      "Epoch:0251 train loss:0.106 acc:98.33 | val loss:0.527 acc:80.40\r\n",
      "Epoch:0252 train loss:0.103 acc:98.33 | val loss:0.514 acc:79.60\r\n",
      "Epoch:0253 train loss:0.113 acc:98.33 | val loss:0.508 acc:80.60\r\n",
      "Epoch:0254 train loss:0.113 acc:98.33 | val loss:0.538 acc:79.20\r\n",
      "Epoch:0255 train loss:0.075 acc:100.00 | val loss:0.587 acc:78.20\r\n",
      "Epoch:0256 train loss:0.115 acc:98.33 | val loss:0.585 acc:78.60\r\n",
      "Epoch:0257 train loss:0.101 acc:98.33 | val loss:0.560 acc:79.00\r\n",
      "Epoch:0258 train loss:0.064 acc:100.00 | val loss:0.544 acc:79.00\r\n",
      "Epoch:0259 train loss:0.062 acc:100.00 | val loss:0.531 acc:79.20\r\n",
      "Epoch:0260 train loss:0.086 acc:98.33 | val loss:0.526 acc:79.40\r\n",
      "Epoch:0261 train loss:0.060 acc:100.00 | val loss:0.516 acc:79.80\r\n",
      "Epoch:0262 train loss:0.073 acc:98.33 | val loss:0.520 acc:80.60\r\n",
      "Epoch:0263 train loss:0.102 acc:98.33 | val loss:0.572 acc:80.00\r\n",
      "Epoch:0264 train loss:0.055 acc:98.33 | val loss:0.664 acc:78.60\r\n",
      "Epoch:0265 train loss:0.097 acc:98.33 | val loss:0.628 acc:79.40\r\n",
      "Epoch:0266 train loss:0.079 acc:100.00 | val loss:0.558 acc:79.60\r\n",
      "Epoch:0267 train loss:0.040 acc:100.00 | val loss:0.524 acc:79.80\r\n",
      "Epoch:0268 train loss:0.062 acc:98.33 | val loss:0.524 acc:79.80\r\n",
      "Epoch:0269 train loss:0.057 acc:98.33 | val loss:0.544 acc:79.00\r\n",
      "Epoch:0270 train loss:0.050 acc:100.00 | val loss:0.572 acc:78.20\r\n",
      "Epoch:0271 train loss:0.060 acc:100.00 | val loss:0.571 acc:79.60\r\n",
      "Epoch:0272 train loss:0.069 acc:98.33 | val loss:0.607 acc:79.40\r\n",
      "Epoch:0273 train loss:0.115 acc:93.33 | val loss:0.538 acc:80.00\r\n",
      "Epoch:0274 train loss:0.058 acc:100.00 | val loss:0.515 acc:79.80\r\n",
      "Epoch:0275 train loss:0.077 acc:100.00 | val loss:0.517 acc:80.60\r\n",
      "Epoch:0276 train loss:0.065 acc:100.00 | val loss:0.533 acc:80.20\r\n",
      "Epoch:0277 train loss:0.069 acc:100.00 | val loss:0.545 acc:79.60\r\n",
      "Epoch:0278 train loss:0.050 acc:100.00 | val loss:0.557 acc:79.60\r\n",
      "Epoch:0279 train loss:0.072 acc:100.00 | val loss:0.522 acc:80.80\r\n",
      "Epoch:0280 train loss:0.057 acc:100.00 | val loss:0.505 acc:80.00\r\n",
      "Epoch:0281 train loss:0.072 acc:98.33 | val loss:0.529 acc:80.80\r\n",
      "Epoch:0282 train loss:0.060 acc:100.00 | val loss:0.558 acc:80.40\r\n",
      "Epoch:0283 train loss:0.067 acc:98.33 | val loss:0.590 acc:80.40\r\n",
      "Epoch:0284 train loss:0.068 acc:100.00 | val loss:0.536 acc:80.60\r\n",
      "Epoch:0285 train loss:0.061 acc:98.33 | val loss:0.516 acc:80.80\r\n",
      "Epoch:0286 train loss:0.065 acc:100.00 | val loss:0.520 acc:80.60\r\n",
      "Epoch:0287 train loss:0.071 acc:98.33 | val loss:0.573 acc:80.00\r\n",
      "Epoch:0288 train loss:0.100 acc:96.67 | val loss:0.514 acc:80.40\r\n",
      "Epoch:0289 train loss:0.079 acc:100.00 | val loss:0.521 acc:79.40\r\n",
      "Epoch:0290 train loss:0.073 acc:100.00 | val loss:0.561 acc:79.60\r\n",
      "Epoch:0291 train loss:0.068 acc:100.00 | val loss:0.568 acc:79.20\r\n",
      "Epoch:0292 train loss:0.075 acc:98.33 | val loss:0.524 acc:79.20\r\n",
      "Epoch:0293 train loss:0.060 acc:100.00 | val loss:0.516 acc:79.80\r\n",
      "Epoch:0294 train loss:0.076 acc:96.67 | val loss:0.538 acc:80.40\r\n",
      "Epoch:0295 train loss:0.052 acc:100.00 | val loss:0.597 acc:79.00\r\n",
      "Epoch:0296 train loss:0.096 acc:96.67 | val loss:0.524 acc:79.60\r\n",
      "Epoch:0297 train loss:0.070 acc:98.33 | val loss:0.518 acc:79.60\r\n",
      "Epoch:0298 train loss:0.104 acc:98.33 | val loss:0.567 acc:80.00\r\n",
      "Epoch:0299 train loss:0.091 acc:98.33 | val loss:0.663 acc:78.00\r\n",
      "Epoch:0300 train loss:0.088 acc:98.33 | val loss:0.577 acc:79.20\r\n",
      "Epoch:0301 train loss:0.069 acc:100.00 | val loss:0.528 acc:80.20\r\n",
      "Epoch:0302 train loss:0.062 acc:100.00 | val loss:0.513 acc:79.80\r\n",
      "Epoch:0303 train loss:0.110 acc:98.33 | val loss:0.557 acc:80.20\r\n",
      "Epoch:0304 train loss:0.066 acc:100.00 | val loss:0.612 acc:78.00\r\n",
      "Epoch:0305 train loss:0.078 acc:98.33 | val loss:0.589 acc:78.80\r\n",
      "Epoch:0306 train loss:0.060 acc:100.00 | val loss:0.552 acc:79.80\r\n",
      "Epoch:0307 train loss:0.082 acc:96.67 | val loss:0.530 acc:79.80\r\n",
      "Epoch:0308 train loss:0.100 acc:95.00 | val loss:0.612 acc:77.60\r\n",
      "Epoch:0309 train loss:0.093 acc:100.00 | val loss:0.544 acc:80.00\r\n",
      "Epoch:0310 train loss:0.036 acc:100.00 | val loss:0.683 acc:77.80\r\n",
      "Epoch:0311 train loss:0.089 acc:98.33 | val loss:0.593 acc:78.00\r\n",
      "Epoch:0312 train loss:0.134 acc:95.00 | val loss:0.550 acc:78.40\r\n",
      "Epoch:0313 train loss:0.064 acc:100.00 | val loss:0.599 acc:79.00\r\n",
      "Epoch:0314 train loss:0.071 acc:100.00 | val loss:0.665 acc:77.80\r\n",
      "Epoch:0315 train loss:0.101 acc:96.67 | val loss:0.566 acc:80.40\r\n",
      "Epoch:0316 train loss:0.056 acc:100.00 | val loss:0.550 acc:79.40\r\n",
      "Epoch:0317 train loss:0.080 acc:100.00 | val loss:0.545 acc:79.00\r\n",
      "Epoch:0318 train loss:0.085 acc:98.33 | val loss:0.561 acc:79.80\r\n",
      "Epoch:0319 train loss:0.081 acc:98.33 | val loss:0.585 acc:79.40\r\n",
      "Epoch:0320 train loss:0.078 acc:98.33 | val loss:0.525 acc:80.60\r\n",
      "Epoch:0321 train loss:0.053 acc:100.00 | val loss:0.538 acc:80.20\r\n",
      "Epoch:0322 train loss:0.070 acc:100.00 | val loss:0.520 acc:80.40\r\n",
      "Epoch:0323 train loss:0.074 acc:98.33 | val loss:0.582 acc:80.60\r\n",
      "Epoch:0324 train loss:0.084 acc:98.33 | val loss:0.549 acc:81.00\r\n",
      "Epoch:0325 train loss:0.060 acc:100.00 | val loss:0.512 acc:80.20\r\n",
      "Epoch:0326 train loss:0.057 acc:100.00 | val loss:0.515 acc:80.40\r\n",
      "Epoch:0327 train loss:0.043 acc:100.00 | val loss:0.522 acc:80.40\r\n",
      "Epoch:0328 train loss:0.062 acc:100.00 | val loss:0.550 acc:79.80\r\n",
      "Epoch:0329 train loss:0.073 acc:98.33 | val loss:0.631 acc:79.20\r\n",
      "Epoch:0330 train loss:0.082 acc:98.33 | val loss:0.604 acc:79.20\r\n",
      "Epoch:0331 train loss:0.044 acc:100.00 | val loss:0.540 acc:81.40\r\n",
      "Epoch:0332 train loss:0.080 acc:98.33 | val loss:0.528 acc:81.60\r\n",
      "Epoch:0333 train loss:0.100 acc:95.00 | val loss:0.531 acc:81.20\r\n",
      "Epoch:0334 train loss:0.051 acc:98.33 | val loss:0.565 acc:80.40\r\n",
      "Epoch:0335 train loss:0.080 acc:98.33 | val loss:0.592 acc:79.00\r\n",
      "Epoch:0336 train loss:0.056 acc:100.00 | val loss:0.575 acc:80.20\r\n",
      "Epoch:0337 train loss:0.077 acc:100.00 | val loss:0.505 acc:80.00\r\n",
      "Epoch:0338 train loss:0.096 acc:96.67 | val loss:0.537 acc:80.80\r\n",
      "Epoch:0339 train loss:0.087 acc:98.33 | val loss:0.525 acc:80.60\r\n",
      "Epoch:0340 train loss:0.063 acc:100.00 | val loss:0.518 acc:79.80\r\n",
      "Epoch:0341 train loss:0.054 acc:100.00 | val loss:0.518 acc:79.80\r\n",
      "Epoch:0342 train loss:0.056 acc:100.00 | val loss:0.523 acc:79.00\r\n",
      "Epoch:0343 train loss:0.066 acc:98.33 | val loss:0.563 acc:79.40\r\n",
      "Epoch:0344 train loss:0.106 acc:98.33 | val loss:0.588 acc:78.20\r\n",
      "Epoch:0345 train loss:0.092 acc:98.33 | val loss:0.535 acc:80.40\r\n",
      "Epoch:0346 train loss:0.061 acc:100.00 | val loss:0.576 acc:79.80\r\n",
      "Epoch:0347 train loss:0.038 acc:100.00 | val loss:0.714 acc:77.60\r\n",
      "Epoch:0348 train loss:0.174 acc:93.33 | val loss:0.538 acc:80.60\r\n",
      "Epoch:0349 train loss:0.054 acc:100.00 | val loss:0.635 acc:76.80\r\n",
      "Epoch:0350 train loss:0.138 acc:95.00 | val loss:0.628 acc:80.00\r\n",
      "Epoch:0351 train loss:0.072 acc:100.00 | val loss:0.837 acc:74.20\r\n",
      "Epoch:0352 train loss:0.114 acc:98.33 | val loss:0.619 acc:79.20\r\n",
      "Epoch:0353 train loss:0.085 acc:96.67 | val loss:0.540 acc:79.40\r\n",
      "Epoch:0354 train loss:0.114 acc:96.67 | val loss:0.603 acc:77.40\r\n",
      "Epoch:0355 train loss:0.121 acc:98.33 | val loss:0.524 acc:79.20\r\n",
      "Epoch:0356 train loss:0.073 acc:100.00 | val loss:0.550 acc:80.00\r\n",
      "Epoch:0357 train loss:0.068 acc:100.00 | val loss:0.631 acc:77.80\r\n",
      "Epoch:0358 train loss:0.110 acc:98.33 | val loss:0.577 acc:79.80\r\n",
      "Epoch:0359 train loss:0.073 acc:98.33 | val loss:0.516 acc:80.40\r\n",
      "Epoch:0360 train loss:0.070 acc:98.33 | val loss:0.548 acc:79.80\r\n",
      "Epoch:0361 train loss:0.071 acc:100.00 | val loss:0.596 acc:77.20\r\n",
      "Epoch:0362 train loss:0.095 acc:98.33 | val loss:0.548 acc:78.80\r\n",
      "Epoch:0363 train loss:0.085 acc:100.00 | val loss:0.562 acc:80.00\r\n",
      "Epoch:0364 train loss:0.062 acc:100.00 | val loss:0.643 acc:79.60\r\n",
      "Epoch:0365 train loss:0.047 acc:100.00 | val loss:0.717 acc:76.80\r\n",
      "Epoch:0366 train loss:0.086 acc:98.33 | val loss:0.610 acc:79.40\r\n",
      "Epoch:0367 train loss:0.082 acc:98.33 | val loss:0.544 acc:80.00\r\n",
      "Epoch:0368 train loss:0.065 acc:98.33 | val loss:0.588 acc:77.60\r\n",
      "Epoch:0369 train loss:0.109 acc:98.33 | val loss:0.524 acc:79.40\r\n",
      "Epoch:0370 train loss:0.041 acc:100.00 | val loss:0.546 acc:79.40\r\n",
      "Epoch:0371 train loss:0.117 acc:93.33 | val loss:0.537 acc:80.60\r\n",
      "Epoch:0372 train loss:0.088 acc:96.67 | val loss:0.575 acc:78.40\r\n",
      "Epoch:0373 train loss:0.075 acc:98.33 | val loss:0.577 acc:78.20\r\n",
      "Epoch:0374 train loss:0.058 acc:98.33 | val loss:0.522 acc:79.60\r\n",
      "Epoch:0375 train loss:0.072 acc:98.33 | val loss:0.513 acc:80.60\r\n",
      "Epoch:0376 train loss:0.071 acc:98.33 | val loss:0.540 acc:80.40\r\n",
      "Epoch:0377 train loss:0.061 acc:100.00 | val loss:0.562 acc:80.20\r\n",
      "Epoch:0378 train loss:0.083 acc:98.33 | val loss:0.538 acc:81.40\r\n",
      "Epoch:0379 train loss:0.077 acc:100.00 | val loss:0.515 acc:80.40\r\n",
      "Epoch:0380 train loss:0.079 acc:100.00 | val loss:0.520 acc:80.60\r\n",
      "Load 280th epoch\r\n",
      "Test acc.:80.1\r\n"
     ]
    }
   ],
   "source": [
    "best_num_layers = int(a[0])\n",
    "best_optim = int(a[1])\n",
    "best_al = int(a[2])\n",
    "best_act = int(a[3])\n",
    "best_earlystop = int(a[4])\n",
    "best_epoch1 = int(a[5])\n",
    "best_hidden_dim = int(a[6])\n",
    "best_variant = a[7]\n",
    "\n",
    "best_alpha = a[8]\n",
    "best_lamda = a[9]\n",
    "best_dropout = a[10]\n",
    "best_lr = a[11]\n",
    "best_weight_decay_1 = a[12]\n",
    "best_weight_decay_2 = a[13]\n",
    "start_time = time.time()\n",
    "best_test_acc_list=[]\n",
    "for k in range(repeat):\n",
    "    if best_al==0 and best_variant==0:\n",
    "        !python -u trainshow.py --data pubmed --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --test  \n",
    "    elif best_al==1 and best_variant==0:\n",
    "        !python -u trainshow.py --data pubmed --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --lamda {best_lamda} --alpha {best_alpha} --test  \n",
    "    elif best_al==0 and best_variant==1:\n",
    "        !python -u trainshow.py --data pubmed --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --test --variant  \n",
    "    else:\n",
    "        !python -u trainshow.py --data pubmed --layer {best_num_layers} --hidden {best_hidden_dim} --dropout {best_dropout} --wd1 {best_weight_decay_1} --wd2 {best_weight_decay_2} --lr {best_lr} --op {best_optim}  --act_fn {best_act} --epochs  {best_epoch1} --patience {best_earlystop} --lamda {best_lamda} --alpha {best_alpha} --test --variant  \n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7b0c495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T05:47:12.271620Z",
     "iopub.status.busy": "2024-04-24T05:47:12.270911Z",
     "iopub.status.idle": "2024-04-24T05:47:12.279099Z",
     "shell.execute_reply": "2024-04-24T05:47:12.278193Z"
    },
    "papermill": {
     "duration": 0.176752,
     "end_time": "2024-04-24T05:47:12.281427",
     "exception": false,
     "start_time": "2024-04-24T05:47:12.104675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pubmed\n",
      "best_num_layers: 16\n",
      "best_optim: 0\n",
      "best_al: 1\n",
      "best_act: 0\n",
      "best_earlystop: 100\n",
      "best_epoch1: 1500\n",
      "best_hidden_dim: 256\n",
      "best_variant: 1\n",
      "---------\n",
      "best_alpha: 0.1\n",
      "best_lamda: 0.4\n",
      "best_dropout: 0.5\n",
      "best_lr: 0.01\n",
      "best_weight_decay_1: 0.0005\n",
      "best_weight_decay_2: 0.0005\n",
      "---------\n",
      ": 78.0488634109497\n"
     ]
    }
   ],
   "source": [
    "print(\"pubmed\")\n",
    "print(f\"best_num_layers: {best_num_layers}\")\n",
    "print(f\"best_optim: {best_optim}\")\n",
    "print(f\"best_al: {best_al}\")\n",
    "print(f\"best_act: {best_act}\")\n",
    "print(f\"best_earlystop: {best_earlystop}\")\n",
    "print(f\"best_epoch1: {best_epoch1}\")\n",
    "print(f\"best_hidden_dim: {best_hidden_dim}\")\n",
    "print(f\"best_variant: {best_variant}\")\n",
    "print(\"---------\")\n",
    "print(f\"best_alpha: {best_alpha}\")\n",
    "print(f\"best_lamda: {best_lamda}\")\n",
    "print(f\"best_dropout: {best_dropout}\")\n",
    "print(f\"best_lr: {best_lr}\")\n",
    "print(f\"best_weight_decay_1: {best_weight_decay_1}\")\n",
    "print(f\"best_weight_decay_2: {best_weight_decay_2}\")\n",
    "print(\"---------\")\n",
    "import numpy as np\n",
    "print(\":\", (end_time-start_time)/repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67efcb8",
   "metadata": {
    "papermill": {
     "duration": 0.172503,
     "end_time": "2024-04-24T05:47:12.620212",
     "exception": false,
     "start_time": "2024-04-24T05:47:12.447709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for chale"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4019076,
     "sourceId": 6992470,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3992311,
     "sourceId": 7047491,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4437867,
     "sourceId": 7619250,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 283.296444,
   "end_time": "2024-04-24T05:47:13.026254",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-24T05:42:29.729810",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

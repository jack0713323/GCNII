{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6958662,"sourceType":"datasetVersion","datasetId":1640734},{"sourceId":7047491,"sourceType":"datasetVersion","datasetId":3992311},{"sourceId":7050903,"sourceType":"datasetVersion","datasetId":3990844},{"sourceId":7184252,"sourceType":"datasetVersion","datasetId":4049779}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":23155.755442,"end_time":"2024-03-02T07:51:23.156174","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-02T01:25:27.400732","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.nn import GCNConv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nimport numpy as np\nimport torch.nn as nn\nimport random\nimport time\nimport matplotlib.pyplot as plt","metadata":{"papermill":{"duration":23.997643,"end_time":"2024-03-02T01:25:54.149240","exception":false,"start_time":"2024-03-02T01:25:30.151597","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"選擇要使用的資料庫","metadata":{"papermill":{"duration":0.01194,"end_time":"2024-03-02T01:25:54.173992","exception":false,"start_time":"2024-03-02T01:25:54.162052","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 定義數據集名稱\n#dataset = 'CiteSeer'\ndataset = 'Cora'\n#dataset = 'PubMed'\npath = \"./\"\n# 創建 Planetoid 數據集對象，同時進行特徵歸一化的轉換\n# Planetoid 是 PyTorch Geometric 提供的常用圖數據集之一\n# transform=T.NormalizeFeatures() 表示對節點特徵進行歸一化處理\ndataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\ndata = dataset[0]","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1697511065627,"user":{"displayName":"林子傑","userId":"16455381566400525515"},"user_tz":-480},"id":"ZhFD01O1bFOj","papermill":{"duration":3.583663,"end_time":"2024-03-02T01:25:57.769628","exception":false,"start_time":"2024-03-02T01:25:54.185965","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"設定參數、代數、初始解等等","metadata":{"papermill":{"duration":0.01382,"end_time":"2024-03-02T01:25:57.798009","exception":false,"start_time":"2024-03-02T01:25:57.784189","status":"completed"},"tags":[]}},{"cell_type":"code","source":"Ngen=3\nNsol=3\nnum_experiments=1\n","metadata":{"papermill":{"duration":0.02001,"end_time":"2024-03-02T01:25:57.830883","exception":false,"start_time":"2024-03-02T01:25:57.810873","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_number_range=[(1,5), (0,3), (0,3), (150,300), (50, 200), (300, 1000), (8, 256), (8, 256), (8, 256)\n                     , (8, 256), (8, 256),(0.2,0.8),(0.001, 0.1),(0.001, 0.1),(1e-5, 1e-2),(1e-5, 1e-2)\n                     ,(1e-5, 1e-2),(1e-5, 1e-2),(1e-5, 1e-2),(1e-5, 1e-2),(1e-5, 1e-2),(1e-5, 1e-2)\n                    ,(1e-5, 1e-2),(1e-5, 1e-2),(1e-5, 1e-2),(1e-5, 1e-2)]\nstart_sol=[4,2,0,173,176, 1589,106,106,106,106,106,0.7414438607175802, 0.038590004301053006\n   , 0.02999691290341772, 0.008409996562194955, 0.008409996562194955\n   , 0.008409996562194955, 0.008409996562194955, 0.008409996562194955, \n   0.00408868119073893,0.00408868119073893, 0.00408868119073893, \n   0.00408868119073893,0.00408868119073893, 0.00955616228967521\n   , 0.00048096144638867577]","metadata":{"papermill":{"duration":0.023389,"end_time":"2024-03-02T01:25:57.867270","exception":false,"start_time":"2024-03-02T01:25:57.843881","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 打印節點特徵矩陣的形狀\nprint(data.x.shape)\nprint(data.y.shape)\n# 打印邊的索引矩陣的形狀\nprint(data.edge_index.shape)\nprint(set(data.y.numpy()))","metadata":{"executionInfo":{"elapsed":259,"status":"ok","timestamp":1697511067952,"user":{"displayName":"林子傑","userId":"16455381566400525515"},"user_tz":-480},"id":"cMEPAhTibFOk","outputId":"28c5f2ea-d9f0-49c7-e38c-798db8b39e5f","papermill":{"duration":0.021457,"end_time":"2024-03-02T01:25:57.902009","exception":false,"start_time":"2024-03-02T01:25:57.880552","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#創建了一個空的 NetworkX 圖\nnxgraph = nx.Graph()\n#將資訊添加到 NetworkX 圖中\nfor x in range(len(data.x)):\n    nxgraph.add_node(x)\nfor u, v in zip(data.edge_index[0].numpy(), data.edge_index[1].numpy()):\n    nxgraph.add_edge(u, v)","metadata":{"executionInfo":{"elapsed":242,"status":"ok","timestamp":1697511070275,"user":{"displayName":"林子傑","userId":"16455381566400525515"},"user_tz":-480},"id":"5OoCN5hAbFOk","papermill":{"duration":0.055306,"end_time":"2024-03-02T01:25:57.970343","exception":false,"start_time":"2024-03-02T01:25:57.915037","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"設可調之網路 optimer都要改","metadata":{"id":"D77jGzQdCRfs","papermill":{"duration":0.012821,"end_time":"2024-03-02T01:25:58.199913","exception":false,"start_time":"2024-03-02T01:25:58.187092","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, num_layers=2, hidden_dims=None, dropout=0.5, activation=1):\n        super(Net, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_dims = hidden_dims if hidden_dims is not None else [16] * num_layers  # 默认隐藏维度为16，如果未指定，则都为16\n        self.conv_layers = nn.ModuleList()\n\n        for i in range(num_layers):\n            input_dim = dataset.num_features if i == 0 else self.hidden_dims[i-1]  # 使用不同的隐藏维度\n            self.conv_layers.append(GCNConv(input_dim, self.hidden_dims[i], cached=True))\n        self.fc1 = nn.Linear(self.hidden_dims[-1], dataset.num_classes)\n        self.dropout = dropout\n        self.activation = activation\n\n    def forward(self):\n        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n\n        for i in range(self.num_layers):\n            conv_layer = self.conv_layers[i]\n            x = conv_layer(x, edge_index, edge_weight)\n            \n            if self.activation == 0:\n                x = F.tanh(x)\n            elif self.activation == 1:\n                x = F.relu(x)\n            elif self.activation == 2:\n                x = F.leaky_relu(x)\n            else:\n                x = F.tanh(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        \n        x = self.fc1(x)\n        return F.log_softmax(x, dim=1)\n","metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1697511072117,"user":{"displayName":"林子傑","userId":"16455381566400525515"},"user_tz":-480},"id":"ntaBn2rMHBNO","papermill":{"duration":0.027147,"end_time":"2024-03-02T01:25:58.240302","exception":false,"start_time":"2024-03-02T01:25:58.213155","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#設備選擇\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndef train1():\n    model.train()\n    optimizer.zero_grad()\n    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n    optimizer.step()\n@torch.no_grad()\ndef test():\n    model.eval()\n    logits, accs = model(), []\n    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n        pred = logits[mask].max(1)[1]\n        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n        accs.append(acc)\n    return accs\n# 定義訓練函數，包括損失計算和反向傳播\ndef train2():\n    model.train()\n    optimizer.zero_grad()\n    model_out = model()[data.train_mask]\n    y_true = data.y[data.train_mask]\n    nllloss = F.nll_loss(model_out, y_true)\n    disloss_neg = F.logsigmoid(-1 * (model_out[y_neg_pairs.T[0]]*model_out[y_neg_pairs.T[1]])).sum(-1).mean()\n    disloss_pos = F.logsigmoid((model_out[y_pos_pairs.T[0]]*model_out[y_pos_pairs.T[1]])).sum(-1).mean()\n    loss = 10 * nllloss - disloss_neg - disloss_pos\n    loss.backward()\n    optimizer.step()\n# 定義測試函數，評估模型性能\ndef test():\n    model.eval()\n    logits, accs = model(), []\n    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n        pred = logits[mask].max(1)[1]\n        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n        accs.append(acc)\n    return accs\n# 定義要優化的目標函數。\ndef objective_function(params):\n    data = dataset[0]\n    lr, weight_decay ,dropout,hidden_dim,N= params  # 從參數中提取學習率和權重衰減。\n    model, data = Net(num_layers=N,dropout=dropout,hidden_dim=hidden_dim).to(device), data.to(device)\n    optimizer = torch.optim.Adam([\n    {'params': model.conv_layers.parameters(), 'weight_decay': weight_decay},\n    {'params': model.fc1.parameters(), 'weight_decay': weight_decay}\n    ], lr=lr)\n      #訓練迴圈\n    best_val_acc = test_acc = 0\n    train_losses, val_losses = [], []  # 用於存儲訓練和驗證的LOSS值\n    for epoch in range(1, 101):\n      train1()\n      train_acc, val_acc, tmp_test_acc = test()\n      train_loss = F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).item()\n      val_loss = F.nll_loss(model()[data.val_mask], data.y[data.val_mask]).item()\n      train_losses.append(train_loss)\n      val_losses.append(val_loss)\n\n      if val_acc > best_val_acc:\n          best_val_acc = val_acc\n          test_acc = tmp_test_acc\n          epoch_best = epoch\n    with torch.no_grad():\n      model.eval()\n      out = model()\n    y_neg_pairs = []\n    y_pos_pairs = []\n    # 獲取數據索引\n    data_idx = np.arange(len(data.x))\n    # 遍歷數據集中的標籤\n    for idx1, y1 in enumerate(data.y[data.train_mask].cpu().numpy()):\n        for idx2, y2 in enumerate(data.y[data.train_mask].cpu().numpy()):\n            # 創建負樣本對：當兩個樣本的索引不相等且標籤不相同時，將它們添加到負樣本對列表中\n            if idx1 > idx2 and y1 != y2:\n                y_neg_pairs.append([idx1, idx2])\n            # 創建正樣本對：當兩個樣本的索引不相等且標籤相同時，將它們添加到正樣本對列表中\n            if idx1 > idx2 and y1 == y2:\n                y_pos_pairs.append([idx1, idx2])\n    # 轉換負樣本對和正樣本對列表為NumPy數組\n    y_neg_pairs = np.array(y_neg_pairs)\n    y_pos_pairs = np.array(y_pos_pairs)\n    # 創建並訓練模型，使用給定的學習率和權重衰減。\n    model = Net(dropout = dropout).to(device)\n    optimizer = torch.optim.Adam([\n    {'params': model.conv_layers.parameters(), 'weight_decay': weight_decay},\n    {'params': model.fc1.parameters(), 'weight_decay': weight_decay}\n    ], lr=lr)\n\n    best_val_acc = 0  # 用於追蹤最佳驗證準確度的變數\n    best_model_state_dict = None  # 用於保存最佳模型權重的變數\n\n    for epoch in range(1, 301):\n        train2()\n        # 計算驗證準確度\n        val_acc = test()[1]\n        # 更新最佳驗證準確度和最佳模型權重\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n    return best_val_acc\n\n#隨機範圍取值\ndef random_select(n):\n    return random.uniform(random_number_range[n][0],random_number_range[n][1])\n","metadata":{"executionInfo":{"elapsed":250,"status":"ok","timestamp":1697511074377,"user":{"displayName":"林子傑","userId":"16455381566400525515"},"user_tz":-480},"id":"RMcDJ37EFKxz","papermill":{"duration":0.092584,"end_time":"2024-03-02T01:25:58.346070","exception":false,"start_time":"2024-03-02T01:25:58.253486","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, data = Net().to(device), data.to(device)","metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1697511112182,"user":{"displayName":"林子傑","userId":"16455381566400525515"},"user_tz":-480},"id":"cI-e7rUCXcG0","papermill":{"duration":0.24353,"end_time":"2024-03-02T01:25:58.602966","exception":false,"start_time":"2024-03-02T01:25:58.359436","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 调用函数生成随机数\ndef generate_random_numbers(number_range):\n    random_numbers = []\n    for min_value, max_value in number_range:\n        random_value = random.uniform(min_value, max_value)\n        if random_value>1:\n          random_value=int(random_value)\n        random_numbers.append(random_value)\n\n    return random_numbers\n# 定义范围\n\n\nrX = generate_random_numbers(random_number_range)\n","metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1697511102245,"user":{"displayName":"林子傑","userId":"16455381566400525515"},"user_tz":-480},"id":"4LbJ4DjNaJ6Y","papermill":{"duration":0.021937,"end_time":"2024-03-02T01:25:58.638531","exception":false,"start_time":"2024-03-02T01:25:58.616594","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"C1=0.4#GBEST區間\nC2=0.65#PBEST區間\nCg=0.5\nCp=0.8\nCw=0.9","metadata":{"papermill":{"duration":0.022288,"end_time":"2024-03-02T01:26:27.830675","exception":false,"start_time":"2024-03-02T01:26:27.808387","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Njob=11 # 表離散型參數個數\nNjob2=15 # 表連續型參數個數","metadata":{"papermill":{"duration":0.021813,"end_time":"2024-03-02T01:26:27.867882","exception":false,"start_time":"2024-03-02T01:26:27.846069","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#iSSO-ssoHPO（超參數優化）老師版本\n\nimport numpy as np\nimport time\nimport copy\nimport random\nimport os\n\nrandom.seed()\n\n#random_number_range=[(0.0005, 0.1), (1e-4, 1e-3),(0.2,0.7),(8,256),(1,4)]\n# 定義參數。\n\n#代數與每代幾組解\nBest_time=0\ndata = dataset[0]\ngbest_list=[]\n\nstart_time = time.time()\n'''----- 生成初始种群 -----'''\nX, FX, pX, pF, gBest, genBest= [], [], [], [], 0, 0,\nvalue_to_x_dict = {}\numax=np.array([0]*Njob, dtype=np.float64)\numin=np.array([1000]*Njob, dtype=np.float64)\n#pX存gbest解pF存值\ngen=0\n#初始解\nfor sol in range(Nsol):\n    rX = generate_random_numbers(random_number_range)\n    if sol==Nsol-1:\n        rX=list(start_sol)\n    X.append(rX) # add to the X2.\n    pX.append(rX) # add to the X2.\n    for job in range(Njob):\n        if X[sol][job]>umax[job]:\n            umax[job]=X[sol][job]\n        if X[sol][job]<umin[job]:\n            umin[job]=X[sol][job]\n    best_val_acc_list = []\n    \n    best_num_layers = int(X[sol][0])\n    best_optim = int(X[sol][1])\n    best_activation = int(X[sol][2])\n    earlystop = int(X[sol][3])\n    epoch1 = int(X[sol][4])\n    epoch2 = int(X[sol][5])\n    best_hidden_dim_list = [X[sol][6+i] for i in range(best_num_layers)]\n    best_dropout = X[sol][11]\n    best_lr1 = X[sol][12]\n    best_lr2 = X[sol][13]\n    best_weight_decay_list1 = [X[sol][14+i] for i in range(best_num_layers)]\n    best_weight_decay_list2 = [X[sol][19+i] for i in range(best_num_layers)]\n    best_weight_decay_fc1 = X[sol][24]\n    best_weight_decay_fc2 = X[sol][25]\n     \n    model, data = Net(hidden_dims=best_hidden_dim_list,dropout = best_dropout,num_layers=best_num_layers,activation=best_activation).to(device), data.to(device)\n    optimizer_params1 = []\n    for i, conv_layer in enumerate(model.conv_layers):\n        optimizer_params1.append({'params': conv_layer.parameters(), 'weight_decay': best_weight_decay_list1[i]})\n    optimizer_params1.append({'params': model.fc1.parameters(), 'weight_decay': best_weight_decay_fc1})\n    if best_optim==0:\n        optimizer = torch.optim.Adam(optimizer_params1, lr=best_lr1)\n    elif best_optim==1:\n        optimizer = torch.optim.AdamW(optimizer_params1, lr=best_lr1)\n    elif best_optim==2:\n        optimizer = torch.optim.Adagrad(optimizer_params1, lr=best_lr1)\n    elif best_optim==3:\n        optimizer = torch.optim.AMSGrad(optimizer_params1, lr=best_lr1)\n    best_val_acc = test_acc = 0\n    train_losses, val_losses = [], []  # 用於存儲訓練和驗證的LOSS值\n    for epoch in range(1, epoch1):\n        train1()\n        train_acc, val_acc, tmp_test_acc = test()\n        train_loss = F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).item()\n        val_loss = F.nll_loss(model()[data.val_mask], data.y[data.val_mask]).item()\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            test_acc = tmp_test_acc\n            epoch_best = epoch\n    with torch.no_grad():\n        model.eval()\n        out = model()\n    y_neg_pairs = []\n    y_pos_pairs = []\n      # 獲取數據索引\n    data_idx = np.arange(len(data.x))\n      # 遍歷數據集中的標籤\n    for idx1, y1 in enumerate(data.y[data.train_mask].cpu().numpy()):\n          for idx2, y2 in enumerate(data.y[data.train_mask].cpu().numpy()):\n              # 創建負樣本對：當兩個樣本的索引不相等且標籤不相同時，將它們添加到負樣本對列表中\n              if idx1 > idx2 and y1 != y2:\n                  y_neg_pairs.append([idx1, idx2])\n              # 創建正樣本對：當兩個樣本的索引不相等且標籤相同時，將它們添加到正樣本對列表中\n              if idx1 > idx2 and y1 == y2:\n                  y_pos_pairs.append([idx1, idx2])\n      # 轉換負樣本對和正樣本對列表為NumPy數組\n    y_neg_pairs = np.array(y_neg_pairs)\n    y_pos_pairs = np.array(y_pos_pairs)\n      # 創建神經網絡模型並將其移動到指定設備（通常是GPU）\n    for k in range(num_experiments):\n        model, data = Net(hidden_dims=best_hidden_dim_list,dropout = best_dropout,num_layers=best_num_layers,activation=best_activation).to(device), data.to(device)\n        optimizer_params2 = []\n        for i, conv_layer in enumerate(model.conv_layers):\n            optimizer_params2.append({'params': conv_layer.parameters(), 'weight_decay': best_weight_decay_list2[i]})\n        optimizer_params2.append({'params': model.fc1.parameters(), 'weight_decay': best_weight_decay_fc2})\n        if best_optim==0:\n            optimizer = torch.optim.Adam(optimizer_params2, lr=best_lr2)\n        elif best_optim==1:\n            optimizer = torch.optim.AdamW(optimizer_params2, lr=best_lr2)\n        elif best_optim==2:\n            optimizer = torch.optim.Adagrad(optimizer_params2, lr=best_lr2)\n        elif best_optim==3:\n            optimizer = torch.optim.AMSGrad(optimizer_params2, lr=best_lr2)\n        # 初始化最佳驗證準確率和測試準確率\n        best_val_acc = test_acc = 0\n        patience = earlystop  # 最大停止等待周期\n        train_losses, val_losses,test_acc_list,val_acc_list  =[], [], [],[]  # 用於存儲訓練和驗證的LOSS值\n        for epoch in range(1, epoch2):\n          train2()\n          train_acc, val_acc, tmp_test_acc = test()\n          train_loss = F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).item()\n          val_loss = F.nll_loss(model()[data.val_mask], data.y[data.val_mask]).item()\n          train_losses.append(train_loss)\n          val_losses.append(val_loss)\n          test_acc_list.append(tmp_test_acc)\n          val_acc_list.append(val_acc)\n          if val_acc > best_val_acc:\n              best_val_acc = val_acc\n              test_acc = tmp_test_acc\n              epoch_best = epoch  \n              wait = 0\n          else:\n              wait += 1\n          if wait >= patience:\n            #print(f'Early stopping after {epoch} epochs.',best_val_acc)\n            break\n\n        #print(k,epoch,best_val_acc)\n        best_val_acc_list.append(best_val_acc)\n    value=np.mean(best_val_acc_list)\n    FX.append(value)\n    pF.append(value)\n    if FX[sol]>FX[gBest]:\n        gBest=sol\n        pX[gBest]=rX\n        gbest_list=best_val_acc_list\n    print(gen,sol ,rX,value)\n    value_to_x_dict[value] = (tuple(X[sol]),(gen,sol))\nfor gen in range(1,Ngen+1):\n  for sol in range(Nsol):\n      job=-1\n      u = (umax-umin)/2*Njob \n      #isso\n      while Njob<=job<Njob+Njob2-1:\n        job+=1\n        rnd=np.random.rand()\n        #print(\"isso\",job,rnd)\n        if sol<1:\n          #重要!未有初始u只好隨機兩個解之後預計刪除\n          X[sol][job]=random_select(job)\n        else:\n          if rnd<C1 or (X[sol-1][job]==value_to_x_dict[pF[gBest]][0][job]):\n            X[sol][job]=X[sol-1][job]+u[job]*generate_random_numbers(random_number_range)[job]\n            if X[sol][job]<random_number_range[job][0] or X[sol][job]>random_number_range[job][1]:\n                #print(\"random\",random_number_range[job])\n                X[sol][job]=random_select(job)\n          elif rnd<C2:\n            X[sol][job]=pX[gBest][job]+u[job]*generate_random_numbers(random_number_range)[job]\n            if X[sol][job]<random_number_range[job][0] or X[sol][job]>random_number_range[job][1]:\n                #print(\"random\",random_number_range[job])\n                X[sol][job]=random_select(job)\n          else:\n            X[sol][job]=X[sol-1][job]+(X[sol-1][job]-pX[gBest][job])*generate_random_numbers(random_number_range)[job]\n            if X[sol][job]<random_number_range[job][0] or X[sol][job]>random_number_range[job][1]:\n                #print(\"random\",random_number_range[job])\n                X[sol][job]=random_select(job)\n        if X[sol][job]>umax[job]:\n            umax[job]=X[sol][job]\n        if X[sol][job]<umin[job]:\n            umin[job]=X[sol][job]\n      best_num_layers = int(X[sol][0])\n      best_optim = int(X[sol][1])\n      best_activation = int(X[sol][2])\n      earlystop = int(X[sol][3])\n      epoch1 = int(X[sol][4])\n      epoch2 = int(X[sol][5])\n      best_hidden_dim_list = [X[sol][6+i] for i in range(best_num_layers)]\n      best_dropout = X[sol][11]\n      best_lr1 = X[sol][12]\n      best_lr2 = X[sol][13]\n      best_weight_decay_list1 = [X[sol][14+i] for i in range(best_num_layers)]\n      best_weight_decay_list2 = [X[sol][19+i] for i in range(best_num_layers)]\n      best_weight_decay_fc1 = X[sol][24]\n      best_weight_decay_fc2 = X[sol][25]      \n      #sso\n      X[sol][1]=int(X[sol][1])\n      X[sol][2]=int(X[sol][2])\n      while job<=Njob-1:\n        job+=1\n\n        rnd2=np.random.rand()\n        #print(\"sso\",job,rnd2)\n        if rnd2<Cg:\n          X[sol][job]=pX[gBest][job]\n          if X[sol][job]<random_number_range[job][0] or X[sol][job]>random_number_range[job][1]:\n              #print(\"random\",random_number_range[job])\n              X[sol][job]=int(random_select(job))\n        elif rnd2<Cp:\n          X[sol][job]=pX[sol][job]\n          if X[sol][job]<random_number_range[job][0] or X[sol][job]>random_number_range[job][1]:\n              #print(\"random\",random_number_range[job])\n              X[sol][job]=int(random_select(job))\n        elif rnd2<Cw:\n          X[sol][job]=X[sol-1][job]\n          if X[sol][job]<random_number_range[job][0] or X[sol][job]>random_number_range[job][1]:\n              #print(\"random\",random_number_range[job])\n              X[sol][job]=int(random_select(job))\n        else:\n          X[sol][job]=int(random_select(job))\n          if X[sol][job]<random_number_range[job][0] or X[sol][job]>random_number_range[job][1]:\n              #print(\"random\",random_number_range[job])\n              X[sol][job]=int(random_select(job))\n      #算u\n      \n      #計算value\n      best_val_acc_list = []\n      break_c=0\n      start_time1 = time.time()        \n      model, data = Net(hidden_dims=best_hidden_dim_list,dropout = best_dropout,num_layers=best_num_layers,activation=best_activation).to(device), data.to(device)\n      optimizer_params1 = []\n      for i, conv_layer in enumerate(model.conv_layers):\n        optimizer_params1.append({'params': conv_layer.parameters(), 'weight_decay': best_weight_decay_list1[i]})\n      optimizer_params1.append({'params': model.fc1.parameters(), 'weight_decay': best_weight_decay_fc1})\n      if best_optim==0:\n        optimizer = torch.optim.Adam(optimizer_params1, lr=best_lr1)\n      elif best_optim==1:\n        optimizer = torch.optim.AdamW(optimizer_params1, lr=best_lr1)\n      elif best_optim==2:\n        optimizer = torch.optim.Adagrad(optimizer_params1, lr=best_lr1)\n      elif best_optim==3:\n        optimizer = torch.optim.AMSGrad(optimizer_params1, lr=best_lr1)\n      best_val_acc = test_acc = 0\n      train_losses, val_losses = [], []  # 用於存儲訓練和驗證的LOSS值\n      for epoch in range(1, epoch1):\n        train1()\n        train_acc, val_acc, tmp_test_acc = test()\n        train_loss = F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).item()\n        val_loss = F.nll_loss(model()[data.val_mask], data.y[data.val_mask]).item()\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            test_acc = tmp_test_acc\n            epoch_best = epoch\n      with torch.no_grad():\n        model.eval()\n        out = model()\n      y_neg_pairs = []\n      y_pos_pairs = []\n      # 獲取數據索引\n      data_idx = np.arange(len(data.x))\n      # 遍歷數據集中的標籤\n      for idx1, y1 in enumerate(data.y[data.train_mask].cpu().numpy()):\n          for idx2, y2 in enumerate(data.y[data.train_mask].cpu().numpy()):\n              # 創建負樣本對：當兩個樣本的索引不相等且標籤不相同時，將它們添加到負樣本對列表中\n              if idx1 > idx2 and y1 != y2:\n                  y_neg_pairs.append([idx1, idx2])\n              # 創建正樣本對：當兩個樣本的索引不相等且標籤相同時，將它們添加到正樣本對列表中\n              if idx1 > idx2 and y1 == y2:\n                  y_pos_pairs.append([idx1, idx2])\n      # 轉換負樣本對和正樣本對列表為NumPy數組\n      y_neg_pairs = np.array(y_neg_pairs)\n      y_pos_pairs = np.array(y_pos_pairs)\n      # 創建神經網絡模型並將其移動到指定設備（通常是GPU）\n      for k in range(num_experiments):\n        model, data = Net(hidden_dims=best_hidden_dim_list,dropout = best_dropout,num_layers=best_num_layers,activation=best_activation).to(device), data.to(device)\n        optimizer_params2 = []\n        for i, conv_layer in enumerate(model.conv_layers):\n            optimizer_params2.append({'params': conv_layer.parameters(), 'weight_decay': best_weight_decay_list2[i]})\n        optimizer_params2.append({'params': model.fc1.parameters(), 'weight_decay': best_weight_decay_fc2})\n        if best_optim==0:\n            optimizer = torch.optim.Adam(optimizer_params2, lr=best_lr2)\n        elif best_optim==1:\n            optimizer = torch.optim.AdamW(optimizer_params2, lr=best_lr2)\n        elif best_optim==2:\n            optimizer = torch.optim.Adagrad(optimizer_params2, lr=best_lr2)\n        elif best_optim==3:\n            optimizer = torch.optim.AMSGrad(optimizer_params2, lr=best_lr2)\n        # 初始化最佳驗證準確率和測試準確率\n        best_val_acc = test_acc = 0\n        patience = earlystop  # 最大停止等待周期\n        train_losses, val_losses,test_acc_list,val_acc_list  =[], [], [],[]  # 用於存儲訓練和驗證的LOSS值\n        for epoch in range(1, epoch2):\n          train2()\n          train_acc, val_acc, tmp_test_acc = test()\n          train_loss = F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).item()\n          val_loss = F.nll_loss(model()[data.val_mask], data.y[data.val_mask]).item()\n          train_losses.append(train_loss)\n          val_losses.append(val_loss)\n          test_acc_list.append(tmp_test_acc)\n          val_acc_list.append(val_acc)\n          if val_acc > best_val_acc:\n              best_val_acc = val_acc\n              test_acc = tmp_test_acc\n              epoch_best = epoch  \n              wait = 0\n          else:\n              wait += 1\n          if wait >= patience:\n            #print(f'Early stopping after {epoch} epochs.',best_val_acc)\n            break\n\n        #print(k,epoch,best_val_acc)\n        best_val_acc_list.append(best_val_acc)\n        if k==5 and np.mean(best_val_acc_list)+0.02<pF[gBest]:\n            break\n        elif k==10 and np.mean(best_val_acc_list)+0.01<pF[gBest]:\n            break\n      end_time1 = time.time()\n      value=np.mean(best_val_acc_list)\n      #計算value結束\n      print(gen,\"  \",sol, 'new', X[sol],value , 'vs',pF[gBest])\n      #print(value,pF[sol],pF[gBest])\n      value_to_x_dict[value] = (tuple(X[sol]),(gen,sol))\n      #print(value_to_x_dict)\n      #判斷大小\n      #print(\"目前gbest\",pF[gBest])\n      if value>pF[sol]:\n        pF[sol]=value\n        #pX[sol]=X[sol]\n        if value>pF[gBest]:\n          gBest=sol\n          genBest=gen\n          gbest_list=best_val_acc_list\n          Best_time=end_time1-start_time1 \n\nend_time = time.time()\nsso_time1=end_time-start_time\nsol0=value_to_x_dict[pF[gBest]][0]\nprint(\"optimal sequence\",value_to_x_dict[pF[gBest]][0])\nprint(\"optimal value:%f\"%pF[gBest])\n\nprint(\"gbest_time:%f\"%Best_time)\nprint(\"optimal generation number:\", value_to_x_dict[pF[gBest]][1][0])\nprint(\"optimal sol number:\", value_to_x_dict[pF[gBest]][1][1])\nprint(\"sso_time:\", sso_time1)","metadata":{"papermill":{"duration":5933.982149,"end_time":"2024-03-02T03:05:21.865510","exception":false,"start_time":"2024-03-02T01:26:27.883361","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbest_list","metadata":{"papermill":{"duration":0.03486,"end_time":"2024-03-02T03:05:21.924952","exception":false,"start_time":"2024-03-02T03:05:21.890092","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_value = np.max(gbest_list)\nmin_value = np.min(gbest_list)\nsorted_data = sorted(gbest_list)\n\n# 獲取第二小的值\nsecond_smallest = sorted_data[1]\nstd_deviation = np.std(gbest_list)\nprint(\"最大值:\", max_value)\nprint(\"最小值:\", second_smallest)\nprint(\"平均值:\", sum(gbest_list)/num_experiments)\nprint(\"標準差:\", std_deviation)","metadata":{"papermill":{"duration":0.034907,"end_time":"2024-03-02T03:05:21.984232","exception":false,"start_time":"2024-03-02T03:05:21.949325","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"第一組","metadata":{"papermill":{"duration":0.024371,"end_time":"2024-03-02T03:05:22.033176","exception":false,"start_time":"2024-03-02T03:05:22.008805","status":"completed"},"tags":[]}}]}